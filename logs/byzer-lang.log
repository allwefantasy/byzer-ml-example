23/03/08 19:05:16  INFO MLSQLLanguageServer: start....
23/03/08 19:05:17  WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 192.168.3.13 instead (on interface en0)
23/03/08 19:05:17  WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/03/08 19:05:17  INFO SparkRuntime: register HiveSqlDialect.....
23/03/08 19:05:17  INFO SparkRuntime: create Runtime...
23/03/08 19:05:17  INFO SparkRuntime: PSExecutor configured...
23/03/08 19:05:17  INFO log: Logging initialized @1895ms
23/03/08 19:05:17  WARN NetTool: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 192.168.3.13 instead (on interface en0)
23/03/08 19:05:17  WARN NetTool: Set LOCAL_IP if you need to bind to another address
23/03/08 19:05:17  INFO Server: jetty-9.2.z-SNAPSHOT
23/03/08 19:05:17  INFO ServerConnector: Started ServerConnector@4faea094{HTTP/1.1}{192.168.3.13:50591}
23/03/08 19:05:17  INFO Server: Started @1986ms
23/03/08 19:05:17  INFO NetTool: Successfully started service 'driver-log-server' on port 50591.
23/03/08 19:05:17  INFO SparkRuntime: DriverLogServer is started in http://192.168.3.13:50591/v2/writelog with token:e493fa45-122c-494b-b752-dc4c82657f08
23/03/08 19:05:17  INFO SparkContext: Running Spark version 3.3.0
23/03/08 19:05:17  WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/03/08 19:05:17  INFO ResourceUtils: ==============================================================
23/03/08 19:05:17  INFO ResourceUtils: No custom resources configured for spark.driver.
23/03/08 19:05:17  INFO ResourceUtils: ==============================================================
23/03/08 19:05:17  INFO SparkContext: Submitted application: MLSQL-desktop
23/03/08 19:05:17  INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/03/08 19:05:17  INFO ResourceProfile: Limiting resource is cpu
23/03/08 19:05:17  INFO ResourceProfileManager: Added ResourceProfile id: 0
23/03/08 19:05:17  INFO SecurityManager: Changing view acls to: allwefantasy
23/03/08 19:05:17  INFO SecurityManager: Changing modify acls to: allwefantasy
23/03/08 19:05:17  INFO SecurityManager: Changing view acls groups to: 
23/03/08 19:05:17  INFO SecurityManager: Changing modify acls groups to: 
23/03/08 19:05:17  INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(allwefantasy); groups with view permissions: Set(); users  with modify permissions: Set(allwefantasy); groups with modify permissions: Set()
23/03/08 19:05:18  INFO Utils: Successfully started service 'sparkDriver' on port 50592.
23/03/08 19:05:18  INFO SparkEnv: Registering MapOutputTracker
23/03/08 19:05:18  INFO SparkEnv: Registering BlockManagerMaster
23/03/08 19:05:18  INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/03/08 19:05:18  INFO SparkEnv: Registering OutputCommitCoordinator
23/03/08 19:05:18  WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/03/08 19:05:18  INFO Utils: Successfully started service 'SparkUI' on port 4041.
23/03/08 19:05:18  INFO PSExecutorPlugin: PSExecutorPlugin starting.....
23/03/08 19:05:18  INFO Utils: Successfully started service 'PSExecutorBackend' on port 50593.
23/03/08 19:05:18  INFO ExecutorPluginContainer: Initialized executor component for plugin org.apache.spark.ps.cluster.PSExecutorPlugin.
23/03/08 19:05:18  INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 50594.
23/03/08 19:05:18  INFO NettyBlockTransferService: Server created on 192.168.3.13:50594
23/03/08 19:05:18  INFO JobManager: JobManager started with initialDelay=30 checkTimeInterval=5
23/03/08 19:05:18  INFO SparkRuntime: PSDriver starting...
23/03/08 19:05:18  INFO PSDriverBackend: setup ps driver rpc env: 192.168.3.13:7778 clientMode=false
23/03/08 19:05:18  INFO Utils: Successfully started service 'PSDriverEndpoint' on port 7778.
23/03/08 19:05:19  INFO SparkRuntime: mlsql server start with configuration!
23/03/08 19:05:19  INFO SparkRuntime: --------------------------------------------------------------------------------------------------------------------------------------------------------------------------
23/03/08 19:05:19  INFO SparkRuntime: |streaming.plugin.clzznames                        |tech.mlsql.plugins.ds.MLSQLExcelApp,tech.mlsql.plugins.shell.app.MLSQLShell,tech.mlsql.plugins.assert.app.MLSQLAssert|
23/03/08 19:05:19  INFO SparkRuntime: |spark.sql.files.maxPartitionBytes                 |5242880                                                                                                              |
23/03/08 19:05:19  INFO SparkRuntime: |spark.io.compression.lz4.blockSize                |128k                                                                                                                 |
23/03/08 19:05:19  INFO SparkRuntime: |streaming.job.cancel                              |true                                                                                                                 |
23/03/08 19:05:19  INFO SparkRuntime: |spark.shuffle.spill.batchSize                     |1000                                                                                                                 |
23/03/08 19:05:19  INFO SparkRuntime: |spark.sql.pivotMaxValues                          |1000                                                                                                                 |
23/03/08 19:05:19  INFO SparkRuntime: |spark.sql.catalog.spark_catalog                   |org.apache.spark.sql.delta.catalog.DeltaCatalog                                                                      |
23/03/08 19:05:19  INFO SparkRuntime: |spark.memory.storageFraction                      |0.1                                                                                                                  |
23/03/08 19:05:19  INFO SparkRuntime: |spark.file.transferTo                             |false                                                                                                                |
23/03/08 19:05:19  INFO SparkRuntime: |spark.sql.parquet.columnarReaderBatchSize         |1000                                                                                                                 |
23/03/08 19:05:19  INFO SparkRuntime: |streaming.name                                    |MLSQL-desktop                                                                                                        |
23/03/08 19:05:19  INFO SparkRuntime: |streaming.rest                                    |true                                                                                                                 |
23/03/08 19:05:19  INFO SparkRuntime: |streaming.thrift                                  |false                                                                                                                |
23/03/08 19:05:19  INFO SparkRuntime: |spark.shuffle.unsafe.file.ouput.buffer            |1m                                                                                                                   |
23/03/08 19:05:19  INFO SparkRuntime: |streaming.master                                  |local[*]                                                                                                             |
23/03/08 19:05:19  INFO SparkRuntime: |streaming.driver.port                             |8000                                                                                                                 |
23/03/08 19:05:19  INFO SparkRuntime: |streaming.spark.service                           |true                                                                                                                 |
23/03/08 19:05:19  INFO SparkRuntime: |streaming.platform                                |spark                                                                                                                |
23/03/08 19:05:19  INFO SparkRuntime: |spark.unsafe.sorter.spill.reader.buffer.size      |1m                                                                                                                   |
23/03/08 19:05:19  INFO SparkRuntime: |spark.shuffle.spill.numElementsForceSpillThreshold|10000                                                                                                                |
23/03/08 19:05:19  INFO SparkRuntime: |spark.shuffle.accurateBlockThreshold              |5242880                                                                                                              |
23/03/08 19:05:19  INFO SparkRuntime: |streaming.datalake.path                           |./data                                                                                                               |
23/03/08 19:05:19  INFO SparkRuntime: |spark.shuffle.file.buffer                         |1m                                                                                                                   |
23/03/08 19:05:19  INFO SparkRuntime: |spark.sql.inMemoryColumnarStorage.batchSize       |1000                                                                                                                 |
23/03/08 19:05:19  INFO SparkRuntime: |spark.memory.fraction                             |0.1                                                                                                                  |
23/03/08 19:05:19  INFO SparkRuntime: |spark.sql.extensions                              |io.delta.sql.DeltaSparkSessionExtension                                                                              |
23/03/08 19:05:19  INFO SparkRuntime: |spark.sql.shuffle.partitions                      |8                                                                                                                    |
23/03/08 19:05:19  INFO SparkRuntime: --------------------------------------------------------------------------------------------------------------------------------------------------------------------------
23/03/08 19:05:19  INFO SparkSessionCacheManager: Scheduling SparkSession cache cleaning every 60 seconds
23/03/08 19:05:19  INFO SparkRuntime: register functions.....
23/03/08 19:05:19  INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/03/08 19:05:19  INFO SharedState: Warehouse path is 'file:/Users/allwefantasy/projects/byzer-ml-example/spark-warehouse'.
23/03/08 19:05:21  WARN SimpleFunctionRegistry: The function array_intersect replaced a previously registered function.
23/03/08 19:05:21  WARN SimpleFunctionRegistry: The function uuid replaced a previously registered function.
23/03/08 19:05:21  WARN SimpleFunctionRegistry: The function bytestringasgb replaced a previously registered function.
23/03/08 19:05:21  INFO SparkRuntime: register functions.....
23/03/08 19:05:21  WARN SimpleFunctionRegistry: The function rest_request replaced a previously registered function.
23/03/08 19:05:21  INFO SparkRuntime: register functions.....
23/03/08 19:05:21  WARN SimpleFunctionRegistry: The function aes_decrypt replaced a previously registered function.
23/03/08 19:05:21  WARN SimpleFunctionRegistry: The function aes_encrypt replaced a previously registered function.
23/03/08 19:05:21  INFO MLSQLStreamManager: Start streaming job monitor....
23/03/08 19:05:21  INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
23/03/08 19:05:22  INFO PSExecutorBackend: Connecting to driver: spark://ps-driver-endpoint@192.168.3.13:7778
23/03/08 19:05:22  INFO TransportClientFactory: Successfully created connection to /192.168.3.13:7778 after 122 ms (0 ms spent in bootstraps)
23/03/08 19:05:22  INFO PSExecutorBackend: 0@192.168.3.13 register with driver: spark://ps-driver-endpoint@192.168.3.13:7778 success
23/03/08 19:05:22  INFO impl: scan service package => null
23/03/08 19:05:22  INFO impl: load service in ServiceFramwork.serviceModules =>0
23/03/08 19:05:22  INFO impl: total load service  =>7
23/03/08 19:05:23  INFO impl: controller load :    streaming.rest.RestController
23/03/08 19:05:23  INFO impl: controller load :    streaming.rest.RestPredictController
23/03/08 19:05:23  INFO impl: controller load :    streaming.rest.HealthController
23/03/08 19:05:23  INFO Server: jetty-9.2.z-SNAPSHOT
23/03/08 19:05:23  INFO PluginHook: Load build-in plugins....
23/03/08 19:05:23  INFO ServerConnector: Started ServerConnector@b984b1a{HTTP/1.1}{0.0.0.0:8000}
23/03/08 19:05:23  INFO Server: Started @8380ms
23/03/08 19:05:24  INFO DelegatingLogStore: LogStore `LogStoreAdapter(io.delta.storage.HDFSLogStore)` is used for scheme `file`
23/03/08 19:05:24  INFO DeltaLog: Creating initial snapshot without metadata, because the directory is empty
23/03/08 19:05:24  INFO InitialSnapshot: [tableId=9b1f7f33-39c3-4b7f-8796-325b26367183] Created snapshot InitialSnapshot(path=file:/Users/allwefantasy/projects/byzer-ml-example/data/__instances__/MLSQL-desktop/__mlsql__/config/_delta_log, version=-1, metadata=Metadata(cadbc3a5-daf8-4873-939a-f4e6bdb27bdf,null,null,Format(parquet,Map()),null,List(),Map(),Some(1678273524237)), logSegment=LogSegment(file:/Users/allwefantasy/projects/byzer-ml-example/data/__instances__/MLSQL-desktop/__mlsql__/config/_delta_log,-1,List(),List(),None,-1), checksumOpt=None)
23/03/08 19:05:24  INFO CodeGenerator: Code generated in 189.671789 ms
23/03/08 19:05:24  INFO SparkContext: Starting job: collect at DeltaLakeDBStore.scala:24
23/03/08 19:05:25  INFO SnapshotTimer: Scheduler MLSQL state every 3 seconds
23/03/08 19:05:25  INFO MLSQLExcelApp: Load ds: tech.mlsql.plugins.ds.MLSQLExcel
23/03/08 19:05:27  INFO DelegatingLogStore: LogStore `LogStoreAdapter(io.delta.storage.HDFSLogStore)` is used for scheme `file`
23/03/08 19:05:27  INFO DeltaLog: Creating initial snapshot without metadata, because the directory is empty
23/03/08 19:05:27  INFO InitialSnapshot: [tableId=3d0f5550-fe31-4ffc-90fa-9f760af38a0f] Created snapshot InitialSnapshot(path=file:/Users/allwefantasy/projects/byzer-ml-example/data/__instances__/MLSQL-desktop/__mlsql__/plugins/_delta_log, version=-1, metadata=Metadata(a970a5cf-7e30-4604-976e-c06bf94053d2,null,null,Format(parquet,Map()),null,List(),Map(),Some(1678273527077)), logSegment=LogSegment(file:/Users/allwefantasy/projects/byzer-ml-example/data/__instances__/MLSQL-desktop/__mlsql__/plugins/_delta_log,-1,List(),List(),None,-1), checksumOpt=None)
23/03/08 19:05:27  INFO DelegatingLogStore: LogStore `LogStoreAdapter(io.delta.storage.HDFSLogStore)` is used for scheme `file`
23/03/08 19:05:27  INFO DeltaLog: Creating initial snapshot without metadata, because the directory is empty
23/03/08 19:05:27  INFO InitialSnapshot: [tableId=187ae76c-a2f9-480a-9536-1821fb438a77] Created snapshot InitialSnapshot(path=file:/Users/allwefantasy/projects/byzer-ml-example/data/__instances__/MLSQL-desktop/__mlsql__/etRecord/_delta_log, version=-1, metadata=Metadata(08d7738c-8966-497e-8a5a-ab26026af5fa,null,null,Format(parquet,Map()),null,List(),Map(),Some(1678273527105)), logSegment=LogSegment(file:/Users/allwefantasy/projects/byzer-ml-example/data/__instances__/MLSQL-desktop/__mlsql__/etRecord/_delta_log,-1,List(),List(),None,-1), checksumOpt=None)
23/03/08 19:05:27  INFO DelegatingLogStore: LogStore `LogStoreAdapter(io.delta.storage.HDFSLogStore)` is used for scheme `file`
23/03/08 19:05:27  INFO DeltaLog: Creating initial snapshot without metadata, because the directory is empty
23/03/08 19:05:27  INFO InitialSnapshot: [tableId=db089e38-3f8a-44f6-a29e-ab1a24a91526] Created snapshot InitialSnapshot(path=file:/Users/allwefantasy/projects/byzer-ml-example/data/__instances__/MLSQL-desktop/__mlsql__/dsRecord/_delta_log, version=-1, metadata=Metadata(1b40b992-3b5e-4872-9f37-58d5c28cea45,null,null,Format(parquet,Map()),null,List(),Map(),Some(1678273527132)), logSegment=LogSegment(file:/Users/allwefantasy/projects/byzer-ml-example/data/__instances__/MLSQL-desktop/__mlsql__/dsRecord/_delta_log,-1,List(),List(),None,-1), checksumOpt=None)
23/03/08 19:05:27  INFO DelegatingLogStore: LogStore `LogStoreAdapter(io.delta.storage.HDFSLogStore)` is used for scheme `file`
23/03/08 19:05:27  INFO DeltaLog: Creating initial snapshot without metadata, because the directory is empty
23/03/08 19:05:27  INFO InitialSnapshot: [tableId=ae3e900a-4945-448d-9cb6-9b01649b68ea] Created snapshot InitialSnapshot(path=file:/Users/allwefantasy/projects/byzer-ml-example/data/__instances__/MLSQL-desktop/__mlsql__/appRecord/_delta_log, version=-1, metadata=Metadata(25191cee-49f8-4306-8a5c-146510ff87d1,null,null,Format(parquet,Map()),null,List(),Map(),Some(1678273527156)), logSegment=LogSegment(file:/Users/allwefantasy/projects/byzer-ml-example/data/__instances__/MLSQL-desktop/__mlsql__/appRecord/_delta_log,-1,List(),List(),None,-1), checksumOpt=None)
23/03/08 19:11:10  INFO DefaultConsoleClient: [owner] [admin] [groupId] [e88d3827-30e6-4705-adc5-5cc51aad0613] __MMMMMM__ auth admin  want access tables: [{"db":"","table":"./example-data/iris.csv","operateType":{"i":1,"name":"load"},"sourceType":"csv","tableType":{"name":"hdfs","includes":["rate","json","image","parquet","csv","text","binlogRate","excel","libsvm","xml","delta","streamParquet"]}},{"table":"iris","operateType":{"i":1,"name":"load"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}}]
23/03/08 19:11:10  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [e88d3827-30e6-4705-adc5-5cc51aad0613] __MMMMMM__ Total jobs: 1 current job:1 job script:load csv.`./example-data/iris.csv` where `header` = "true" 
as iris 
23/03/08 19:11:10  INFO InMemoryFileIndex: It took 38 ms to list leaf files for 1 paths.
23/03/08 19:11:10  INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
23/03/08 19:11:10  INFO FileSourceStrategy: Pushed Filters: 
23/03/08 19:11:10  INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#320, None)) > 0)
23/03/08 19:11:10  INFO FileSourceStrategy: Output Data Schema: struct<value: string>
23/03/08 19:11:10  INFO CodeGenerator: Code generated in 18.633583 ms
23/03/08 19:11:10  INFO SparkContext: Created broadcast 0 from load at MLSQLCSV.scala:65
23/03/08 19:11:10  INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 19:11:11  INFO SparkContext: Starting job: load at MLSQLCSV.scala:65
23/03/08 19:11:11  INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
23/03/08 19:11:11  INFO FileScanRDD: Reading File path: file:///Users/allwefantasy/projects/byzer-ml-example/example-data/iris.csv, range: 0-4169, partition values: [empty row]
23/03/08 19:11:11  INFO CodeGenerator: Code generated in 10.885672 ms
23/03/08 19:11:11  INFO CodeGenerator: Code generated in 14.37195 ms
23/03/08 19:11:11  INFO FileSourceStrategy: Pushed Filters: 
23/03/08 19:11:11  INFO FileSourceStrategy: Post-Scan Filters: 
23/03/08 19:11:11  INFO FileSourceStrategy: Output Data Schema: struct<value: string>
23/03/08 19:11:11  INFO SparkContext: Created broadcast 2 from load at MLSQLCSV.scala:65
23/03/08 19:11:11  INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 19:11:11  INFO FileSourceStrategy: Pushed Filters: 
23/03/08 19:11:11  INFO FileSourceStrategy: Post-Scan Filters: 
23/03/08 19:11:11  INFO FileSourceStrategy: Output Data Schema: struct<sepal_length: string, sepal_width: string, petal_length: string, petal_width: string, species: string ... 1 more field>
23/03/08 19:11:11  INFO CodeGenerator: Code generated in 14.89361 ms
23/03/08 19:11:12  INFO SparkContext: Created broadcast 3 from take at RestController.scala:290
23/03/08 19:11:12  INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 19:11:12  INFO SparkContext: Starting job: take at RestController.scala:290
23/03/08 19:11:12  INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513
23/03/08 19:11:12  INFO CodeGenerator: Code generated in 20.086308 ms
23/03/08 19:11:12  INFO FileScanRDD: Reading File path: file:///Users/allwefantasy/projects/byzer-ml-example/example-data/iris.csv, range: 0-4169, partition values: [empty row]
23/03/08 19:11:12  INFO CodeGenerator: Code generated in 15.237178 ms
23/03/08 19:11:12  INFO CodeGenerator: Code generated in 60.700939 ms
23/03/08 19:11:12  INFO impl: Completed 200 in 2522ms 	POST /run/script



23/03/08 19:11:15  INFO DefaultConsoleClient: [owner] [admin] [groupId] [2fd49503-ea76-411b-a7ae-06e5ea2a618e] __MMMMMM__ auth admin  want access tables: [{"db":"","table":"./example-data/iris.csv","operateType":{"i":1,"name":"load"},"sourceType":"csv","tableType":{"name":"hdfs","includes":["rate","json","image","parquet","csv","text","binlogRate","excel","libsvm","xml","delta","streamParquet"]}},{"table":"iris","operateType":{"i":1,"name":"load"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}}]
23/03/08 19:11:15  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [2fd49503-ea76-411b-a7ae-06e5ea2a618e] __MMMMMM__ Total jobs: 1 current job:1 job script:load csv.`./example-data/iris.csv` where header = "true" 
as iris 
23/03/08 19:11:15  INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
23/03/08 19:11:15  INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
23/03/08 19:11:15  INFO FileSourceStrategy: Pushed Filters: 
23/03/08 19:11:15  INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#385, None)) > 0)
23/03/08 19:11:15  INFO FileSourceStrategy: Output Data Schema: struct<value: string>
23/03/08 19:11:15  INFO SparkContext: Created broadcast 5 from load at MLSQLCSV.scala:65
23/03/08 19:11:15  INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 19:11:15  INFO SparkContext: Starting job: load at MLSQLCSV.scala:65
23/03/08 19:11:15  INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513
23/03/08 19:11:15  INFO FileScanRDD: Reading File path: file:///Users/allwefantasy/projects/byzer-ml-example/example-data/iris.csv, range: 0-4169, partition values: [empty row]
23/03/08 19:11:15  INFO FileSourceStrategy: Pushed Filters: 
23/03/08 19:11:15  INFO FileSourceStrategy: Post-Scan Filters: 
23/03/08 19:11:15  INFO FileSourceStrategy: Output Data Schema: struct<value: string>
23/03/08 19:11:15  INFO SparkContext: Created broadcast 7 from load at MLSQLCSV.scala:65
23/03/08 19:11:15  INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 19:11:16  INFO FileSourceStrategy: Pushed Filters: 
23/03/08 19:11:16  INFO FileSourceStrategy: Post-Scan Filters: 
23/03/08 19:11:16  INFO FileSourceStrategy: Output Data Schema: struct<sepal_length: string, sepal_width: string, petal_length: string, petal_width: string, species: string ... 1 more field>
23/03/08 19:11:16  INFO SparkContext: Created broadcast 8 from take at RestController.scala:290
23/03/08 19:11:16  INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 19:11:16  INFO SparkContext: Starting job: take at RestController.scala:290
23/03/08 19:11:16  INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
23/03/08 19:11:16  INFO FileScanRDD: Reading File path: file:///Users/allwefantasy/projects/byzer-ml-example/example-data/iris.csv, range: 0-4169, partition values: [empty row]
23/03/08 19:11:16  INFO impl: Completed 200 in 452ms 	POST /run/script



23/03/08 19:41:17  INFO DefaultConsoleClient: [owner] [admin] [groupId] [83a2baf5-f851-45e6-8b2c-6ecfd5904eea] __MMMMMM__ auth admin  want access tables: [{"table":"command","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"8a8596e590e4476cb310baaf44fcbcb1","operateType":{"i":1,"name":"load"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"command","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"78843657cf634806bd486be0869d5ffd","operateType":{"i":1,"name":"load"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"command","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"7a47231e28e741a9a4c164bc83e9d775","operateType":{"i":1,"name":"load"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"command","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"1d32bcc386af4bb19924dceb3a0aec2f","operateType":{"i":1,"name":"load"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"command","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"db":"","table":"/tmp/__python__cache.iris_model","operateType":{"i":0,"name":"save"},"sourceType":"parquet","tableType":{"name":"hdfs","includes":["rate","json","image","parquet","csv","text","binlogRate","excel","libsvm","xml","delta","streamParquet"]}},{"db":"","table":"/tmp/__python__cache.iris_model","operateType":{"i":1,"name":"load"},"sourceType":"parquet","tableType":{"name":"hdfs","includes":["rate","json","image","parquet","csv","text","binlogRate","excel","libsvm","xml","delta","streamParquet"]}},{"table":"iris_model","operateType":{"i":1,"name":"load"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}}]
23/03/08 19:41:17  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [83a2baf5-f851-45e6-8b2c-6ecfd5904eea] __MMMMMM__ Total jobs: 7 current job:1 job script:run command as PythonCommand.`` where parameters='''["conf","schema=file"]''' as 8a8596e590e4476cb310baaf44fcbcb1 
23/03/08 19:41:17  INFO CodeGenerator: Code generated in 54.664145 ms
23/03/08 19:41:17  INFO CodeGenerator: Code generated in 15.009215 ms
23/03/08 19:41:17  INFO CodeGenerator: Code generated in 15.794455 ms
23/03/08 19:41:17  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [83a2baf5-f851-45e6-8b2c-6ecfd5904eea] __MMMMMM__ Total jobs: 7 current job:2 job script:run command as PythonCommand.`` where parameters='''["conf","dataMode=model"]''' as 78843657cf634806bd486be0869d5ffd 
23/03/08 19:41:18  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [83a2baf5-f851-45e6-8b2c-6ecfd5904eea] __MMMMMM__ Total jobs: 7 current job:3 job script:run command as PythonCommand.`` where parameters='''["conf","runIn=driver"]''' as 7a47231e28e741a9a4c164bc83e9d775 
23/03/08 19:41:18  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [83a2baf5-f851-45e6-8b2c-6ecfd5904eea] __MMMMMM__ Total jobs: 7 current job:4 job script:run command as PythonCommand.`` where parameters='''["conf","pythonExec=/opt/miniconda3/envs/ray-2.3.0/bin/python"]''' as 1d32bcc386af4bb19924dceb3a0aec2f 
23/03/08 19:41:18  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [83a2baf5-f851-45e6-8b2c-6ecfd5904eea] __MMMMMM__ Total jobs: 7 current job:5 job script:run command as Ray.`` where
        inputTable="iris" and
        outputTable="iris_model_0" and
        
        
        code='''#%python
#%input=iris
#%output=iris_model
#%schema=file
#%runIn=driver
#%dataMode=model
#%cache=true
#%pythonExec=/opt/miniconda3/envs/ray-2.3.0/bin/python

from pyjava.api.mlsql import RayContext,PythonContext
import pandas as pd
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn import metrics
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
import joblib
from pyjava.storage import streaming_tar
import os

ray_context = RayContext.connect(globals(),None)
data = ray_context.to_pandas()

train,test = train_test_split(data, test_size = 0.4, stratify = data['species_id'], random_state = 42)

X_train = train[['sepal_length','sepal_width','petal_length','petal_width']]
y_train = train.species_id
X_test = test[['sepal_length','sepal_width','petal_length','petal_width']]
y_test = test.species_id

mod_dt = DecisionTreeClassifier(max_depth = 3, random_state = 1)
mod_dt.fit(X_train.values,y_train.values)
prediction=mod_dt.predict(X_test.values)
print('The accuracy of the Decision Tree is',"{:.3f}".format(metrics.accuracy_score(prediction,y_test)))

model_path = "/tmp/model"

if not os.path.exists(model_path):
    os.makedirs(model_path, exist_ok=True)
    
joblib.dump(mod_dt,os.path.join(model_path,"iris_model") )

model_binary = streaming_tar.build_rows_from_file(model_path)

ray_context.build_result(model_binary)''' 
23/03/08 19:41:18  INFO CodeGenerator: Code generated in 22.49451 ms
23/03/08 19:41:18  INFO SparkContext: Starting job: collect at Ray.scala:85
23/03/08 19:41:18  INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1513
23/03/08 19:41:18  INFO SparkContext: Starting job: collect at Ray.scala:259
23/03/08 19:41:18  INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
23/03/08 19:41:18  INFO FileSourceStrategy: Pushed Filters: 
23/03/08 19:41:18  INFO FileSourceStrategy: Post-Scan Filters: 
23/03/08 19:41:18  INFO FileSourceStrategy: Output Data Schema: struct<sepal_length: string, sepal_width: string, petal_length: string, petal_width: string, species: string ... 1 more field>
23/03/08 19:41:18  INFO SparkContext: Created broadcast 12 from rdd at MasterSlaveInSpark.scala:72
23/03/08 19:41:18  INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 19:41:18  INFO MasterSlaveInSpark: RayMode: Resource:[16(16-0)] TargetLen:[1]
23/03/08 19:41:18  INFO FileSourceStrategy: Pushed Filters: 
23/03/08 19:41:18  INFO FileSourceStrategy: Post-Scan Filters: 
23/03/08 19:41:18  INFO FileSourceStrategy: Output Data Schema: struct<sepal_length: string, sepal_width: string, petal_length: string, petal_width: string, species: string ... 1 more field>
23/03/08 19:41:18  INFO SparkContext: Created broadcast 13 from rdd at MasterSlaveInSpark.scala:123
23/03/08 19:41:18  INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 19:41:18  INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1513
23/03/08 19:41:18  INFO FileScanRDD: Reading File path: file:///Users/allwefantasy/projects/byzer-ml-example/example-data/iris.csv, range: 0-4169, partition values: [empty row]
23/03/08 19:41:18  INFO SparkContext: Starting job: collect at MasterSlaveInSpark.scala:123
23/03/08 19:41:18  INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
23/03/08 19:41:18  INFO SocketServerInExecutor: Received connection fromSocket[addr=/192.168.3.13,port=51896,localport=51893]
23/03/08 19:41:19  INFO SocketServerInExecutor: 
java.net.SocketException: Socket closed
	at java.net.PlainSocketImpl.socketAccept(Native Method) ~[?:1.8.0_151]
	at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409) ~[?:1.8.0_151]
	at java.net.ServerSocket.implAccept(ServerSocket.java:545) ~[?:1.8.0_151]
	at java.net.ServerSocket.accept(ServerSocket.java:513) ~[?:1.8.0_151]
	at tech.mlsql.common.utils.distribute.socket.server.SocketServerInExecutor$$anon$2.run(ExecutorSocketServer.scala:71) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
23/03/08 19:41:19  INFO CodeGenerator: Code generated in 19.483687 ms
23/03/08 19:41:19  INFO CodeGenerator: Code generated in 19.495476 ms
23/03/08 19:41:20  INFO BaseAllocator: Debug mode disabled.
23/03/08 19:41:20  INFO DefaultAllocationManagerOption: allocation manager type not specified, using netty as the default type
23/03/08 19:41:20  WARN CheckAllocator: More than one DefaultAllocationManager on classpath. Choosing first found
23/03/08 19:41:20  INFO CheckAllocator: Using DefaultAllocationManager at memory/DefaultAllocationManagerFactory.class
23/03/08 19:41:22  INFO CodeGenerator: Code generated in 8.246203 ms
23/03/08 19:41:22  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [83a2baf5-f851-45e6-8b2c-6ecfd5904eea] __MMMMMM__ Total jobs: 7 current job:6 job script:save overwrite iris_model_0 as parquet.`/tmp/__python__cache.iris_model` 
23/03/08 19:41:22  INFO DriverLogServer: [owner] [admin] [groupId] [83a2baf5-f851-45e6-8b2c-6ecfd5904eea] __MMMMMM__ The accuracy of the Decision Tree is 0.983
23/03/08 19:41:22  INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:41:22  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:41:22  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:41:22  INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:41:22  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:41:22  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:41:22  INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:41:22  INFO CodeGenerator: Code generated in 8.20927 ms
23/03/08 19:41:22  INFO SparkContext: Starting job: save at MLSQLBaseFileSource.scala:30
23/03/08 19:41:22  INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
23/03/08 19:41:22  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:41:22  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:41:22  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:41:22  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:41:22  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:41:22  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:41:22  INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:41:22  INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:41:22  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:41:22  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:41:22  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:41:22  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:41:22  INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:41:22  INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:41:22  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:41:22  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:41:22  INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:41:22  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:41:22  INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:41:22  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:41:22  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:41:22  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:41:22  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:41:22  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:41:22  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:41:22  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:41:22  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:41:22  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:41:22  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:41:22  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:41:22  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:41:22  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:41:22  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:41:22  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:41:22  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:41:22  INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:41:22  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:41:22  INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:41:22  INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:41:22  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:41:22  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:41:22  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:41:22  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:41:22  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:41:22  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:41:22  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:41:22  INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:41:22  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:41:22  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:41:22  INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:41:22  INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:41:22  INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:41:22  INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:41:22  INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:41:22  INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:41:22  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:41:22  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:41:22  INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:41:22  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:41:22  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:41:22  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:41:22  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:41:22  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:41:22  INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:41:22  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:41:22  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:41:22  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:41:22  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:41:22  INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:41:22  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:41:22  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:41:22  INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:41:22  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:41:22  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:41:22  INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:41:22  INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:41:22  INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:41:22  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:41:22  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:41:22  INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:41:22  INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:41:22  INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:41:22  INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:41:22  INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:41:22  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:41:22  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:41:22  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:41:22  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:41:22  INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:41:22  INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:41:22  INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202303081941222333174326424085648_0009_m_000013_29
23/03/08 19:41:22  INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202303081941226342730995627694115_0009_m_000003_19
23/03/08 19:41:22  INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202303081941222212044547888884468_0009_m_000008_24
23/03/08 19:41:22  INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202303081941228323218381941375682_0009_m_000002_18
23/03/08 19:41:22  INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202303081941225450694080194939434_0009_m_000006_22
23/03/08 19:41:22  INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202303081941225370733629738998986_0009_m_000001_17
23/03/08 19:41:22  INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_2023030819412248496976223330126_0009_m_000004_20
23/03/08 19:41:22  INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202303081941227537939703643871079_0009_m_000014_30
23/03/08 19:41:22  INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202303081941227517080792631029423_0009_m_000011_27
23/03/08 19:41:22  INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202303081941222891516475774332789_0009_m_000009_25
23/03/08 19:41:22  INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202303081941224154738395164462874_0009_m_000007_23
23/03/08 19:41:22  INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202303081941226843673576621980174_0009_m_000012_28
23/03/08 19:41:22  INFO CodeGenerator: Code generated in 29.695251 ms
23/03/08 19:41:22  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:41:22  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:41:22  INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:41:22  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:41:22  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:41:22  INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:41:22  INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "start",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "offset",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "value",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int64 start;
  optional int64 offset;
  optional binary value;
}

       
23/03/08 19:41:22  INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "start",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "offset",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "value",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int64 start;
  optional int64 offset;
  optional binary value;
}

       
23/03/08 19:41:22  INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "start",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "offset",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "value",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int64 start;
  optional int64 offset;
  optional binary value;
}

       
23/03/08 19:41:22  INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "start",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "offset",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "value",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int64 start;
  optional int64 offset;
  optional binary value;
}

       
23/03/08 19:41:22  INFO CodecPool: Got brand-new compressor [.snappy]
23/03/08 19:41:22  INFO CodecPool: Got brand-new compressor [.snappy]
23/03/08 19:41:22  INFO CodecPool: Got brand-new compressor [.snappy]
23/03/08 19:41:22  INFO CodecPool: Got brand-new compressor [.snappy]
23/03/08 19:41:23  INFO MasterSlaveInSpark: Exit all data server
23/03/08 19:41:23  INFO FileOutputCommitter: Saved output of task 'attempt_202303081941221387826298782315090_0009_m_000000_16' to file:/tmp/__python__cache.iris_model/_temporary/0/task_202303081941221387826298782315090_0009_m_000000
23/03/08 19:41:23  INFO SparkHadoopMapRedUtil: attempt_202303081941221387826298782315090_0009_m_000000_16: Committed. Elapsed time: 1 ms.
23/03/08 19:41:23  INFO FileOutputCommitter: Saved output of task 'attempt_202303081941228892367721401306207_0009_m_000010_26' to file:/tmp/__python__cache.iris_model/_temporary/0/task_202303081941228892367721401306207_0009_m_000010
23/03/08 19:41:23  INFO FileOutputCommitter: Saved output of task 'attempt_202303081941228534407698219204940_0009_m_000015_31' to file:/tmp/__python__cache.iris_model/_temporary/0/task_202303081941228534407698219204940_0009_m_000015
23/03/08 19:41:23  INFO SparkHadoopMapRedUtil: attempt_202303081941228892367721401306207_0009_m_000010_26: Committed. Elapsed time: 0 ms.
23/03/08 19:41:23  INFO SparkHadoopMapRedUtil: attempt_202303081941228534407698219204940_0009_m_000015_31: Committed. Elapsed time: 0 ms.
23/03/08 19:41:23  INFO FileOutputCommitter: Saved output of task 'attempt_202303081941222316955205859806011_0009_m_000005_21' to file:/tmp/__python__cache.iris_model/_temporary/0/task_202303081941222316955205859806011_0009_m_000005
23/03/08 19:41:23  INFO SparkHadoopMapRedUtil: attempt_202303081941222316955205859806011_0009_m_000005_21: Committed. Elapsed time: 1 ms.
23/03/08 19:41:23  INFO FileFormatWriter: Start to commit write Job 76cbbd2b-2be7-4a86-a4e2-4ca0b8eadac4.
23/03/08 19:41:23  INFO FileFormatWriter: Write Job 76cbbd2b-2be7-4a86-a4e2-4ca0b8eadac4 committed. Elapsed time: 25 ms.
23/03/08 19:41:23  INFO FileFormatWriter: Finished processing stats for write job 76cbbd2b-2be7-4a86-a4e2-4ca0b8eadac4.
23/03/08 19:41:24  INFO CodeGenerator: Code generated in 18.526066 ms
23/03/08 19:41:24  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [83a2baf5-f851-45e6-8b2c-6ecfd5904eea] __MMMMMM__ Total jobs: 7 current job:7 job script:load parquet.`/tmp/__python__cache.iris_model` as iris_model 
23/03/08 19:41:24  WARN DataSource: All paths were ignored:
  file:/tmp/__python__cache.iris_model
23/03/08 19:41:24  INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.
23/03/08 19:41:24  INFO SparkContext: Starting job: load at MLSQLBaseFileSource.scala:15
23/03/08 19:41:24  INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1513
23/03/08 19:41:24  INFO FileSourceStrategy: Pushed Filters: 
23/03/08 19:41:24  INFO FileSourceStrategy: Post-Scan Filters: 
23/03/08 19:41:24  INFO FileSourceStrategy: Output Data Schema: struct<start: bigint, offset: bigint, value: binary ... 1 more fields>
23/03/08 19:41:24  INFO CodeGenerator: Code generated in 9.236354 ms
23/03/08 19:41:24  INFO SparkContext: Created broadcast 18 from rdd at ShowFileTable.scala:21
23/03/08 19:41:24  INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 19:41:24  INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1513
23/03/08 19:41:24  INFO FileScanRDD: Reading File path: file:///tmp/__python__cache.iris_model/part-00010-6096e33e-6701-49c5-9ea5-febe65d5f184-c000.snappy.parquet, range: 0-2293, partition values: [empty row]
23/03/08 19:41:24  INFO FileScanRDD: Reading File path: file:///tmp/__python__cache.iris_model/part-00015-6096e33e-6701-49c5-9ea5-febe65d5f184-c000.snappy.parquet, range: 0-2630, partition values: [empty row]
23/03/08 19:41:24  INFO FileScanRDD: Reading File path: file:///tmp/__python__cache.iris_model/part-00000-6096e33e-6701-49c5-9ea5-febe65d5f184-c000.snappy.parquet, range: 0-459, partition values: [empty row]
23/03/08 19:41:24  INFO FileScanRDD: Reading File path: file:///tmp/__python__cache.iris_model/part-00005-6096e33e-6701-49c5-9ea5-febe65d5f184-c000.snappy.parquet, range: 0-7436, partition values: [empty row]
23/03/08 19:41:24  INFO CodecPool: Got brand-new decompressor [.snappy]
23/03/08 19:41:24  INFO CodecPool: Got brand-new decompressor [.snappy]
23/03/08 19:41:24  INFO CodecPool: Got brand-new decompressor [.snappy]
23/03/08 19:41:24  INFO CodeGenerator: Code generated in 9.73454 ms
23/03/08 19:41:24  INFO CodeGenerator: Code generated in 7.252989 ms
23/03/08 19:41:24  INFO CodeGenerator: Code generated in 4.581537 ms
23/03/08 19:41:24  INFO SparkContext: Starting job: take at RestController.scala:290
23/03/08 19:41:24  INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1513
23/03/08 19:41:24  INFO CodeGenerator: Code generated in 11.412029 ms
23/03/08 19:41:24  INFO CodeGenerator: Code generated in 4.696057 ms
23/03/08 19:41:24  INFO CodeGenerator: Code generated in 4.565645 ms
23/03/08 19:41:24  INFO CodeGenerator: Code generated in 9.391833 ms
23/03/08 19:41:24  INFO impl: Completed 200 in 7257ms 	POST /run/script



23/03/08 19:42:02  INFO SparkContext: Starting job: collect at ScriptSQLExec.scala:221
23/03/08 19:42:02  INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1513
23/03/08 19:42:02  INFO DefaultConsoleClient: [owner] [admin] [groupId] [97cbdb33-6b47-4128-b505-eb2e0c228719] __MMMMMM__ auth admin  want access tables: [{"table":"command","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"0d076cc1d29c4a51b5d7a4cdc3306202","operateType":{"i":1,"name":"load"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"command","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"6c1f640eb4e24cb4b61318da1fa8c8fd","operateType":{"i":1,"name":"load"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"command","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"70d1bc8ca82743dfa7bbbdba86787c80","operateType":{"i":1,"name":"load"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"command","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"cdb76b9fb5114d76a3e55c80dead01d2","operateType":{"i":1,"name":"load"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"command","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"db":"","table":"/tmp/__python__cache.iris_model","operateType":{"i":0,"name":"save"},"sourceType":"parquet","tableType":{"name":"hdfs","includes":["rate","json","image","parquet","csv","text","binlogRate","excel","libsvm","xml","delta","streamParquet"]}},{"db":"","table":"/tmp/__python__cache.iris_model","operateType":{"i":1,"name":"load"},"sourceType":"parquet","tableType":{"name":"hdfs","includes":["rate","json","image","parquet","csv","text","binlogRate","excel","libsvm","xml","delta","streamParquet"]}},{"table":"iris_model","operateType":{"i":1,"name":"load"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}}]
23/03/08 19:42:02  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [97cbdb33-6b47-4128-b505-eb2e0c228719] __MMMMMM__ Total jobs: 7 current job:1 job script:run command as PythonCommand.`` where parameters='''["conf","schema=file"]''' as 0d076cc1d29c4a51b5d7a4cdc3306202 
23/03/08 19:42:02  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [97cbdb33-6b47-4128-b505-eb2e0c228719] __MMMMMM__ Total jobs: 7 current job:2 job script:run command as PythonCommand.`` where parameters='''["conf","dataMode=model"]''' as 6c1f640eb4e24cb4b61318da1fa8c8fd 
23/03/08 19:42:02  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [97cbdb33-6b47-4128-b505-eb2e0c228719] __MMMMMM__ Total jobs: 7 current job:3 job script:run command as PythonCommand.`` where parameters='''["conf","runIn=driver"]''' as 70d1bc8ca82743dfa7bbbdba86787c80 
23/03/08 19:42:02  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [97cbdb33-6b47-4128-b505-eb2e0c228719] __MMMMMM__ Total jobs: 7 current job:4 job script:run command as PythonCommand.`` where parameters='''["conf","pythonExec=/opt/miniconda3/envs/ray-2.3.0/bin/python"]''' as cdb76b9fb5114d76a3e55c80dead01d2 
23/03/08 19:42:02  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [97cbdb33-6b47-4128-b505-eb2e0c228719] __MMMMMM__ Total jobs: 7 current job:5 job script:run command as Ray.`` where
        inputTable="iris" and
        outputTable="iris_model_0" and
        
        
        code='''#%python
#%input=iris
#%output=iris_model
#%schema=file
#%runIn=driver
#%dataMode=model
#%cache=true
#%pythonExec=/opt/miniconda3/envs/ray-2.3.0/bin/python

from pyjava.api.mlsql import RayContext,PythonContext
import pandas as pd
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn import metrics
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
import joblib
from pyjava.storage import streaming_tar
import os

ray_context = RayContext.connect(globals(),None)
data = ray_context.to_pandas()

train,test = train_test_split(data, test_size = 0.4, stratify = data['species_id'], random_state = 42)

X_train = train[['sepal_length','sepal_width','petal_length','petal_width']]
y_train = train.species_id
X_test = test[['sepal_length','sepal_width','petal_length','petal_width']]
y_test = test.species_id

mod_dt = DecisionTreeClassifier(max_depth = 3, random_state = 1)
mod_dt.fit(X_train.values,y_train.values)
prediction=mod_dt.predict(X_test.values)
print('The accuracy of the Decision Tree is',"{:.3f}".format(metrics.accuracy_score(prediction,y_test)))

model_path = "/tmp/model"

if not os.path.exists(model_path):
    os.makedirs(model_path, exist_ok=True)
    
joblib.dump(mod_dt,os.path.join(model_path,"iris_model") )

model_binary = streaming_tar.build_rows_from_file(model_path)

ray_context.build_result(model_binary)''' 
23/03/08 19:42:03  INFO SparkContext: Starting job: collect at Ray.scala:85
23/03/08 19:42:03  INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1513
23/03/08 19:42:03  INFO SparkContext: Starting job: collect at Ray.scala:259
23/03/08 19:42:03  INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1513
23/03/08 19:42:03  INFO FileSourceStrategy: Pushed Filters: 
23/03/08 19:42:03  INFO FileSourceStrategy: Post-Scan Filters: 
23/03/08 19:42:03  INFO FileSourceStrategy: Output Data Schema: struct<sepal_length: string, sepal_width: string, petal_length: string, petal_width: string, species: string ... 1 more field>
23/03/08 19:42:03  INFO SparkContext: Created broadcast 24 from rdd at MasterSlaveInSpark.scala:72
23/03/08 19:42:03  INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 19:42:03  INFO MasterSlaveInSpark: RayMode: Resource:[16(16-0)] TargetLen:[1]
23/03/08 19:42:03  INFO FileSourceStrategy: Pushed Filters: 
23/03/08 19:42:03  INFO FileSourceStrategy: Post-Scan Filters: 
23/03/08 19:42:03  INFO FileSourceStrategy: Output Data Schema: struct<sepal_length: string, sepal_width: string, petal_length: string, petal_width: string, species: string ... 1 more field>
23/03/08 19:42:03  INFO SparkContext: Created broadcast 25 from rdd at MasterSlaveInSpark.scala:123
23/03/08 19:42:03  INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 19:42:03  INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1513
23/03/08 19:42:03  INFO FileScanRDD: Reading File path: file:///Users/allwefantasy/projects/byzer-ml-example/example-data/iris.csv, range: 0-4169, partition values: [empty row]
23/03/08 19:42:03  INFO SparkContext: Starting job: collect at MasterSlaveInSpark.scala:123
23/03/08 19:42:03  INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1513
23/03/08 19:42:03  INFO SocketServerInExecutor: Received connection fromSocket[addr=/192.168.3.13,port=51932,localport=51930]
23/03/08 19:42:03  INFO SocketServerInExecutor: 
java.net.SocketException: Socket closed
	at java.net.PlainSocketImpl.socketAccept(Native Method) ~[?:1.8.0_151]
	at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409) ~[?:1.8.0_151]
	at java.net.ServerSocket.implAccept(ServerSocket.java:545) ~[?:1.8.0_151]
	at java.net.ServerSocket.accept(ServerSocket.java:513) ~[?:1.8.0_151]
	at tech.mlsql.common.utils.distribute.socket.server.SocketServerInExecutor$$anon$2.run(ExecutorSocketServer.scala:71) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
23/03/08 19:42:03  INFO DriverLogServer: [owner] [admin] [groupId] [83a2baf5-f851-45e6-8b2c-6ecfd5904eea] __MMMMMM__ The accuracy of the Decision Tree is 0.983
23/03/08 19:42:03  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [97cbdb33-6b47-4128-b505-eb2e0c228719] __MMMMMM__ Total jobs: 7 current job:6 job script:save overwrite iris_model_0 as parquet.`/tmp/__python__cache.iris_model` 
23/03/08 19:42:03  INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:42:03  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:42:03  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:42:03  INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:42:03  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:42:03  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:42:03  INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:42:03  INFO SparkContext: Starting job: save at MLSQLBaseFileSource.scala:30
23/03/08 19:42:03  INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1513
23/03/08 19:42:03  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:42:03  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:42:03  INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:42:03  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:42:03  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:42:03  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:42:03  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:42:03  INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:42:03  INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:42:03  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:42:03  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:42:03  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:42:03  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:42:03  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:42:03  INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:42:03  INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202303081942039156375764992187235_0020_m_000001_56
23/03/08 19:42:03  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:42:03  INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202303081942034120105875055806322_0020_m_000003_58
23/03/08 19:42:03  INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:42:03  INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:42:03  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:42:03  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:42:03  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:42:03  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:42:03  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:42:03  INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:42:03  INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:42:03  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:42:03  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:42:03  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:42:03  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:42:03  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:42:03  INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:42:03  INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:42:03  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:42:03  INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:42:03  INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202303081942035105939332113984305_0020_m_000012_67
23/03/08 19:42:03  INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202303081942037281290574340099758_0020_m_000007_62
23/03/08 19:42:03  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:42:03  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:42:03  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:42:03  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:42:03  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:42:03  INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:42:03  INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:42:03  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:42:03  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:42:03  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:42:03  INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:42:03  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:42:03  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:42:03  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:42:03  INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:42:03  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:42:03  INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20230308194203525378639832725253_0020_m_000014_69
23/03/08 19:42:03  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:42:03  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:42:03  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:42:03  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:42:03  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:42:03  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:42:03  INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:42:03  INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:42:03  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:42:03  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:42:03  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:42:03  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:42:03  INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:42:03  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:42:03  INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:42:03  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:42:03  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:42:03  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:42:03  INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:42:03  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:42:03  INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:42:03  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:42:03  INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:42:03  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:42:03  INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:42:03  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:42:03  INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202303081942035861766002123969989_0020_m_000004_59
23/03/08 19:42:03  INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:42:03  INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:42:03  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:42:03  INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:42:03  INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:42:03  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:42:03  INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20230308194203213958149490804819_0020_m_000011_66
23/03/08 19:42:03  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:42:03  INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202303081942038588753437527031788_0020_m_000008_63
23/03/08 19:42:03  INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202303081942032776056691213230532_0020_m_000002_57
23/03/08 19:42:03  INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202303081942034861298388751127804_0020_m_000006_61
23/03/08 19:42:03  INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:42:03  INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202303081942039054049685834976241_0020_m_000009_64
23/03/08 19:42:03  INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "start",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "offset",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "value",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int64 start;
  optional int64 offset;
  optional binary value;
}

       
23/03/08 19:42:03  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:42:03  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:42:03  INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:42:03  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:42:03  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:42:03  INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:42:03  INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202303081942034930424522795721082_0020_m_000013_68
23/03/08 19:42:03  INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "start",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "offset",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "value",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int64 start;
  optional int64 offset;
  optional binary value;
}

       
23/03/08 19:42:03  INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "start",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "offset",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "value",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int64 start;
  optional int64 offset;
  optional binary value;
}

       
23/03/08 19:42:03  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:42:03  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:42:03  INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:42:03  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 19:42:03  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 19:42:03  INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:42:03  INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "start",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "offset",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "value",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int64 start;
  optional int64 offset;
  optional binary value;
}

       
23/03/08 19:42:03  INFO FileOutputCommitter: Saved output of task 'attempt_202303081942032144782079688429643_0020_m_000005_60' to file:/tmp/__python__cache.iris_model/_temporary/0/task_202303081942032144782079688429643_0020_m_000005
23/03/08 19:42:03  INFO SparkHadoopMapRedUtil: attempt_202303081942032144782079688429643_0020_m_000005_60: Committed. Elapsed time: 0 ms.
23/03/08 19:42:03  INFO FileOutputCommitter: Saved output of task 'attempt_20230308194203569886296505079506_0020_m_000010_65' to file:/tmp/__python__cache.iris_model/_temporary/0/task_20230308194203569886296505079506_0020_m_000010
23/03/08 19:42:03  INFO SparkHadoopMapRedUtil: attempt_20230308194203569886296505079506_0020_m_000010_65: Committed. Elapsed time: 0 ms.
23/03/08 19:42:03  INFO FileOutputCommitter: Saved output of task 'attempt_202303081942035302774932197690271_0020_m_000000_55' to file:/tmp/__python__cache.iris_model/_temporary/0/task_202303081942035302774932197690271_0020_m_000000
23/03/08 19:42:03  INFO SparkHadoopMapRedUtil: attempt_202303081942035302774932197690271_0020_m_000000_55: Committed. Elapsed time: 0 ms.
23/03/08 19:42:03  INFO FileOutputCommitter: Saved output of task 'attempt_202303081942034323127540645873486_0020_m_000015_70' to file:/tmp/__python__cache.iris_model/_temporary/0/task_202303081942034323127540645873486_0020_m_000015
23/03/08 19:42:03  INFO SparkHadoopMapRedUtil: attempt_202303081942034323127540645873486_0020_m_000015_70: Committed. Elapsed time: 0 ms.
23/03/08 19:42:03  INFO FileFormatWriter: Start to commit write Job 2263331a-b155-4ef7-be41-c37d67607147.
23/03/08 19:42:03  INFO FileFormatWriter: Write Job 2263331a-b155-4ef7-be41-c37d67607147 committed. Elapsed time: 16 ms.
23/03/08 19:42:03  INFO FileFormatWriter: Finished processing stats for write job 2263331a-b155-4ef7-be41-c37d67607147.
23/03/08 19:42:03  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [97cbdb33-6b47-4128-b505-eb2e0c228719] __MMMMMM__ Total jobs: 7 current job:7 job script:load parquet.`/tmp/__python__cache.iris_model` as iris_model 
23/03/08 19:42:03  WARN DataSource: All paths were ignored:
  file:/tmp/__python__cache.iris_model
23/03/08 19:42:03  INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
23/03/08 19:42:03  INFO SparkContext: Starting job: load at MLSQLBaseFileSource.scala:15
23/03/08 19:42:04  INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1513
23/03/08 19:42:04  INFO FileSourceStrategy: Pushed Filters: 
23/03/08 19:42:04  INFO FileSourceStrategy: Post-Scan Filters: 
23/03/08 19:42:04  INFO FileSourceStrategy: Output Data Schema: struct<start: bigint, offset: bigint, value: binary ... 1 more fields>
23/03/08 19:42:04  INFO SparkContext: Created broadcast 30 from rdd at ShowFileTable.scala:21
23/03/08 19:42:04  INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 19:42:04  INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1513
23/03/08 19:42:04  INFO FileScanRDD: Reading File path: file:///tmp/__python__cache.iris_model/part-00015-92e1063b-f4f9-4611-9b00-5bf7ef2a8721-c000.snappy.parquet, range: 0-2630, partition values: [empty row]
23/03/08 19:42:04  INFO FileScanRDD: Reading File path: file:///tmp/__python__cache.iris_model/part-00000-92e1063b-f4f9-4611-9b00-5bf7ef2a8721-c000.snappy.parquet, range: 0-459, partition values: [empty row]
23/03/08 19:42:04  INFO FileScanRDD: Reading File path: file:///tmp/__python__cache.iris_model/part-00005-92e1063b-f4f9-4611-9b00-5bf7ef2a8721-c000.snappy.parquet, range: 0-7437, partition values: [empty row]
23/03/08 19:42:04  INFO FileScanRDD: Reading File path: file:///tmp/__python__cache.iris_model/part-00010-92e1063b-f4f9-4611-9b00-5bf7ef2a8721-c000.snappy.parquet, range: 0-2293, partition values: [empty row]
23/03/08 19:42:04  INFO SparkContext: Starting job: take at RestController.scala:290
23/03/08 19:42:04  INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1513
23/03/08 19:42:04  INFO impl: Completed 200 in 1594ms 	POST /run/script



23/03/08 19:42:04  INFO MasterSlaveInSpark: Exit all data server
23/03/08 19:44:13  INFO SparkContext: Starting job: collect at ScriptSQLExec.scala:221
23/03/08 19:44:13  INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1513
23/03/08 19:44:13  INFO DefaultConsoleClient: [owner] [admin] [groupId] [a25514a5-2492-4578-933d-d5d5e2dcdc4c] __MMMMMM__ auth admin  want access tables: [{"db":"","table":"./data/ai/iris_model","operateType":{"i":0,"name":"save"},"sourceType":"delta","tableType":{"name":"hdfs","includes":["rate","json","image","parquet","csv","text","binlogRate","excel","libsvm","xml","delta","streamParquet"]}}]
23/03/08 19:44:13  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [a25514a5-2492-4578-933d-d5d5e2dcdc4c] __MMMMMM__ Total jobs: 1 current job:1 job script:save overwrite iris_model as delta.`ai.iris_model` 
23/03/08 19:44:13  INFO DelegatingLogStore: LogStore `LogStoreAdapter(io.delta.storage.HDFSLogStore)` is used for scheme `file`
23/03/08 19:44:13  INFO DeltaLog: Creating initial snapshot without metadata, because the directory is empty
23/03/08 19:44:13  INFO InitialSnapshot: [tableId=8f514610-2d26-4c9a-8cbb-a39800349784] Created snapshot InitialSnapshot(path=file:/Users/allwefantasy/projects/byzer-ml-example/data/ai/iris_model/_delta_log, version=-1, metadata=Metadata(2b272c6b-c318-455b-8fea-37ea6f41e060,null,null,Format(parquet,Map()),null,List(),Map(),Some(1678275853437)), logSegment=LogSegment(file:/Users/allwefantasy/projects/byzer-ml-example/data/ai/iris_model/_delta_log,-1,List(),List(),None,-1), checksumOpt=None)
23/03/08 19:44:13  INFO DeltaLog: No delta log found for the Delta table at file:/Users/allwefantasy/projects/byzer-ml-example/data/ai/iris_model/_delta_log
23/03/08 19:44:13  INFO InitialSnapshot: [tableId=2b272c6b-c318-455b-8fea-37ea6f41e060] Created snapshot InitialSnapshot(path=file:/Users/allwefantasy/projects/byzer-ml-example/data/ai/iris_model/_delta_log, version=-1, metadata=Metadata(ebf5189c-6571-481f-a1c6-181f44b2b597,null,null,Format(parquet,Map()),null,List(),Map(),Some(1678275853476)), logSegment=LogSegment(file:/Users/allwefantasy/projects/byzer-ml-example/data/ai/iris_model/_delta_log,-1,List(),List(),None,-1), checksumOpt=None)
23/03/08 19:44:13  INFO OptimisticTransaction: [tableId=ebf5189c,txnId=b7650d77] Updated metadata from - to Metadata(e9f38efc-0bb1-4ae2-b0d2-64e7567c7377,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"start","type":"long","nullable":true,"metadata":{}},{"name":"offset","type":"long","nullable":true,"metadata":{}},{"name":"value","type":"binary","nullable":true,"metadata":{}}]},List(),Map(path -> ./data/ai/iris_model),Some(1678275853491))
23/03/08 19:44:13  INFO FileSourceStrategy: Pushed Filters: 
23/03/08 19:44:13  INFO FileSourceStrategy: Post-Scan Filters: 
23/03/08 19:44:13  INFO FileSourceStrategy: Output Data Schema: struct<start: bigint, offset: bigint, value: binary ... 1 more fields>
23/03/08 19:44:13  INFO DeltaParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 19:44:13  INFO SparkContext: Created broadcast 34 from save at MLSQLDelta.scala:104
23/03/08 19:44:13  INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 19:44:13  INFO SparkContext: Starting job: save at MLSQLDelta.scala:104
23/03/08 19:44:13  INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1513
23/03/08 19:44:13  INFO FileScanRDD: Reading File path: file:///tmp/__python__cache.iris_model/part-00010-92e1063b-f4f9-4611-9b00-5bf7ef2a8721-c000.snappy.parquet, range: 0-2293, partition values: [empty row]
23/03/08 19:44:13  INFO FileScanRDD: Reading File path: file:///tmp/__python__cache.iris_model/part-00015-92e1063b-f4f9-4611-9b00-5bf7ef2a8721-c000.snappy.parquet, range: 0-2630, partition values: [empty row]
23/03/08 19:44:13  INFO FileScanRDD: Reading File path: file:///tmp/__python__cache.iris_model/part-00000-92e1063b-f4f9-4611-9b00-5bf7ef2a8721-c000.snappy.parquet, range: 0-459, partition values: [empty row]
23/03/08 19:44:13  INFO CodeGenerator: Code generated in 6.556693 ms
23/03/08 19:44:13  INFO CodeGenerator: Code generated in 13.855628 ms
23/03/08 19:44:13  INFO CodeGenerator: Code generated in 6.914572 ms
23/03/08 19:44:13  INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "start",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "offset",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "value",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int64 start;
  optional int64 offset;
  optional binary value;
}

       
23/03/08 19:44:13  INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "start",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "offset",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "value",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int64 start;
  optional int64 offset;
  optional binary value;
}

       
23/03/08 19:44:13  INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "start",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "offset",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "value",
    "type" : "binary",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int64 start;
  optional int64 offset;
  optional binary value;
}

       
23/03/08 19:44:13  INFO FileScanRDD: Reading File path: file:///tmp/__python__cache.iris_model/part-00005-92e1063b-f4f9-4611-9b00-5bf7ef2a8721-c000.snappy.parquet, range: 0-7437, partition values: [empty row]
23/03/08 19:44:13  INFO FileFormatWriter: Start to commit write Job 12209c1e-d135-4a48-891f-a485d98a0417.
23/03/08 19:44:13  INFO FileFormatWriter: Write Job 12209c1e-d135-4a48-891f-a485d98a0417 committed. Elapsed time: 1 ms.
23/03/08 19:44:13  INFO FileFormatWriter: Finished processing stats for write job 12209c1e-d135-4a48-891f-a485d98a0417.
23/03/08 19:44:14  INFO CodeGenerator: Code generated in 90.43859 ms
23/03/08 19:44:14  INFO SparkContext: Starting job: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:77
23/03/08 19:44:14  INFO OptimisticTransaction: [tableId=ebf5189c,txnId=b7650d77] Attempting to commit version 0 with 6 actions with Serializable isolation level
23/03/08 19:44:14  INFO DeltaLog: Loading version 0.
23/03/08 19:44:14  INFO Snapshot: [tableId=ebf5189c-6571-481f-a1c6-181f44b2b597] DELTA: Compute snapshot for version: 0
23/03/08 19:44:14  INFO SparkContext: Created broadcast 36 from $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:77
23/03/08 19:44:14  INFO DeltaLogFileIndex: Created DeltaLogFileIndex(JSON, numFilesInSegment: 1, totalFileSize: 1924)
23/03/08 19:44:14  INFO FileSourceStrategy: Pushed Filters: 
23/03/08 19:44:14  INFO FileSourceStrategy: Post-Scan Filters: 
23/03/08 19:44:14  INFO FileSourceStrategy: Output Data Schema: struct<txn: struct<appId: string, version: bigint, lastUpdated: bigint ... 1 more fields>, add: struct<path: string, partitionValues: map<string,string>, size: bigint, modificationTime: bigint, dataChange: boolean ... 5 more fields>, remove: struct<path: string, deletionTimestamp: bigint, dataChange: boolean, extendedFileMetadata: boolean, partitionValues: map<string,string> ... 5 more fields>, metaData: struct<id: string, name: string, description: string, format: struct<provider: string, options: map<string,string>>, schemaString: string ... 6 more fields>, protocol: struct<minReaderVersion: int, minWriterVersion: int> ... 5 more fields>
23/03/08 19:44:14  INFO CodeGenerator: Code generated in 55.997806 ms
23/03/08 19:44:15  INFO SparkContext: Created broadcast 37 from $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:77
23/03/08 19:44:15  INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 19:44:15  INFO SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1513
23/03/08 19:44:15  INFO CodeGenerator: Code generated in 6.740249 ms
23/03/08 19:44:15  INFO FileScanRDD: Reading File path: file:///Users/allwefantasy/projects/byzer-ml-example/data/ai/iris_model/_delta_log/00000000000000000000.json, range: 0-1924, partition values: [empty row]
23/03/08 19:44:15  INFO CodeGenerator: Code generated in 25.40002 ms
23/03/08 19:44:15  INFO CodeGenerator: Code generated in 6.630922 ms
23/03/08 19:44:15  INFO CodeGenerator: Generated method too long to be JIT compiled: org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.serializefromobject_doConsume_0$ is 14899 bytes
23/03/08 19:44:15  INFO CodeGenerator: Code generated in 137.198324 ms
23/03/08 19:44:15  INFO CodeGenerator: Code generated in 46.448565 ms
23/03/08 19:44:15  INFO CodeGenerator: Code generated in 29.663507 ms
23/03/08 19:44:15  INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1513
23/03/08 19:44:15  INFO CodeGenerator: Code generated in 6.788693 ms
23/03/08 19:44:15  INFO CodeGenerator: Code generated in 11.508985 ms
23/03/08 19:44:15  INFO CodeGenerator: Code generated in 63.18781 ms
23/03/08 19:44:16  INFO CodeGenerator: Code generated in 11.917293 ms
23/03/08 19:44:16  INFO CodeGenerator: Code generated in 15.919989 ms
23/03/08 19:44:16  INFO CodeGenerator: Code generated in 7.373268 ms
23/03/08 19:44:16  INFO CodeGenerator: Code generated in 17.826916 ms
23/03/08 19:44:16  INFO CodeGenerator: Code generated in 13.866342 ms
23/03/08 19:44:16  INFO CodeGenerator: Code generated in 9.68494 ms
23/03/08 19:44:16  INFO SparkContext: Starting job: $anonfun$recordDeltaOperationInternal$1 at DatabricksLogging.scala:77
23/03/08 19:44:16  INFO SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1513
23/03/08 19:44:16  INFO CodeGenerator: Code generated in 6.209301 ms
23/03/08 19:44:16  INFO CodeGenerator: Code generated in 13.715278 ms
23/03/08 19:44:16  INFO CodeGenerator: Code generated in 25.950385 ms
23/03/08 19:44:16  INFO Snapshot: [tableId=ebf5189c-6571-481f-a1c6-181f44b2b597] DELTA: Done
23/03/08 19:44:16  INFO Snapshot: [tableId=ebf5189c-6571-481f-a1c6-181f44b2b597] Created snapshot Snapshot(path=file:/Users/allwefantasy/projects/byzer-ml-example/data/ai/iris_model/_delta_log, version=0, metadata=Metadata(e9f38efc-0bb1-4ae2-b0d2-64e7567c7377,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"start","type":"long","nullable":true,"metadata":{}},{"name":"offset","type":"long","nullable":true,"metadata":{}},{"name":"value","type":"binary","nullable":true,"metadata":{}}]},List(),Map(path -> ./data/ai/iris_model),Some(1678275853491)), logSegment=LogSegment(file:/Users/allwefantasy/projects/byzer-ml-example/data/ai/iris_model/_delta_log,0,ArrayBuffer(SerializableFileStatus(file:/Users/allwefantasy/projects/byzer-ml-example/data/ai/iris_model/_delta_log/00000000000000000000.json,1924,false,1678275854000)),List(),None,1678275854000), checksumOpt=None)
23/03/08 19:44:16  INFO DeltaLog: Updated snapshot to Snapshot(path=file:/Users/allwefantasy/projects/byzer-ml-example/data/ai/iris_model/_delta_log, version=0, metadata=Metadata(e9f38efc-0bb1-4ae2-b0d2-64e7567c7377,null,null,Format(parquet,Map()),{"type":"struct","fields":[{"name":"start","type":"long","nullable":true,"metadata":{}},{"name":"offset","type":"long","nullable":true,"metadata":{}},{"name":"value","type":"binary","nullable":true,"metadata":{}}]},List(),Map(path -> ./data/ai/iris_model),Some(1678275853491)), logSegment=LogSegment(file:/Users/allwefantasy/projects/byzer-ml-example/data/ai/iris_model/_delta_log,0,ArrayBuffer(SerializableFileStatus(file:/Users/allwefantasy/projects/byzer-ml-example/data/ai/iris_model/_delta_log/00000000000000000000.json,1924,false,1678275854000)),List(),None,1678275854000), checksumOpt=None)
23/03/08 19:44:16  INFO OptimisticTransaction: [tableId=ebf5189c,txnId=b7650d77] Committed delta #0 to file:/Users/allwefantasy/projects/byzer-ml-example/data/ai/iris_model/_delta_log
23/03/08 19:44:16  INFO CodeGenerator: Code generated in 7.220715 ms
23/03/08 19:44:16  INFO SparkContext: Starting job: take at RestController.scala:290
23/03/08 19:44:16  INFO SparkContext: Created broadcast 41 from broadcast at DAGScheduler.scala:1513
23/03/08 19:44:16  INFO CodeGenerator: Code generated in 11.091736 ms
23/03/08 19:44:16  INFO CodeGenerator: Code generated in 27.791658 ms
23/03/08 19:44:16  INFO impl: Completed 200 in 3563ms 	POST /run/script



23/03/08 19:46:17  INFO SocketServerInExecutor: The server ServerSocket[addr=/192.168.3.13,localport=51889] is closing the socket null connection
23/03/08 19:46:17  INFO SocketServerInExecutor: The server ServerSocket[addr=/192.168.3.13,localport=51890] is closing the socket null connection
23/03/08 19:46:18  INFO SocketServerInExecutor: The server ServerSocket[addr=/192.168.3.13,localport=51891] is closing the socket null connection
23/03/08 19:46:18  INFO SocketServerInExecutor: The server ServerSocket[addr=/192.168.3.13,localport=51892] is closing the socket null connection
23/03/08 19:47:02  INFO SocketServerInExecutor: The server ServerSocket[addr=/192.168.3.13,localport=51926] is closing the socket null connection
23/03/08 19:47:02  INFO SocketServerInExecutor: The server ServerSocket[addr=/192.168.3.13,localport=51927] is closing the socket null connection
23/03/08 19:47:02  INFO SocketServerInExecutor: The server ServerSocket[addr=/192.168.3.13,localport=51928] is closing the socket null connection
23/03/08 19:47:02  INFO SocketServerInExecutor: The server ServerSocket[addr=/192.168.3.13,localport=51929] is closing the socket null connection
23/03/08 19:54:06  INFO SparkContext: Starting job: collect at ScriptSQLExec.scala:221
23/03/08 19:54:06  INFO SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:1513
23/03/08 19:54:07  INFO DefaultConsoleClient: [owner] [admin] [groupId] [0f9190bf-88de-4536-a37c-5fcece2d496d] __MMMMMM__ auth admin  want access tables: [{"table":"command","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"9532e29fd54b4bfcaec5da14e1fce7bf","operateType":{"i":1,"name":"load"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"command","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"e6fb2faefb4f4184babdcc1838aa5a4d","operateType":{"i":1,"name":"load"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"command","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"3258fe4408cc491893a9420ce140d9a3","operateType":{"i":1,"name":"load"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"command","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"0988aa02530a4b319ec2b1b2116aa771","operateType":{"i":1,"name":"load"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"command","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"3acb3624023c4d4f97033bfc958f7e9c","operateType":{"i":1,"name":"load"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}}]
23/03/08 19:54:07  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [0f9190bf-88de-4536-a37c-5fcece2d496d] __MMMMMM__ Total jobs: 6 current job:1 job script:run command as PythonCommand.`` where parameters='''["conf","rayAddress=127.0.0.1:10001"]''' as 9532e29fd54b4bfcaec5da14e1fce7bf 
23/03/08 19:54:07  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [0f9190bf-88de-4536-a37c-5fcece2d496d] __MMMMMM__ Total jobs: 6 current job:2 job script:run command as PythonCommand.`` where parameters='''["conf","schema=file"]''' as e6fb2faefb4f4184babdcc1838aa5a4d 
23/03/08 19:54:07  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [0f9190bf-88de-4536-a37c-5fcece2d496d] __MMMMMM__ Total jobs: 6 current job:3 job script:run command as PythonCommand.`` where parameters='''["env","PYTHON_ENV=source /opt/miniconda3/bin/activate ray-2.3.0 && export OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES"]''' as 3258fe4408cc491893a9420ce140d9a3 
23/03/08 19:54:07  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [0f9190bf-88de-4536-a37c-5fcece2d496d] __MMMMMM__ Total jobs: 6 current job:4 job script:run command as PythonCommand.`` where parameters='''["conf","dataMode=model"]''' as 0988aa02530a4b319ec2b1b2116aa771 
23/03/08 19:54:07  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [0f9190bf-88de-4536-a37c-5fcece2d496d] __MMMMMM__ Total jobs: 6 current job:5 job script:run command as PythonCommand.`` where parameters='''["conf","runIn=driver"]''' as 3acb3624023c4d4f97033bfc958f7e9c 
23/03/08 19:54:07  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [0f9190bf-88de-4536-a37c-5fcece2d496d] __MMMMMM__ Total jobs: 6 current job:6 job script:register Ray.`iris_model` as iris_model_predict where 
maxConcurrency="2"
and debugMode="true"
and registerCode='''

import ray
import numpy as np
from pyjava.api.mlsql import RayContext
from pyjava.udf import UDFMaster,UDFWorker,UDFBuilder,UDFBuildInFunc

from typing import Any, NoReturn, Callable, Dict, List
import time
from ray.util.client.common import ClientActorHandle, ClientObjectRef

from pyjava.storage import streaming_tar
import uuid

import joblib
import os


ray_context = RayContext.connect(globals(), context.conf["rayAddress"])

def init_sklearn(model_refs: List[ClientObjectRef], conf: Dict[str, str]) -> Any:
    model_path = "/tmp/model/{}".format(str(uuid.uuid4()))
    streaming_tar.save_rows_as_file((ray.get(ref) for ref in model_refs), model_path)
    return joblib.load(os.path.join(model_path,"iris_model"))

def predict_func(model,v):
    prediction=model.predict(v)
    return {"value":[[float(item) for item in prediction]]}


UDFBuilder.build(ray_context,init_sklearn,predict_func)

''' 
23/03/08 19:54:07  INFO SparkContext: Starting job: collect at Ray.scala:85
23/03/08 19:54:07  INFO SparkContext: Created broadcast 43 from broadcast at DAGScheduler.scala:1513
23/03/08 19:54:07  INFO SparkContext: Starting job: collect at Ray.scala:259
23/03/08 19:54:07  INFO SparkContext: Created broadcast 44 from broadcast at DAGScheduler.scala:1513
23/03/08 19:54:07  INFO FileSourceStrategy: Pushed Filters: 
23/03/08 19:54:07  INFO FileSourceStrategy: Post-Scan Filters: 
23/03/08 19:54:07  INFO FileSourceStrategy: Output Data Schema: struct<start: bigint, offset: bigint, value: binary ... 1 more fields>
23/03/08 19:54:07  INFO SparkContext: Created broadcast 45 from rdd at MasterSlaveInSpark.scala:72
23/03/08 19:54:07  INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 19:54:07  INFO FileSourceStrategy: Pushed Filters: 
23/03/08 19:54:07  INFO FileSourceStrategy: Post-Scan Filters: 
23/03/08 19:54:07  INFO FileSourceStrategy: Output Data Schema: struct<start: bigint, offset: bigint, value: binary ... 1 more fields>
23/03/08 19:54:07  INFO SparkContext: Created broadcast 46 from rdd at MasterSlaveInSpark.scala:123
23/03/08 19:54:07  INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 19:54:07  INFO SparkContext: Created broadcast 47 from broadcast at DAGScheduler.scala:1513
23/03/08 19:54:07  INFO FileScanRDD: Reading File path: file:///tmp/__python__cache.iris_model/part-00005-92e1063b-f4f9-4611-9b00-5bf7ef2a8721-c000.snappy.parquet, range: 0-7437, partition values: [empty row]
23/03/08 19:54:07  INFO FileScanRDD: Reading File path: file:///tmp/__python__cache.iris_model/part-00015-92e1063b-f4f9-4611-9b00-5bf7ef2a8721-c000.snappy.parquet, range: 0-2630, partition values: [empty row]
23/03/08 19:54:07  INFO FileScanRDD: Reading File path: file:///tmp/__python__cache.iris_model/part-00000-92e1063b-f4f9-4611-9b00-5bf7ef2a8721-c000.snappy.parquet, range: 0-459, partition values: [empty row]
23/03/08 19:54:07  INFO FileScanRDD: Reading File path: file:///tmp/__python__cache.iris_model/part-00010-92e1063b-f4f9-4611-9b00-5bf7ef2a8721-c000.snappy.parquet, range: 0-2293, partition values: [empty row]
23/03/08 19:54:07  INFO SparkContext: Starting job: collect at MasterSlaveInSpark.scala:123
23/03/08 19:54:07  INFO SparkContext: Created broadcast 48 from broadcast at DAGScheduler.scala:1513
23/03/08 19:54:07  INFO SocketServerInExecutor: Received connection fromSocket[addr=/192.168.3.13,port=52390,localport=52388]
23/03/08 19:54:07  INFO SocketServerInExecutor: 
java.net.SocketException: Socket closed
	at java.net.PlainSocketImpl.socketAccept(Native Method) ~[?:1.8.0_151]
	at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409) ~[?:1.8.0_151]
	at java.net.ServerSocket.implAccept(ServerSocket.java:545) ~[?:1.8.0_151]
	at java.net.ServerSocket.accept(ServerSocket.java:513) ~[?:1.8.0_151]
	at tech.mlsql.common.utils.distribute.socket.server.SocketServerInExecutor$$anon$2.run(ExecutorSocketServer.scala:71) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
23/03/08 19:54:07  INFO CodeGenerator: Code generated in 8.236839 ms
23/03/08 19:54:07  INFO MasterSlaveInSpark: RayMode: Resource:[16(16-0)] TargetLen:[1]
23/03/08 19:54:07  INFO SparkContext: Created broadcast 49 from broadcast at DAGScheduler.scala:1513
23/03/08 19:54:08  INFO SparkContext: Starting job: collect at MasterSlaveInSpark.scala:123
23/03/08 19:54:08  INFO SparkContext: Created broadcast 50 from broadcast at DAGScheduler.scala:1513
23/03/08 19:54:08  INFO CodeGenerator: Code generated in 4.999975 ms
23/03/08 19:54:08  INFO SocketServerInExecutor: Received connection fromSocket[addr=/192.168.3.13,port=52393,localport=52391]
23/03/08 19:54:08  INFO SocketServerInExecutor: 
java.net.SocketException: Socket closed
	at java.net.PlainSocketImpl.socketAccept(Native Method) ~[?:1.8.0_151]
	at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409) ~[?:1.8.0_151]
	at java.net.ServerSocket.implAccept(ServerSocket.java:545) ~[?:1.8.0_151]
	at java.net.ServerSocket.accept(ServerSocket.java:513) ~[?:1.8.0_151]
	at tech.mlsql.common.utils.distribute.socket.server.SocketServerInExecutor$$anon$2.run(ExecutorSocketServer.scala:71) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
23/03/08 19:54:40  INFO DriverLogServer: [owner] [admin] [groupId] [0f9190bf-88de-4536-a37c-5fcece2d496d] __MMMMMM__ /opt/miniconda3/envs/ray-2.3.0/lib/python3.10/site-packages/ray/util/client/worker.py:253: UserWarning: Ray Client connection timed out. Ensure that the Ray Client port on the head node is reachable from your local machine. See https://docs.ray.io/en/latest/cluster/ray-client.html#step-2-check-ports for more information.
23/03/08 19:54:40  INFO DriverLogServer: [owner] [admin] [groupId] [0f9190bf-88de-4536-a37c-5fcece2d496d] __MMMMMM__   warnings.warn(
23/03/08 19:54:40  ERROR RestController: An error occurred while the job manager was executing the task, 
org.apache.spark.sql.mlsql.session.MLSQLException: An exception was encountered in the execution of this python task! Please check your code and try again after modification.
	at tech.mlsql.ets.Ray.distribute_execute(Ray.scala:186) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at tech.mlsql.ets.Ray._load$1(Ray.scala:303) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at tech.mlsql.ets.Ray.predict(Ray.scala:306) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at tech.mlsql.dsl.adaptor.RegisterAdaptor.parse(RegisterAdaptor.scala:71) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at streaming.dsl.ScriptSQLExecListener.execute$1(ScriptSQLExec.scala:406) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at streaming.dsl.ScriptSQLExecListener.exitSql(ScriptSQLExec.scala:448) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at streaming.dsl.parser.DSLSQLParser$SqlContext.exitRule(DSLSQLParser.java:296) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.antlr.v4.runtime.tree.ParseTreeWalker.exitRule(ParseTreeWalker.java:47) ~[antlr4-runtime-4.8.jar:4.8]
	at org.antlr.v4.runtime.tree.ParseTreeWalker.walk(ParseTreeWalker.java:30) ~[antlr4-runtime-4.8.jar:4.8]
	at org.antlr.v4.runtime.tree.ParseTreeWalker.walk(ParseTreeWalker.java:28) ~[antlr4-runtime-4.8.jar:4.8]
	at streaming.dsl.ScriptSQLExec$._parse(ScriptSQLExec.scala:159) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at streaming.dsl.ScriptSQLExec$.parse(ScriptSQLExec.scala:146) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at streaming.rest.RestController.$anonfun$script$9(RestController.scala:201) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at tech.mlsql.job.JobManager$.run(JobManager.scala:74) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at streaming.rest.RestController.query$1(RestController.scala:196) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at streaming.rest.RestController.script(RestController.scala:223) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_151]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_151]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_151]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_151]
	at net.csdn.modules.http.RestController.filter(RestController.java:139) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at net.csdn.modules.http.RestController.dispatchRequest(RestController.java:99) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at net.csdn.modules.http.HttpServer$DefaultHandler.handle(HttpServer.java:182) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:52) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:97) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.server.Server.handle(Server.java:499) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:311) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:257) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.io.AbstractConnection$2.run(AbstractConnection.java:544) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_151]
Caused by: org.apache.spark.SparkException: Traceback (most recent call last):
  File "/opt/miniconda3/envs/ray-2.3.0/lib/python3.10/site-packages/pyjava/worker.py", line 155, in main
    process()
  File "/opt/miniconda3/envs/ray-2.3.0/lib/python3.10/site-packages/pyjava/worker.py", line 132, in process
    exec(code, n_local, n_local)
  File "<string>", line 19, in <module>
  File "/opt/miniconda3/envs/ray-2.3.0/lib/python3.10/site-packages/pyjava/api/mlsql.py", line 266, in connect
    ray.init(url, **kwargs)
  File "/opt/miniconda3/envs/ray-2.3.0/lib/python3.10/site-packages/pyjava/rayfix.py", line 33, in init
    ray.util.connect(conn_str=address, namespace="default", **kwargs)
  File "/opt/miniconda3/envs/ray-2.3.0/lib/python3.10/site-packages/ray/util/client_connect.py", line 57, in connect
    conn = ray.connect(
  File "/opt/miniconda3/envs/ray-2.3.0/lib/python3.10/site-packages/ray/util/client/__init__.py", line 252, in connect
    conn = self.get_context().connect(*args, **kw_args)
  File "/opt/miniconda3/envs/ray-2.3.0/lib/python3.10/site-packages/ray/util/client/__init__.py", line 94, in connect
    self.client_worker = Worker(
  File "/opt/miniconda3/envs/ray-2.3.0/lib/python3.10/site-packages/ray/util/client/worker.py", line 139, in __init__
    self._connect_channel()
  File "/opt/miniconda3/envs/ray-2.3.0/lib/python3.10/site-packages/ray/util/client/worker.py", line 260, in _connect_channel
    raise ConnectionError("ray client connection timeout")
ConnectionError: ray client connection timeout

	at tech.mlsql.arrow.python.runner.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:324) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at tech.mlsql.arrow.python.runner.ArrowPythonRunner$$anon$2.read(ArrowPythonRunner.scala:154) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at tech.mlsql.arrow.python.runner.ArrowPythonRunner$$anon$2.read(ArrowPythonRunner.scala:151) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at tech.mlsql.arrow.python.runner.ArrowPythonRunner$$anon$2.read(ArrowPythonRunner.scala:102) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at tech.mlsql.arrow.python.runner.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:293) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at tech.mlsql.arrow.python.runner.InterruptibleIterator.hasNext(PythonRunner.scala:415) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491) ~[scala-library-2.12.15.jar:?]
	at scala.collection.Iterator.foreach(Iterator.scala:943) ~[scala-library-2.12.15.jar:?]
	at scala.collection.Iterator.foreach$(Iterator.scala:943) ~[scala-library-2.12.15.jar:?]
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431) ~[scala-library-2.12.15.jar:?]
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62) ~[scala-library-2.12.15.jar:?]
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53) ~[scala-library-2.12.15.jar:?]
	at scala.collection.mutable.ListBuffer.$plus$plus$eq(ListBuffer.scala:184) ~[scala-library-2.12.15.jar:?]
	at scala.collection.mutable.ListBuffer.$plus$plus$eq(ListBuffer.scala:47) ~[scala-library-2.12.15.jar:?]
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:366) ~[scala-library-2.12.15.jar:?]
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:364) ~[scala-library-2.12.15.jar:?]
	at scala.collection.AbstractIterator.to(Iterator.scala:1431) ~[scala-library-2.12.15.jar:?]
	at scala.collection.TraversableOnce.toList(TraversableOnce.scala:350) ~[scala-library-2.12.15.jar:?]
	at scala.collection.TraversableOnce.toList$(TraversableOnce.scala:350) ~[scala-library-2.12.15.jar:?]
	at scala.collection.AbstractIterator.toList(Iterator.scala:1431) ~[scala-library-2.12.15.jar:?]
	at tech.mlsql.ets.Ray.distribute_execute(Ray.scala:176) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	... 31 more
23/03/08 19:54:40  INFO impl: Completed 500 in 33496ms 	POST /run/script



23/03/08 19:55:04  INFO SparkContext: Starting job: collect at ScriptSQLExec.scala:221
23/03/08 19:55:04  INFO SparkContext: Created broadcast 51 from broadcast at DAGScheduler.scala:1513
23/03/08 19:55:04  INFO DefaultConsoleClient: [owner] [admin] [groupId] [b1d13e0c-3300-4315-84a2-a9b1653e1b59] __MMMMMM__ auth admin  want access tables: [{"table":"command","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"e1c37eb7dfdc44db9a1773f56cc0538c","operateType":{"i":1,"name":"load"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"command","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"d7176240eb57432fa5019be596bb0290","operateType":{"i":1,"name":"load"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"command","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"4ae5cc9e87d74864965e1a84b96507d2","operateType":{"i":1,"name":"load"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"command","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"7edc9aba01f24213a6f008ca21ea06e1","operateType":{"i":1,"name":"load"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"command","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"3cabd655afc24946b40a12d191dc0ccb","operateType":{"i":1,"name":"load"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}}]
23/03/08 19:55:04  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [b1d13e0c-3300-4315-84a2-a9b1653e1b59] __MMMMMM__ Total jobs: 6 current job:1 job script:run command as PythonCommand.`` where parameters='''["conf","rayAddress=127.0.0.1:10001"]''' as e1c37eb7dfdc44db9a1773f56cc0538c 
23/03/08 19:55:04  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [b1d13e0c-3300-4315-84a2-a9b1653e1b59] __MMMMMM__ Total jobs: 6 current job:2 job script:run command as PythonCommand.`` where parameters='''["conf","schema=file"]''' as d7176240eb57432fa5019be596bb0290 
23/03/08 19:55:04  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [b1d13e0c-3300-4315-84a2-a9b1653e1b59] __MMMMMM__ Total jobs: 6 current job:3 job script:run command as PythonCommand.`` where parameters='''["env","PYTHON_ENV=source /opt/miniconda3/bin/activate ray-2.3.0 && export OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES"]''' as 4ae5cc9e87d74864965e1a84b96507d2 
23/03/08 19:55:04  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [b1d13e0c-3300-4315-84a2-a9b1653e1b59] __MMMMMM__ Total jobs: 6 current job:4 job script:run command as PythonCommand.`` where parameters='''["conf","dataMode=model"]''' as 7edc9aba01f24213a6f008ca21ea06e1 
23/03/08 19:55:04  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [b1d13e0c-3300-4315-84a2-a9b1653e1b59] __MMMMMM__ Total jobs: 6 current job:5 job script:run command as PythonCommand.`` where parameters='''["conf","runIn=driver"]''' as 3cabd655afc24946b40a12d191dc0ccb 
23/03/08 19:55:04  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [b1d13e0c-3300-4315-84a2-a9b1653e1b59] __MMMMMM__ Total jobs: 6 current job:6 job script:register Ray.`iris_model` as iris_model_predict where 
maxConcurrency="2"
and debugMode="true"
and registerCode='''

import ray
import numpy as np
from pyjava.api.mlsql import RayContext
from pyjava.udf import UDFMaster,UDFWorker,UDFBuilder,UDFBuildInFunc

from typing import Any, NoReturn, Callable, Dict, List
import time
from ray.util.client.common import ClientActorHandle, ClientObjectRef

from pyjava.storage import streaming_tar
import uuid

import joblib
import os


ray_context = RayContext.connect(globals(), context.conf["rayAddress"])

def init_sklearn(model_refs: List[ClientObjectRef], conf: Dict[str, str]) -> Any:
    model_path = "/tmp/model/{}".format(str(uuid.uuid4()))
    streaming_tar.save_rows_as_file((ray.get(ref) for ref in model_refs), model_path)
    return joblib.load(os.path.join(model_path,"iris_model"))

def predict_func(model,v):
    prediction=model.predict(v)
    return {"value":[[float(item) for item in prediction]]}


UDFBuilder.build(ray_context,init_sklearn,predict_func)

''' 
23/03/08 19:55:04  INFO SparkContext: Starting job: collect at Ray.scala:85
23/03/08 19:55:04  INFO SparkContext: Created broadcast 52 from broadcast at DAGScheduler.scala:1513
23/03/08 19:55:04  INFO SparkContext: Starting job: collect at Ray.scala:259
23/03/08 19:55:04  INFO SparkContext: Created broadcast 53 from broadcast at DAGScheduler.scala:1513
23/03/08 19:55:04  INFO FileSourceStrategy: Pushed Filters: 
23/03/08 19:55:04  INFO FileSourceStrategy: Post-Scan Filters: 
23/03/08 19:55:04  INFO FileSourceStrategy: Output Data Schema: struct<start: bigint, offset: bigint, value: binary ... 1 more fields>
23/03/08 19:55:04  INFO SparkContext: Created broadcast 54 from rdd at MasterSlaveInSpark.scala:72
23/03/08 19:55:04  INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 19:55:04  INFO FileSourceStrategy: Pushed Filters: 
23/03/08 19:55:04  INFO FileSourceStrategy: Post-Scan Filters: 
23/03/08 19:55:04  INFO FileSourceStrategy: Output Data Schema: struct<start: bigint, offset: bigint, value: binary ... 1 more fields>
23/03/08 19:55:04  INFO SparkContext: Created broadcast 55 from rdd at MasterSlaveInSpark.scala:123
23/03/08 19:55:04  INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 19:55:04  INFO SparkContext: Created broadcast 56 from broadcast at DAGScheduler.scala:1513
23/03/08 19:55:04  INFO FileScanRDD: Reading File path: file:///tmp/__python__cache.iris_model/part-00005-92e1063b-f4f9-4611-9b00-5bf7ef2a8721-c000.snappy.parquet, range: 0-7437, partition values: [empty row]
23/03/08 19:55:04  INFO FileScanRDD: Reading File path: file:///tmp/__python__cache.iris_model/part-00015-92e1063b-f4f9-4611-9b00-5bf7ef2a8721-c000.snappy.parquet, range: 0-2630, partition values: [empty row]
23/03/08 19:55:04  INFO FileScanRDD: Reading File path: file:///tmp/__python__cache.iris_model/part-00000-92e1063b-f4f9-4611-9b00-5bf7ef2a8721-c000.snappy.parquet, range: 0-459, partition values: [empty row]
23/03/08 19:55:04  INFO FileScanRDD: Reading File path: file:///tmp/__python__cache.iris_model/part-00010-92e1063b-f4f9-4611-9b00-5bf7ef2a8721-c000.snappy.parquet, range: 0-2293, partition values: [empty row]
23/03/08 19:55:04  INFO SparkContext: Starting job: collect at MasterSlaveInSpark.scala:123
23/03/08 19:55:04  INFO SparkContext: Created broadcast 57 from broadcast at DAGScheduler.scala:1513
23/03/08 19:55:04  INFO SocketServerInExecutor: Received connection fromSocket[addr=/192.168.3.13,port=52485,localport=52483]
23/03/08 19:55:05  INFO SocketServerInExecutor: 
java.net.SocketException: Socket closed
	at java.net.PlainSocketImpl.socketAccept(Native Method) ~[?:1.8.0_151]
	at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409) ~[?:1.8.0_151]
	at java.net.ServerSocket.implAccept(ServerSocket.java:545) ~[?:1.8.0_151]
	at java.net.ServerSocket.accept(ServerSocket.java:513) ~[?:1.8.0_151]
	at tech.mlsql.common.utils.distribute.socket.server.SocketServerInExecutor$$anon$2.run(ExecutorSocketServer.scala:71) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
23/03/08 19:55:05  INFO MasterSlaveInSpark: RayMode: Resource:[14(16-2)] TargetLen:[1]
23/03/08 19:55:05  INFO SparkContext: Created broadcast 58 from broadcast at DAGScheduler.scala:1513
23/03/08 19:55:05  INFO SparkContext: Starting job: collect at MasterSlaveInSpark.scala:123
23/03/08 19:55:05  INFO SparkContext: Created broadcast 59 from broadcast at DAGScheduler.scala:1513
23/03/08 19:55:05  INFO SocketServerInExecutor: Received connection fromSocket[addr=/192.168.3.13,port=52488,localport=52486]
23/03/08 19:55:05  INFO SocketServerInExecutor: 
java.net.SocketException: Socket closed
	at java.net.PlainSocketImpl.socketAccept(Native Method) ~[?:1.8.0_151]
	at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409) ~[?:1.8.0_151]
	at java.net.ServerSocket.implAccept(ServerSocket.java:545) ~[?:1.8.0_151]
	at java.net.ServerSocket.accept(ServerSocket.java:513) ~[?:1.8.0_151]
	at tech.mlsql.common.utils.distribute.socket.server.SocketServerInExecutor$$anon$2.run(ExecutorSocketServer.scala:71) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
23/03/08 19:55:07  INFO DriverLogServer: [owner] [admin] [groupId] [0f9190bf-88de-4536-a37c-5fcece2d496d] __MMMMMM__ File descriptor limit 2560 is too low for production servers and may result in connection errors. At least 8192 is recommended. --- Fix with 'ulimit -n 8192'
23/03/08 19:55:07  INFO DriverLogServer: [owner] [admin] [groupId] [0f9190bf-88de-4536-a37c-5fcece2d496d] __MMMMMM__ Failed to look up actor with name 'iris_model_predict'. This could because 1. You are trying to look up a named actor you didn't create. 2. The named actor died. 3. You did not use a namespace matching the namespace of the actor.
23/03/08 19:55:08  INFO SparkContext: Starting job: collect at Ray.scala:323
23/03/08 19:55:08  INFO SparkContext: Created broadcast 60 from broadcast at DAGScheduler.scala:1513
23/03/08 19:55:08  INFO SparkContext: Starting job: collect at Ray.scala:259
23/03/08 19:55:08  INFO SparkContext: Created broadcast 61 from broadcast at DAGScheduler.scala:1513
23/03/08 19:55:08  INFO impl: Completed 200 in 4173ms 	POST /run/script



23/03/08 19:55:09  INFO MasterSlaveInSpark: Exit all data server
23/03/08 19:55:13  INFO SparkContext: Starting job: collect at ScriptSQLExec.scala:221
23/03/08 19:55:13  INFO SparkContext: Created broadcast 62 from broadcast at DAGScheduler.scala:1513
23/03/08 19:55:13  INFO DefaultConsoleClient: [owner] [admin] [groupId] [6571def5-9dac-45db-befc-6fee91bfe512] __MMMMMM__ auth admin  want access tables: [{"table":"iris","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"testTable","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"output","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}}]
23/03/08 19:55:14  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [6571def5-9dac-45db-befc-6fee91bfe512] __MMMMMM__ Total jobs: 1 current job:1 job script:select iris_model_predict(array(feature)) as predicted from testTable as output 
23/03/08 19:55:14  ERROR RestController: An error occurred while the job manager was executing the task, 
org.apache.spark.sql.AnalysisException: Cannot up cast array element from "STRING" to "DOUBLE".
The type path of the target object is:
- array element class: "scala.Double"
- array element class: "scala.collection.Seq"
- root class: "scala.collection.Seq"
You can either add an explicit cast to the input data or choose a higher precision type of the field in the target object
	at org.apache.spark.sql.errors.QueryCompilationErrors$.upCastFailureError(QueryCompilationErrors.scala:171) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveUpCast$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveUpCast$$fail(Analyzer.scala:3621) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveUpCast$$anonfun$apply$60$$anonfun$applyOrElse$183.applyOrElse(Analyzer.scala:3652) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveUpCast$$anonfun$apply$60$$anonfun$applyOrElse$183.applyOrElse(Analyzer.scala:3629) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:589) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1228) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1227) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:513) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:589) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:589) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.trees.TernaryLike.mapChildren(TreeNode.scala:1287) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.trees.TernaryLike.mapChildren$(TreeNode.scala:1284) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.expressions.objects.MapObjects.mapChildren(objects.scala:812) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:589) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:589) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.trees.TernaryLike.mapChildren(TreeNode.scala:1287) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.trees.TernaryLike.mapChildren$(TreeNode.scala:1284) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.expressions.objects.MapObjects.mapChildren(objects.scala:812) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:589) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsDownWithPruning$1(QueryPlan.scala:159) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:200) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:200) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:211) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:221) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:427) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:221) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDownWithPruning(QueryPlan.scala:159) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsWithPruning(QueryPlan.scala:130) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveUpCast$$anonfun$apply$60.applyOrElse(Analyzer.scala:3629) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveUpCast$$anonfun$apply$60.applyOrElse(Analyzer.scala:3625) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveUpCast$.apply(Analyzer.scala:3625) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveUpCast$.apply(Analyzer.scala:3615) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:211) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126) ~[scala-library-2.12.15.jar:?]
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122) ~[scala-library-2.12.15.jar:?]
	at scala.collection.immutable.List.foldLeft(List.scala:91) ~[scala-library-2.12.15.jar:?]
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:208) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:200) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at scala.collection.immutable.List.foreach(List.scala:431) ~[scala-library-2.12.15.jar:?]
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:200) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:227) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:223) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:172) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:223) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder.resolveAndBind(ExpressionEncoder.scala:345) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveEncodersInUDF$$anonfun$apply$45$$anonfun$applyOrElse$166.$anonfun$applyOrElse$168(Analyzer.scala:3259) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at scala.Option.map(Option.scala:230) ~[scala-library-2.12.15.jar:?]
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveEncodersInUDF$$anonfun$apply$45$$anonfun$applyOrElse$166.$anonfun$applyOrElse$167(Analyzer.scala:3251) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at scala.collection.immutable.List.map(List.scala:293) ~[scala-library-2.12.15.jar:?]
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveEncodersInUDF$$anonfun$apply$45$$anonfun$applyOrElse$166.applyOrElse(Analyzer.scala:3249) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveEncodersInUDF$$anonfun$apply$45$$anonfun$applyOrElse$166.applyOrElse(Analyzer.scala:3246) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$2(TreeNode.scala:638) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:638) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$1(TreeNode.scala:635) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1228) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1227) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:513) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:635) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUpWithPruning$1(QueryPlan.scala:188) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:200) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:200) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:211) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(QueryPlan.scala:216) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286) ~[scala-library-2.12.15.jar:?]
	at scala.collection.immutable.List.foreach(List.scala:431) ~[scala-library-2.12.15.jar:?]
	at scala.collection.TraversableLike.map(TraversableLike.scala:286) ~[scala-library-2.12.15.jar:?]
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279) ~[scala-library-2.12.15.jar:?]
	at scala.collection.immutable.List.map(List.scala:305) ~[scala-library-2.12.15.jar:?]
	at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:216) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:221) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:427) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:221) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUpWithPruning(QueryPlan.scala:188) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveEncodersInUDF$$anonfun$apply$45.applyOrElse(Analyzer.scala:3246) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveEncodersInUDF$$anonfun$apply$45.applyOrElse(Analyzer.scala:3243) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveEncodersInUDF$.apply(Analyzer.scala:3243) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveEncodersInUDF$.apply(Analyzer.scala:3241) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:211) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60) ~[scala-library-2.12.15.jar:?]
	at scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68) ~[scala-library-2.12.15.jar:?]
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38) ~[scala-library-2.12.15.jar:?]
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:208) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:200) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at scala.collection.immutable.List.foreach(List.scala:431) ~[scala-library-2.12.15.jar:?]
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:200) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:227) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:223) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:172) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:223) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:187) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:179) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:179) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:208) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:207) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76) ~[spark-sql_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185) ~[spark-sql_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510) ~[spark-sql_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185) ~[spark-sql_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779) ~[spark-sql_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184) ~[spark-sql_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76) ~[spark-sql_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74) ~[spark-sql_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66) ~[spark-sql_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99) ~[spark-sql_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779) ~[spark-sql_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97) ~[spark-sql_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622) ~[spark-sql_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779) ~[spark-sql_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617) ~[spark-sql_2.12-3.3.0.jar:3.3.0]
	at tech.mlsql.dsl.adaptor.SelectAdaptor.parse(SelectAdaptor.scala:73) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at streaming.dsl.ScriptSQLExecListener.execute$1(ScriptSQLExec.scala:406) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at streaming.dsl.ScriptSQLExecListener.exitSql(ScriptSQLExec.scala:420) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at streaming.dsl.parser.DSLSQLParser$SqlContext.exitRule(DSLSQLParser.java:296) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.antlr.v4.runtime.tree.ParseTreeWalker.exitRule(ParseTreeWalker.java:47) ~[antlr4-runtime-4.8.jar:4.8]
	at org.antlr.v4.runtime.tree.ParseTreeWalker.walk(ParseTreeWalker.java:30) ~[antlr4-runtime-4.8.jar:4.8]
	at org.antlr.v4.runtime.tree.ParseTreeWalker.walk(ParseTreeWalker.java:28) ~[antlr4-runtime-4.8.jar:4.8]
	at streaming.dsl.ScriptSQLExec$._parse(ScriptSQLExec.scala:159) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at streaming.dsl.ScriptSQLExec$.parse(ScriptSQLExec.scala:146) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at streaming.rest.RestController.$anonfun$script$9(RestController.scala:201) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at tech.mlsql.job.JobManager$.run(JobManager.scala:74) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at streaming.rest.RestController.query$1(RestController.scala:196) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at streaming.rest.RestController.script(RestController.scala:223) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_151]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_151]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_151]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_151]
	at net.csdn.modules.http.RestController.filter(RestController.java:139) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at net.csdn.modules.http.RestController.dispatchRequest(RestController.java:99) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at net.csdn.modules.http.HttpServer$DefaultHandler.handle(HttpServer.java:182) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:52) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:97) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.server.Server.handle(Server.java:499) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:311) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:257) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.io.AbstractConnection$2.run(AbstractConnection.java:544) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_151]
23/03/08 19:55:14  INFO impl: Completed 500 in 333ms 	POST /run/script



23/03/08 19:56:00  INFO SparkContext: Starting job: collect at ScriptSQLExec.scala:221
23/03/08 19:56:00  INFO SparkContext: Created broadcast 63 from broadcast at DAGScheduler.scala:1513
23/03/08 19:56:00  INFO DefaultConsoleClient: [owner] [admin] [groupId] [1dce148d-b53e-4f7c-a4e6-41984ebe997f] __MMMMMM__ auth admin  want access tables: [{"db":"","table":"./example-data/iris.csv","operateType":{"i":1,"name":"load"},"sourceType":"csv","tableType":{"name":"hdfs","includes":["rate","json","image","parquet","csv","text","binlogRate","excel","libsvm","xml","delta","streamParquet"]}},{"table":"iris","operateType":{"i":1,"name":"load"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}}]
23/03/08 19:56:00  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [1dce148d-b53e-4f7c-a4e6-41984ebe997f] __MMMMMM__ Total jobs: 1 current job:1 job script:load csv.`./example-data/iris.csv` where header = "true"
and inferSchema = "true" 
as iris 
23/03/08 19:56:00  INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
23/03/08 19:56:00  INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
23/03/08 19:56:00  INFO FileSourceStrategy: Pushed Filters: 
23/03/08 19:56:00  INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#2978, None)) > 0)
23/03/08 19:56:00  INFO FileSourceStrategy: Output Data Schema: struct<value: string>
23/03/08 19:56:00  INFO SparkContext: Created broadcast 64 from load at MLSQLCSV.scala:65
23/03/08 19:56:00  INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 19:56:00  INFO SparkContext: Starting job: load at MLSQLCSV.scala:65
23/03/08 19:56:00  INFO SparkContext: Created broadcast 65 from broadcast at DAGScheduler.scala:1513
23/03/08 19:56:00  INFO FileScanRDD: Reading File path: file:///Users/allwefantasy/projects/byzer-ml-example/example-data/iris.csv, range: 0-4169, partition values: [empty row]
23/03/08 19:56:00  INFO FileSourceStrategy: Pushed Filters: 
23/03/08 19:56:00  INFO FileSourceStrategy: Post-Scan Filters: 
23/03/08 19:56:00  INFO FileSourceStrategy: Output Data Schema: struct<value: string>
23/03/08 19:56:01  INFO SparkContext: Created broadcast 66 from load at MLSQLCSV.scala:65
23/03/08 19:56:01  INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 19:56:01  INFO SparkContext: Starting job: load at MLSQLCSV.scala:65
23/03/08 19:56:01  INFO SparkContext: Created broadcast 67 from broadcast at DAGScheduler.scala:1513
23/03/08 19:56:01  INFO FileScanRDD: Reading File path: file:///Users/allwefantasy/projects/byzer-ml-example/example-data/iris.csv, range: 0-4169, partition values: [empty row]
23/03/08 19:56:01  INFO FileSourceStrategy: Pushed Filters: 
23/03/08 19:56:01  INFO FileSourceStrategy: Post-Scan Filters: 
23/03/08 19:56:01  INFO FileSourceStrategy: Output Data Schema: struct<sepal_length: double, sepal_width: double, petal_length: double, petal_width: double, species: string ... 1 more field>
23/03/08 19:56:01  INFO SparkContext: Created broadcast 68 from take at RestController.scala:290
23/03/08 19:56:01  INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 19:56:01  INFO SparkContext: Starting job: take at RestController.scala:290
23/03/08 19:56:01  INFO SparkContext: Created broadcast 69 from broadcast at DAGScheduler.scala:1513
23/03/08 19:56:01  INFO CodeGenerator: Code generated in 10.128602 ms
23/03/08 19:56:01  INFO FileScanRDD: Reading File path: file:///Users/allwefantasy/projects/byzer-ml-example/example-data/iris.csv, range: 0-4169, partition values: [empty row]
23/03/08 19:56:01  INFO CodeGenerator: Code generated in 8.144944 ms
23/03/08 19:56:01  INFO CodeGenerator: Code generated in 45.199112 ms
23/03/08 19:56:01  INFO impl: Completed 200 in 538ms 	POST /run/script



23/03/08 19:56:06  INFO SparkContext: Starting job: collect at ScriptSQLExec.scala:221
23/03/08 19:56:06  INFO SparkContext: Created broadcast 70 from broadcast at DAGScheduler.scala:1513
23/03/08 19:56:06  INFO DefaultConsoleClient: [owner] [admin] [groupId] [196a5d67-9770-489c-a629-3fee1fb9d808] __MMMMMM__ auth admin  want access tables: [{"table":"iris","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"testTable","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"output","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}}]
23/03/08 19:56:06  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [196a5d67-9770-489c-a629-3fee1fb9d808] __MMMMMM__ Total jobs: 1 current job:1 job script:select iris_model_predict(array(feature)) as predicted from testTable as output 
23/03/08 19:56:06  INFO FileSourceStrategy: Pushed Filters: 
23/03/08 19:56:06  INFO FileSourceStrategy: Post-Scan Filters: 
23/03/08 19:56:06  INFO FileSourceStrategy: Output Data Schema: struct<sepal_length: double, sepal_width: double, petal_length: double, petal_width: double ... 2 more fields>
23/03/08 19:56:06  INFO CodeGenerator: Code generated in 11.627708 ms
23/03/08 19:56:06  INFO SparkContext: Created broadcast 71 from take at RestController.scala:290
23/03/08 19:56:06  INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 19:56:06  INFO SparkContext: Starting job: take at RestController.scala:290
23/03/08 19:56:06  INFO SparkContext: Created broadcast 72 from broadcast at DAGScheduler.scala:1513
23/03/08 19:56:06  INFO CodeGenerator: Code generated in 23.415877 ms
23/03/08 19:56:06  INFO FileScanRDD: Reading File path: file:///Users/allwefantasy/projects/byzer-ml-example/example-data/iris.csv, range: 0-4169, partition values: [empty row]
23/03/08 19:56:06  INFO CodeGenerator: Code generated in 9.719035 ms
23/03/08 19:56:06  INFO CodeGenerator: Code generated in 11.783921 ms
23/03/08 19:56:07  INFO CodeGenerator: Code generated in 11.27535 ms
23/03/08 19:56:09  INFO DriverLogServer: [owner] [admin] [groupId] [b1d13e0c-3300-4315-84a2-a9b1653e1b59] __MMMMMM__ File descriptor limit 2560 is too low for production servers and may result in connection errors. At least 8192 is recommended. --- Fix with 'ulimit -n 8192'
23/03/08 19:56:11  INFO Ray: Execute predict code time:4583
23/03/08 19:56:11  INFO CodeGenerator: Code generated in 9.639828 ms
23/03/08 19:56:11  INFO CodeGenerator: Code generated in 21.894561 ms
23/03/08 19:56:11  INFO Ray: Execute predict code time:39
23/03/08 19:56:11  INFO Ray: Execute predict code time:53
23/03/08 19:56:11  INFO Ray: Execute predict code time:54
23/03/08 19:56:11  INFO Ray: Execute predict code time:53
23/03/08 19:56:11  INFO Ray: Execute predict code time:55
23/03/08 19:56:11  INFO impl: Completed 200 in 5195ms 	POST /run/script



23/03/08 19:59:07  INFO SocketServerInExecutor: The server ServerSocket[addr=/192.168.3.13,localport=52383] is closing the socket null connection
23/03/08 19:59:07  INFO SocketServerInExecutor: The server ServerSocket[addr=/192.168.3.13,localport=52384] is closing the socket null connection
23/03/08 19:59:07  INFO SocketServerInExecutor: The server ServerSocket[addr=/192.168.3.13,localport=52385] is closing the socket null connection
23/03/08 19:59:07  INFO SocketServerInExecutor: The server ServerSocket[addr=/192.168.3.13,localport=52386] is closing the socket null connection
23/03/08 19:59:07  INFO SocketServerInExecutor: The server ServerSocket[addr=/192.168.3.13,localport=52387] is closing the socket null connection
23/03/08 19:59:07  INFO SocketServerInExecutor: The server ServerSocket[addr=/192.168.3.13,localport=52389] is closing the socket null connection
23/03/08 19:59:07  INFO MasterSlaveInSpark: Exit all data server
23/03/08 19:59:08  INFO SocketServerInExecutor: The server ServerSocket[addr=/192.168.3.13,localport=52392] is closing the socket null connection
23/03/08 19:59:08  INFO MasterSlaveInSpark: Exit all data server
23/03/08 19:59:34  INFO SparkContext: Starting job: collect at ScriptSQLExec.scala:221
23/03/08 19:59:34  INFO SparkContext: Created broadcast 73 from broadcast at DAGScheduler.scala:1513
23/03/08 19:59:34  INFO DefaultConsoleClient: [owner] [admin] [groupId] [aef6c2e4-bb43-4579-81db-d17ca5baa07b] __MMMMMM__ auth admin  want access tables: [{"db":"","table":"./example-data/iris.csv","operateType":{"i":1,"name":"load"},"sourceType":"csv","tableType":{"name":"hdfs","includes":["rate","json","image","parquet","csv","text","binlogRate","excel","libsvm","xml","delta","streamParquet"]}},{"table":"iris","operateType":{"i":1,"name":"load"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}}]
23/03/08 19:59:34  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [aef6c2e4-bb43-4579-81db-d17ca5baa07b] __MMMMMM__ Total jobs: 1 current job:1 job script:load csv.`./example-data/iris.csv` where header = "true"
and inferSchema = "true" 
as iris 
23/03/08 19:59:34  INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
23/03/08 19:59:34  INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
23/03/08 19:59:34  INFO FileSourceStrategy: Pushed Filters: 
23/03/08 19:59:34  INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#3145, None)) > 0)
23/03/08 19:59:34  INFO FileSourceStrategy: Output Data Schema: struct<value: string>
23/03/08 19:59:34  INFO SparkContext: Created broadcast 74 from load at MLSQLCSV.scala:65
23/03/08 19:59:34  INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 19:59:34  INFO SparkContext: Starting job: load at MLSQLCSV.scala:65
23/03/08 19:59:34  INFO SparkContext: Created broadcast 75 from broadcast at DAGScheduler.scala:1513
23/03/08 19:59:34  INFO FileScanRDD: Reading File path: file:///Users/allwefantasy/projects/byzer-ml-example/example-data/iris.csv, range: 0-4169, partition values: [empty row]
23/03/08 19:59:34  INFO FileSourceStrategy: Pushed Filters: 
23/03/08 19:59:34  INFO FileSourceStrategy: Post-Scan Filters: 
23/03/08 19:59:34  INFO FileSourceStrategy: Output Data Schema: struct<value: string>
23/03/08 19:59:34  INFO SparkContext: Created broadcast 76 from load at MLSQLCSV.scala:65
23/03/08 19:59:34  INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 19:59:34  INFO SparkContext: Starting job: load at MLSQLCSV.scala:65
23/03/08 19:59:34  INFO SparkContext: Created broadcast 77 from broadcast at DAGScheduler.scala:1513
23/03/08 19:59:34  INFO FileScanRDD: Reading File path: file:///Users/allwefantasy/projects/byzer-ml-example/example-data/iris.csv, range: 0-4169, partition values: [empty row]
23/03/08 19:59:34  INFO FileSourceStrategy: Pushed Filters: 
23/03/08 19:59:34  INFO FileSourceStrategy: Post-Scan Filters: 
23/03/08 19:59:34  INFO FileSourceStrategy: Output Data Schema: struct<sepal_length: double, sepal_width: double, petal_length: double, petal_width: double, species: string ... 1 more field>
23/03/08 19:59:34  INFO SparkContext: Created broadcast 78 from take at RestController.scala:290
23/03/08 19:59:34  INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 19:59:34  INFO SparkContext: Starting job: take at RestController.scala:290
23/03/08 19:59:34  INFO SparkContext: Created broadcast 79 from broadcast at DAGScheduler.scala:1513
23/03/08 19:59:34  INFO FileScanRDD: Reading File path: file:///Users/allwefantasy/projects/byzer-ml-example/example-data/iris.csv, range: 0-4169, partition values: [empty row]
23/03/08 19:59:34  INFO impl: Completed 200 in 314ms 	POST /run/script



23/03/08 20:00:04  INFO SocketServerInExecutor: The server ServerSocket[addr=/192.168.3.13,localport=52476] is closing the socket null connection
23/03/08 20:00:04  INFO SocketServerInExecutor: The server ServerSocket[addr=/192.168.3.13,localport=52477] is closing the socket null connection
23/03/08 20:00:04  INFO SocketServerInExecutor: The server ServerSocket[addr=/192.168.3.13,localport=52479] is closing the socket null connection
23/03/08 20:00:04  INFO SocketServerInExecutor: The server ServerSocket[addr=/192.168.3.13,localport=52481] is closing the socket null connection
23/03/08 20:00:04  INFO SocketServerInExecutor: The server ServerSocket[addr=/192.168.3.13,localport=52482] is closing the socket null connection
23/03/08 20:00:05  INFO SocketServerInExecutor: The server ServerSocket[addr=/192.168.3.13,localport=52487] is closing the socket null connection
23/03/08 20:00:05  INFO MasterSlaveInSpark: Exit all data server
23/03/08 20:01:46  INFO SparkContext: Starting job: collect at ScriptSQLExec.scala:221
23/03/08 20:01:46  INFO SparkContext: Created broadcast 80 from broadcast at DAGScheduler.scala:1513
23/03/08 20:01:46  INFO DefaultConsoleClient: [owner] [admin] [groupId] [7dcdea93-efea-40f4-83eb-433a055a2f8f] __MMMMMM__ auth admin  want access tables: [{"table":"iris","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"iris_features","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}}]
23/03/08 20:01:46  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [7dcdea93-efea-40f4-83eb-433a055a2f8f] __MMMMMM__ Total jobs: 1 current job:1 job script:select array(sepal_length,sepal_width,petal_length,petal_width) as feature, cast(species_id as double) as label from iris 
as iris_features 
23/03/08 20:01:46  INFO FileSourceStrategy: Pushed Filters: 
23/03/08 20:01:46  INFO FileSourceStrategy: Post-Scan Filters: 
23/03/08 20:01:46  INFO FileSourceStrategy: Output Data Schema: struct<sepal_length: double, sepal_width: double, petal_length: double, petal_width: double, species_id: int ... 3 more fields>
23/03/08 20:01:46  INFO CodeGenerator: Code generated in 5.363852 ms
23/03/08 20:01:46  INFO SparkContext: Created broadcast 81 from take at RestController.scala:290
23/03/08 20:01:46  INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 20:01:46  INFO SparkContext: Starting job: take at RestController.scala:290
23/03/08 20:01:46  INFO SparkContext: Created broadcast 82 from broadcast at DAGScheduler.scala:1513
23/03/08 20:01:46  INFO CodeGenerator: Code generated in 5.963686 ms
23/03/08 20:01:46  INFO FileScanRDD: Reading File path: file:///Users/allwefantasy/projects/byzer-ml-example/example-data/iris.csv, range: 0-4169, partition values: [empty row]
23/03/08 20:01:46  INFO CodeGenerator: Code generated in 3.550755 ms
23/03/08 20:01:46  INFO CodeGenerator: Code generated in 8.425683 ms
23/03/08 20:01:46  INFO impl: Completed 200 in 163ms 	POST /run/script



23/03/08 20:05:54  INFO SparkContext: Starting job: collect at ScriptSQLExec.scala:221
23/03/08 20:05:54  INFO SparkContext: Created broadcast 83 from broadcast at DAGScheduler.scala:1513
23/03/08 20:05:54  INFO DefaultConsoleClient: [owner] [admin] [groupId] [771666e4-87db-4279-b7d9-454448fb3df1] __MMMMMM__ auth admin  want access tables: [{"table":"iris_features","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"iris_features_split","operateType":{"i":1,"name":"load"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"train_data","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"test_data","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}}]
23/03/08 20:05:54  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [771666e4-87db-4279-b7d9-454448fb3df1] __MMMMMM__ Total jobs: 2 current job:1 job script:run iris_features as SQLRateSampler.`` where sampleRate="0.9,0.1" and labelCol="label" as iris_features_split 
23/03/08 20:05:54  WARN MLMapping: [owner] [admin] [groupId] [771666e4-87db-4279-b7d9-454448fb3df1] __MMMMMM__ Do not calling unregistered ET! If you are using a custom ET, please register it in `ETRegister`.
23/03/08 20:05:54  ERROR RestController: An error occurred while the job manager was executing the task, 
java.lang.RuntimeException: SQLRateSampler is not found
	at tech.mlsql.dsl.adaptor.MLMapping$.findET(TrainAdaptor.scala:286) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at tech.mlsql.dsl.adaptor.MLMapping$.findAlg(TrainAdaptor.scala:260) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at tech.mlsql.dsl.adaptor.TrainAdaptor.parse(TrainAdaptor.scala:73) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at streaming.dsl.ScriptSQLExecListener.execute$1(ScriptSQLExec.scala:406) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at streaming.dsl.ScriptSQLExecListener.exitSql(ScriptSQLExec.scala:445) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at streaming.dsl.parser.DSLSQLParser$SqlContext.exitRule(DSLSQLParser.java:296) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.antlr.v4.runtime.tree.ParseTreeWalker.exitRule(ParseTreeWalker.java:47) ~[antlr4-runtime-4.8.jar:4.8]
	at org.antlr.v4.runtime.tree.ParseTreeWalker.walk(ParseTreeWalker.java:30) ~[antlr4-runtime-4.8.jar:4.8]
	at org.antlr.v4.runtime.tree.ParseTreeWalker.walk(ParseTreeWalker.java:28) ~[antlr4-runtime-4.8.jar:4.8]
	at streaming.dsl.ScriptSQLExec$._parse(ScriptSQLExec.scala:159) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at streaming.dsl.ScriptSQLExec$.parse(ScriptSQLExec.scala:146) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at streaming.rest.RestController.$anonfun$script$9(RestController.scala:201) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at tech.mlsql.job.JobManager$.run(JobManager.scala:74) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at streaming.rest.RestController.query$1(RestController.scala:196) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at streaming.rest.RestController.script(RestController.scala:223) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_151]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_151]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_151]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_151]
	at net.csdn.modules.http.RestController.filter(RestController.java:139) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at net.csdn.modules.http.RestController.dispatchRequest(RestController.java:99) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at net.csdn.modules.http.HttpServer$DefaultHandler.handle(HttpServer.java:182) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:52) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:97) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.server.Server.handle(Server.java:499) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:311) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:257) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.io.AbstractConnection$2.run(AbstractConnection.java:544) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_151]
23/03/08 20:05:54  INFO impl: Completed 500 in 179ms 	POST /run/script



23/03/08 20:06:17  INFO SparkContext: Starting job: collect at ScriptSQLExec.scala:221
23/03/08 20:06:17  INFO SparkContext: Created broadcast 84 from broadcast at DAGScheduler.scala:1513
23/03/08 20:06:17  INFO DefaultConsoleClient: [owner] [admin] [groupId] [5a757836-221d-46bf-94cc-af299a378222] __MMMMMM__ auth admin  want access tables: [{"table":"iris_features","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"iris_features_split","operateType":{"i":1,"name":"load"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"train_data","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"test_data","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}}]
23/03/08 20:06:17  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [5a757836-221d-46bf-94cc-af299a378222] __MMMMMM__ Total jobs: 2 current job:1 job script:run iris_features as RateSampler.`` where sampleRate="0.9,0.1" and labelCol="label" as iris_features_split 
23/03/08 20:06:17  INFO FileSourceStrategy: Pushed Filters: 
23/03/08 20:06:17  INFO FileSourceStrategy: Post-Scan Filters: 
23/03/08 20:06:17  INFO FileSourceStrategy: Output Data Schema: struct<species_id: int>
23/03/08 20:06:18  INFO CodeGenerator: Code generated in 76.803049 ms
23/03/08 20:06:18  INFO SparkContext: Created broadcast 85 from collect at SQLRateSampler.scala:78
23/03/08 20:06:18  INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 20:06:18  INFO SparkContext: Created broadcast 86 from broadcast at DAGScheduler.scala:1513
23/03/08 20:06:18  INFO CodeGenerator: Code generated in 8.876541 ms
23/03/08 20:06:18  INFO CodeGenerator: Code generated in 4.416407 ms
23/03/08 20:06:18  INFO CodeGenerator: Code generated in 9.580529 ms
23/03/08 20:06:18  INFO CodeGenerator: Code generated in 3.78054 ms
23/03/08 20:06:18  INFO FileScanRDD: Reading File path: file:///Users/allwefantasy/projects/byzer-ml-example/example-data/iris.csv, range: 0-4169, partition values: [empty row]
23/03/08 20:06:18  INFO CodeGenerator: Code generated in 3.807339 ms
23/03/08 20:06:18  INFO ShufflePartitionsUtil: For shuffle(10), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/03/08 20:06:18  INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
23/03/08 20:06:18  INFO CodeGenerator: Code generated in 11.659265 ms
23/03/08 20:06:18  INFO CodeGenerator: Code generated in 6.570737 ms
23/03/08 20:06:18  INFO SparkContext: Starting job: collect at SQLRateSampler.scala:78
23/03/08 20:06:18  INFO SparkContext: Created broadcast 87 from broadcast at DAGScheduler.scala:1513
23/03/08 20:06:18  INFO CodeGenerator: Code generated in 3.569008 ms
23/03/08 20:06:18  INFO SparkContext: Created broadcast 88 from broadcast at DAGScheduler.scala:1513
23/03/08 20:06:18  INFO ShufflePartitionsUtil: For shuffle(11), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/03/08 20:06:18  INFO CodeGenerator: Code generated in 9.718131 ms
23/03/08 20:06:18  INFO SparkContext: Starting job: collect at SQLRateSampler.scala:78
23/03/08 20:06:18  INFO SparkContext: Created broadcast 89 from broadcast at DAGScheduler.scala:1513
23/03/08 20:06:18  INFO CodeGenerator: Code generated in 4.34276 ms
23/03/08 20:06:18  INFO CodeGenerator: Code generated in 4.064337 ms
23/03/08 20:06:18  INFO CodeGenerator: Code generated in 4.465499 ms
23/03/08 20:06:18  INFO SQLRateSampler: [owner] [admin] [groupId] [5a757836-221d-46bf-94cc-af299a378222] __MMMMMM__ computing data stat:1:50,3:50,2:50
23/03/08 20:06:18  INFO FileSourceStrategy: Pushed Filters: 
23/03/08 20:06:18  INFO FileSourceStrategy: Post-Scan Filters: 
23/03/08 20:06:18  INFO FileSourceStrategy: Output Data Schema: struct<sepal_length: double, sepal_width: double, petal_length: double, petal_width: double, species_id: int ... 3 more fields>
23/03/08 20:06:18  INFO SparkContext: Created broadcast 90 from rdd at SQLRateSampler.scala:116
23/03/08 20:06:18  INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 20:06:18  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [5a757836-221d-46bf-94cc-af299a378222] __MMMMMM__ Total jobs: 2 current job:2 job script:select * from iris_features_split where __split__=1 as test_data 
23/03/08 20:06:18  INFO CodeGenerator: Code generated in 8.477353 ms
23/03/08 20:06:18  INFO SparkContext: Starting job: take at RestController.scala:290
23/03/08 20:06:18  INFO SparkContext: Created broadcast 91 from broadcast at DAGScheduler.scala:1513
23/03/08 20:06:18  INFO FileScanRDD: Reading File path: file:///Users/allwefantasy/projects/byzer-ml-example/example-data/iris.csv, range: 0-4169, partition values: [empty row]
23/03/08 20:06:18  INFO SparkContext: Created broadcast 92 from broadcast at DAGScheduler.scala:1513
23/03/08 20:06:18  INFO CodeGenerator: Code generated in 6.198301 ms
23/03/08 20:06:18  INFO CodeGenerator: Code generated in 9.751148 ms
23/03/08 20:06:18  INFO impl: Completed 200 in 931ms 	POST /run/script



23/03/08 20:10:18  INFO SparkContext: Starting job: collect at ScriptSQLExec.scala:221
23/03/08 20:10:18  INFO SparkContext: Created broadcast 93 from broadcast at DAGScheduler.scala:1513
23/03/08 20:10:18  INFO DefaultConsoleClient: [owner] [admin] [groupId] [03ded6e4-847a-4895-a5da-1be83bf620bd] __MMMMMM__ auth admin  want access tables: [{"table":"command","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"command","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"db":"default","table":"ets","operateType":{"i":7,"name":"select"},"tableType":{"name":"hive","includes":["hive"]}},{"table":"output","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}}]
23/03/08 20:10:18  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [03ded6e4-847a-4895-a5da-1be83bf620bd] __MMMMMM__ Total jobs: 3 current job:1 job script:run command as ShowCommand.`et///////////` 
23/03/08 20:10:18  INFO DefaultConsoleClient: [owner] [admin] [groupId] [03ded6e4-847a-4895-a5da-1be83bf620bd] __MMMMMM__ auth admin  want access tables: [{"table":"","operateType":{"i":1,"name":"load"},"sourceType":"modelList","tableType":{"name":"system","includes":["model","modelList","_mlsql_","modelExplain","modelExample","modelParams"]}},{"table":"__output__","operateType":{"i":1,"name":"load"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}}]
23/03/08 20:10:18  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [03ded6e4-847a-4895-a5da-1be83bf620bd] __MMMMMM__ Total jobs: 1 current job:1 job script:load modelList.`` as __output__ 
23/03/08 20:10:18  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [03ded6e4-847a-4895-a5da-1be83bf620bd] __MMMMMM__ Total jobs: 1 current job:2 job script:run command as LastCommand.`` where parameters='''["named","ets"]''' 
23/03/08 20:10:18  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [03ded6e4-847a-4895-a5da-1be83bf620bd] __MMMMMM__ Total jobs: 1 current job:3 job script:run command as LastCommand.`` where parameters='''["named","ets"]''' 
23/03/08 20:10:18  INFO CodeGenerator: Code generated in 5.05873 ms
23/03/08 20:10:18  INFO SparkContext: Starting job: take at RestController.scala:290
23/03/08 20:10:18  INFO SparkContext: Created broadcast 94 from broadcast at DAGScheduler.scala:1513
23/03/08 20:10:18  INFO CodeGenerator: Code generated in 8.487873 ms
23/03/08 20:10:18  INFO CodeGenerator: Code generated in 33.144087 ms
23/03/08 20:10:18  INFO impl: Completed 200 in 732ms 	POST /run/script



23/03/08 20:13:08  INFO SparkContext: Starting job: collect at ScriptSQLExec.scala:221
23/03/08 20:13:08  INFO SparkContext: Created broadcast 95 from broadcast at DAGScheduler.scala:1513
23/03/08 20:13:08  INFO DefaultConsoleClient: [owner] [admin] [groupId] [2c91ff38-a2b4-4d47-b1bb-9b700a84f745] __MMMMMM__ auth admin  want access tables: [{"table":"command","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"command","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}}]
23/03/08 20:13:08  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [2c91ff38-a2b4-4d47-b1bb-9b700a84f745] __MMMMMM__ Total jobs: 2 current job:1 job script:run command as ShowCommand.`et/RandomForest///////////` 
23/03/08 20:13:08  INFO DefaultConsoleClient: [owner] [admin] [groupId] [2c91ff38-a2b4-4d47-b1bb-9b700a84f745] __MMMMMM__ auth admin  want access tables: [{"table":"RandomForest","operateType":{"i":1,"name":"load"},"sourceType":"modelExample","tableType":{"name":"system","includes":["model","modelList","_mlsql_","modelExplain","modelExample","modelParams"]}},{"table":"__output__","operateType":{"i":1,"name":"load"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}}]
23/03/08 20:13:08  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [2c91ff38-a2b4-4d47-b1bb-9b700a84f745] __MMMMMM__ Total jobs: 1 current job:1 job script:load modelExample.`RandomForest` as __output__ 
23/03/08 20:13:08  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [2c91ff38-a2b4-4d47-b1bb-9b700a84f745] __MMMMMM__ Total jobs: 1 current job:2 job script:run command as ShowCommand.`et/params/RandomForest///////////` 
23/03/08 20:13:08  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [2c91ff38-a2b4-4d47-b1bb-9b700a84f745] __MMMMMM__ Total jobs: 1 current job:3 job script:run command as ShowCommand.`et/params/RandomForest///////////` 
23/03/08 20:13:08  INFO DefaultConsoleClient: [owner] [admin] [groupId] [2c91ff38-a2b4-4d47-b1bb-9b700a84f745] __MMMMMM__ auth admin  want access tables: [{"table":"RandomForest","operateType":{"i":1,"name":"load"},"sourceType":"modelParams","tableType":{"name":"system","includes":["model","modelList","_mlsql_","modelExplain","modelExample","modelParams"]}},{"table":"__output__","operateType":{"i":1,"name":"load"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}}]
23/03/08 20:13:08  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [2c91ff38-a2b4-4d47-b1bb-9b700a84f745] __MMMMMM__ Total jobs: 1 current job:1 job script:load modelParams.`RandomForest` as __output__ 
23/03/08 20:13:09  INFO CodeGenerator: Code generated in 5.306354 ms
23/03/08 20:13:09  INFO SparkContext: Starting job: take at RestController.scala:290
23/03/08 20:13:09  INFO SparkContext: Created broadcast 96 from broadcast at DAGScheduler.scala:1513
23/03/08 20:13:09  INFO CodeGenerator: Code generated in 9.01957 ms
23/03/08 20:13:09  INFO CodeGenerator: Code generated in 18.573899 ms
23/03/08 20:13:09  INFO impl: Completed 200 in 427ms 	POST /run/script



23/03/08 20:13:24  INFO SparkContext: Starting job: collect at ScriptSQLExec.scala:221
23/03/08 20:13:24  INFO SparkContext: Created broadcast 97 from broadcast at DAGScheduler.scala:1513
23/03/08 20:13:24  ERROR RestController: An error occurred while the job manager was executing the task, 
java.lang.RuntimeException: MLSQL Parser error in [row:1 column:0]  error msg: mismatched input '#' expecting {<EOF>, 'load', 'save', 'select', 'insert', 'create', 'drop', 'refresh', 'set', 'connect', 'train', 'run', 'predict', 'register', 'unregister', 'include', EXECUTE_COMMAND, SIMPLE_COMMENT}
	at org.apache.spark.MLSQLSyntaxErrorListener.syntaxError(MLSQLSyntaxErrorListener.java:29) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:3.3.0]
	at org.antlr.v4.runtime.ProxyErrorListener.syntaxError(ProxyErrorListener.java:41) ~[antlr4-runtime-4.8.jar:4.8]
	at org.antlr.v4.runtime.Parser.notifyErrorListeners(Parser.java:544) ~[antlr4-runtime-4.8.jar:4.8]
	at org.antlr.v4.runtime.DefaultErrorStrategy.reportInputMismatch(DefaultErrorStrategy.java:327) ~[antlr4-runtime-4.8.jar:4.8]
	at org.antlr.v4.runtime.DefaultErrorStrategy.reportError(DefaultErrorStrategy.java:139) ~[antlr4-runtime-4.8.jar:4.8]
	at streaming.dsl.parser.DSLSQLParser.statement(DSLSQLParser.java:168) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at streaming.dsl.ScriptSQLExec$._parse(ScriptSQLExec.scala:158) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at streaming.dsl.ScriptSQLExec$.parse(ScriptSQLExec.scala:100) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at streaming.rest.RestController.$anonfun$script$9(RestController.scala:201) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at tech.mlsql.job.JobManager$.run(JobManager.scala:74) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at streaming.rest.RestController.query$1(RestController.scala:196) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at streaming.rest.RestController.script(RestController.scala:223) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at sun.reflect.GeneratedMethodAccessor119.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_151]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_151]
	at net.csdn.modules.http.RestController.filter(RestController.java:139) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at net.csdn.modules.http.RestController.dispatchRequest(RestController.java:99) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at net.csdn.modules.http.HttpServer$DefaultHandler.handle(HttpServer.java:182) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:52) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:97) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.server.Server.handle(Server.java:499) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:311) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:257) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.io.AbstractConnection$2.run(AbstractConnection.java:544) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_151]
23/03/08 20:13:24  INFO impl: Completed 500 in 84ms 	POST /run/script



23/03/08 20:13:58  INFO SparkContext: Starting job: collect at ScriptSQLExec.scala:221
23/03/08 20:13:58  INFO SparkContext: Created broadcast 98 from broadcast at DAGScheduler.scala:1513
23/03/08 20:13:58  INFO DefaultConsoleClient: [owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ auth admin  want access tables: [{"table":"train_data","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}}]
23/03/08 20:13:58  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ Total jobs: 1 current job:1 job script:train train_data as RandomForest.`/tmp/iris_model` where
keepVersion="true" 
and evaluateTable="test_data"

and `fitParam.0.labelCol`="label"
and `fitParam.0.featuresCol`="features"
and `fitParam.0.maxDepth`="2"

and `fitParam.1.featuresCol`="features"
and `fitParam.1.labelCol`="label"
and `fitParam.1.maxDepth`="10" 
23/03/08 20:13:58  INFO SQLRandomForest: [owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ [training] [alg=org.apache.spark.ml.classification.RandomForestClassifier] [keepVersion=true]
23/03/08 20:13:58  INFO SQLRandomForest: [owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ java.lang.IllegalArgumentException: features does not exist. Available: feature, label, __split__
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.apache.spark.sql.types.StructType.$anonfun$apply$1(StructType.scala:282)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ scala.collection.immutable.Map$Map3.getOrElse(Map.scala:336)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.apache.spark.sql.types.StructType.apply(StructType.scala:281)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.apache.spark.ml.util.SchemaUtils$.checkColumnType(SchemaUtils.scala:42)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.apache.spark.ml.PredictorParams.validateAndTransformSchema(Predictor.scala:52)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.apache.spark.ml.PredictorParams.validateAndTransformSchema$(Predictor.scala:47)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.apache.spark.ml.classification.Classifier.org$apache$spark$ml$classification$ClassifierParams$$super$validateAndTransformSchema(Classifier.scala:73)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.apache.spark.ml.classification.ClassifierParams.validateAndTransformSchema(Classifier.scala:43)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.apache.spark.ml.classification.ClassifierParams.validateAndTransformSchema$(Classifier.scala:39)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.apache.spark.ml.classification.ProbabilisticClassifier.org$apache$spark$ml$classification$ProbabilisticClassifierParams$$super$validateAndTransformSchema(ProbabilisticClassifier.scala:51)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.apache.spark.ml.classification.ProbabilisticClassifierParams.validateAndTransformSchema(ProbabilisticClassifier.scala:38)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.apache.spark.ml.classification.ProbabilisticClassifierParams.validateAndTransformSchema$(ProbabilisticClassifier.scala:34)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.apache.spark.ml.classification.RandomForestClassifier.org$apache$spark$ml$tree$TreeEnsembleClassifierParams$$super$validateAndTransformSchema(RandomForestClassifier.scala:46)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.apache.spark.ml.tree.TreeEnsembleClassifierParams.validateAndTransformSchema(treeParams.scala:404)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.apache.spark.ml.tree.TreeEnsembleClassifierParams.validateAndTransformSchema$(treeParams.scala:400)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.apache.spark.ml.classification.RandomForestClassifier.validateAndTransformSchema(RandomForestClassifier.scala:46)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.apache.spark.ml.Predictor.transformSchema(Predictor.scala:177)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.apache.spark.ml.Predictor.fit(Predictor.scala:133)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.apache.spark.ml.Predictor.fit(Predictor.scala:115)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ streaming.dsl.mmlib.algs.Functions.$anonfun$trainModelsWithMultiParamGroup$2(Functions.scala:331)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ streaming.dsl.mmlib.algs.Functions.$anonfun$trainModelsWithMultiParamGroup$2$adapted(Functions.scala:319)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ streaming.dsl.mmlib.algs.Functions.$anonfun$trainModelsWithMultiParamGroup$7(Functions.scala:366)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ scala.collection.TraversableLike.map(TraversableLike.scala:286)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ scala.collection.TraversableLike.map$(TraversableLike.scala:279)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ streaming.dsl.mmlib.algs.Functions.trainModelsWithMultiParamGroup(Functions.scala:365)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ streaming.dsl.mmlib.algs.Functions.trainModelsWithMultiParamGroup$(Functions.scala:312)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ streaming.dsl.mmlib.algs.SQLRandomForest.trainModelsWithMultiParamGroup(SQLRandomForest.scala:34)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ streaming.dsl.mmlib.algs.SQLRandomForest.train(SQLRandomForest.scala:52)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ tech.mlsql.dsl.adaptor.TrainAdaptor.parse(TrainAdaptor.scala:116)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ streaming.dsl.ScriptSQLExecListener.execute$1(ScriptSQLExec.scala:406)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ streaming.dsl.ScriptSQLExecListener.exitSql(ScriptSQLExec.scala:445)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ streaming.dsl.parser.DSLSQLParser$SqlContext.exitRule(DSLSQLParser.java:296)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.antlr.v4.runtime.tree.ParseTreeWalker.exitRule(ParseTreeWalker.java:47)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.antlr.v4.runtime.tree.ParseTreeWalker.walk(ParseTreeWalker.java:30)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.antlr.v4.runtime.tree.ParseTreeWalker.walk(ParseTreeWalker.java:28)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ streaming.dsl.ScriptSQLExec$._parse(ScriptSQLExec.scala:159)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ streaming.dsl.ScriptSQLExec$.parse(ScriptSQLExec.scala:146)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ streaming.rest.RestController.$anonfun$script$9(RestController.scala:201)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ tech.mlsql.job.JobManager$.run(JobManager.scala:74)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ streaming.rest.RestController.query$1(RestController.scala:196)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ streaming.rest.RestController.script(RestController.scala:223)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ sun.reflect.GeneratedMethodAccessor119.invoke(Unknown Source)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ java.lang.reflect.Method.invoke(Method.java:498)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ net.csdn.modules.http.RestController.filter(RestController.java:139)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ net.csdn.modules.http.RestController.dispatchRequest(RestController.java:99)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ net.csdn.modules.http.HttpServer$DefaultHandler.handle(HttpServer.java:182)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:52)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:97)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.eclipse.jetty.server.Server.handle(Server.java:499)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:311)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:257)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.eclipse.jetty.io.AbstractConnection$2.run(AbstractConnection.java:544)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ java.lang.Thread.run(Thread.java:748)
23/03/08 20:13:58  INFO SQLRandomForest: [owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ [training] [alg=org.apache.spark.ml.classification.RandomForestClassifier] [keepVersion=true]
23/03/08 20:13:58  INFO SQLRandomForest: [owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ java.lang.IllegalArgumentException: features does not exist. Available: feature, label, __split__
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.apache.spark.sql.types.StructType.$anonfun$apply$1(StructType.scala:282)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ scala.collection.immutable.Map$Map3.getOrElse(Map.scala:336)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.apache.spark.sql.types.StructType.apply(StructType.scala:281)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.apache.spark.ml.util.SchemaUtils$.checkColumnType(SchemaUtils.scala:42)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.apache.spark.ml.PredictorParams.validateAndTransformSchema(Predictor.scala:52)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.apache.spark.ml.PredictorParams.validateAndTransformSchema$(Predictor.scala:47)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.apache.spark.ml.classification.Classifier.org$apache$spark$ml$classification$ClassifierParams$$super$validateAndTransformSchema(Classifier.scala:73)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.apache.spark.ml.classification.ClassifierParams.validateAndTransformSchema(Classifier.scala:43)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.apache.spark.ml.classification.ClassifierParams.validateAndTransformSchema$(Classifier.scala:39)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.apache.spark.ml.classification.ProbabilisticClassifier.org$apache$spark$ml$classification$ProbabilisticClassifierParams$$super$validateAndTransformSchema(ProbabilisticClassifier.scala:51)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.apache.spark.ml.classification.ProbabilisticClassifierParams.validateAndTransformSchema(ProbabilisticClassifier.scala:38)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.apache.spark.ml.classification.ProbabilisticClassifierParams.validateAndTransformSchema$(ProbabilisticClassifier.scala:34)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.apache.spark.ml.classification.RandomForestClassifier.org$apache$spark$ml$tree$TreeEnsembleClassifierParams$$super$validateAndTransformSchema(RandomForestClassifier.scala:46)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.apache.spark.ml.tree.TreeEnsembleClassifierParams.validateAndTransformSchema(treeParams.scala:404)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.apache.spark.ml.tree.TreeEnsembleClassifierParams.validateAndTransformSchema$(treeParams.scala:400)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.apache.spark.ml.classification.RandomForestClassifier.validateAndTransformSchema(RandomForestClassifier.scala:46)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.apache.spark.ml.Predictor.transformSchema(Predictor.scala:177)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.apache.spark.ml.Predictor.fit(Predictor.scala:133)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.apache.spark.ml.Predictor.fit(Predictor.scala:115)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ streaming.dsl.mmlib.algs.Functions.$anonfun$trainModelsWithMultiParamGroup$2(Functions.scala:331)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ streaming.dsl.mmlib.algs.Functions.$anonfun$trainModelsWithMultiParamGroup$2$adapted(Functions.scala:319)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ streaming.dsl.mmlib.algs.Functions.$anonfun$trainModelsWithMultiParamGroup$7(Functions.scala:366)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ scala.collection.TraversableLike.map(TraversableLike.scala:286)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ scala.collection.TraversableLike.map$(TraversableLike.scala:279)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ streaming.dsl.mmlib.algs.Functions.trainModelsWithMultiParamGroup(Functions.scala:365)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ streaming.dsl.mmlib.algs.Functions.trainModelsWithMultiParamGroup$(Functions.scala:312)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ streaming.dsl.mmlib.algs.SQLRandomForest.trainModelsWithMultiParamGroup(SQLRandomForest.scala:34)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ streaming.dsl.mmlib.algs.SQLRandomForest.train(SQLRandomForest.scala:52)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ tech.mlsql.dsl.adaptor.TrainAdaptor.parse(TrainAdaptor.scala:116)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ streaming.dsl.ScriptSQLExecListener.execute$1(ScriptSQLExec.scala:406)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ streaming.dsl.ScriptSQLExecListener.exitSql(ScriptSQLExec.scala:445)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ streaming.dsl.parser.DSLSQLParser$SqlContext.exitRule(DSLSQLParser.java:296)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.antlr.v4.runtime.tree.ParseTreeWalker.exitRule(ParseTreeWalker.java:47)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.antlr.v4.runtime.tree.ParseTreeWalker.walk(ParseTreeWalker.java:30)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.antlr.v4.runtime.tree.ParseTreeWalker.walk(ParseTreeWalker.java:28)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ streaming.dsl.ScriptSQLExec$._parse(ScriptSQLExec.scala:159)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ streaming.dsl.ScriptSQLExec$.parse(ScriptSQLExec.scala:146)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ streaming.rest.RestController.$anonfun$script$9(RestController.scala:201)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ tech.mlsql.job.JobManager$.run(JobManager.scala:74)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ streaming.rest.RestController.query$1(RestController.scala:196)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ streaming.rest.RestController.script(RestController.scala:223)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ sun.reflect.GeneratedMethodAccessor119.invoke(Unknown Source)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ java.lang.reflect.Method.invoke(Method.java:498)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ net.csdn.modules.http.RestController.filter(RestController.java:139)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ net.csdn.modules.http.RestController.dispatchRequest(RestController.java:99)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ net.csdn.modules.http.HttpServer$DefaultHandler.handle(HttpServer.java:182)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:52)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:97)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.eclipse.jetty.server.Server.handle(Server.java:499)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:311)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:257)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.eclipse.jetty.io.AbstractConnection$2.run(AbstractConnection.java:544)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555)
[owner] [admin] [groupId] [b89448fd-eaca-4ac0-bb7d-2fd964416679] __MMMMMM__ java.lang.Thread.run(Thread.java:748)
23/03/08 20:13:58  INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 20:13:58  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 20:13:58  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 20:13:58  INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 20:13:58  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 20:13:58  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 20:13:58  INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 20:13:58  INFO CodeGenerator: Code generated in 14.295942 ms
23/03/08 20:13:58  INFO SparkContext: Starting job: parquet at Functions.scala:388
23/03/08 20:13:58  INFO SparkContext: Created broadcast 99 from broadcast at DAGScheduler.scala:1513
23/03/08 20:13:58  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 20:13:58  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 20:13:58  INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 20:13:58  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 20:13:58  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 20:13:58  INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 20:13:58  INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "modelPath",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "algIndex",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "alg",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "metrics",
    "type" : {
      "type" : "array",
      "elementType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "name",
          "type" : "string",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "value",
          "type" : "double",
          "nullable" : true,
          "metadata" : { }
        } ]
      },
      "containsNull" : true
    },
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "status",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "message",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "startTime",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "endTime",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trainParams",
    "type" : {
      "type" : "map",
      "keyType" : "string",
      "valueType" : "string",
      "valueContainsNull" : true
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary modelPath (STRING);
  optional int32 algIndex;
  optional binary alg (STRING);
  optional group metrics (LIST) {
    repeated group list {
      optional group element {
        optional binary name (STRING);
        optional double value;
      }
    }
  }
  optional binary status (STRING);
  optional binary message (STRING);
  optional int64 startTime;
  optional int64 endTime;
  optional group trainParams (MAP) {
    repeated group key_value {
      required binary key (STRING);
      optional binary value (STRING);
    }
  }
}

       
23/03/08 20:13:58  INFO CodeGenerator: Code generated in 57.818941 ms
23/03/08 20:13:58  INFO FileOutputCommitter: Saved output of task 'attempt_202303082013585787490773486119934_0085_m_000000_300' to file:/tmp/iris_model/_model_0/meta/0/_temporary/0/task_202303082013585787490773486119934_0085_m_000000
23/03/08 20:13:58  INFO SparkHadoopMapRedUtil: attempt_202303082013585787490773486119934_0085_m_000000_300: Committed. Elapsed time: 0 ms.
23/03/08 20:13:58  INFO FileFormatWriter: Start to commit write Job c200e377-3e8c-40e6-a973-fa650f447a8d.
23/03/08 20:13:58  INFO FileFormatWriter: Write Job c200e377-3e8c-40e6-a973-fa650f447a8d committed. Elapsed time: 15 ms.
23/03/08 20:13:58  INFO FileFormatWriter: Finished processing stats for write job c200e377-3e8c-40e6-a973-fa650f447a8d.
23/03/08 20:13:58  INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
23/03/08 20:13:58  INFO SparkContext: Starting job: parquet at BaseParams.scala:80
23/03/08 20:13:59  INFO SparkContext: Created broadcast 100 from broadcast at DAGScheduler.scala:1513
23/03/08 20:13:59  INFO FileSourceStrategy: Pushed Filters: 
23/03/08 20:13:59  INFO FileSourceStrategy: Post-Scan Filters: 
23/03/08 20:13:59  INFO FileSourceStrategy: Output Data Schema: struct<modelPath: string, algIndex: int, alg: string, metrics: array<struct<name:string,value:double>>, status: string ... 7 more fields>
23/03/08 20:13:59  INFO SparkContext: Created broadcast 101 from collect at MllibFunctions.scala:54
23/03/08 20:13:59  INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 20:13:59  INFO SparkContext: Starting job: collect at MllibFunctions.scala:54
23/03/08 20:13:59  INFO SparkContext: Created broadcast 102 from broadcast at DAGScheduler.scala:1513
23/03/08 20:13:59  INFO CodeGenerator: Code generated in 12.695412 ms
23/03/08 20:13:59  INFO FileScanRDD: Reading File path: file:///tmp/iris_model/_model_0/meta/0/part-00000-355d4130-6afb-4b1a-9e27-8c32fc84f7bc-c000.snappy.parquet, range: 0-5932, partition values: [empty row]
23/03/08 20:13:59  INFO CodeGenerator: Code generated in 28.913788 ms
23/03/08 20:13:59  INFO CodeGenerator: Code generated in 5.261441 ms
23/03/08 20:13:59  INFO SparkContext: Starting job: take at RestController.scala:290
23/03/08 20:13:59  INFO SparkContext: Created broadcast 103 from broadcast at DAGScheduler.scala:1513
23/03/08 20:13:59  INFO CodeGenerator: Code generated in 6.717118 ms
23/03/08 20:13:59  INFO CodeGenerator: Code generated in 15.215294 ms
23/03/08 20:13:59  INFO impl: Completed 200 in 1199ms 	POST /run/script



23/03/08 20:14:47  INFO SparkContext: Starting job: collect at ScriptSQLExec.scala:221
23/03/08 20:14:47  INFO SparkContext: Created broadcast 104 from broadcast at DAGScheduler.scala:1513
23/03/08 20:14:47  INFO DefaultConsoleClient: [owner] [admin] [groupId] [a44fe3a1-9540-468b-b9fe-6913e51a3b03] __MMMMMM__ auth admin  want access tables: []
23/03/08 20:14:47  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [a44fe3a1-9540-468b-b9fe-6913e51a3b03] __MMMMMM__ Total jobs: 1 current job:1 job script:register RandomForest.`/tmp/model` as model_predict 
23/03/08 20:14:47  ERROR RestController: An error occurred while the job manager was executing the task, 
org.apache.spark.sql.mlsql.session.MLSQLException: /tmp/model is not a validate model path
	at streaming.dsl.mmlib.algs.MllibFunctions.mllibModelAndMetaPath(MllibFunctions.scala:195) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at streaming.dsl.mmlib.algs.MllibFunctions.mllibModelAndMetaPath$(MllibFunctions.scala:194) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at streaming.dsl.mmlib.algs.SQLRandomForest.mllibModelAndMetaPath(SQLRandomForest.scala:34) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at streaming.dsl.mmlib.algs.SQLRandomForest.load(SQLRandomForest.scala:85) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at tech.mlsql.dsl.adaptor.RegisterAdaptor.parse(RegisterAdaptor.scala:70) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at streaming.dsl.ScriptSQLExecListener.execute$1(ScriptSQLExec.scala:406) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at streaming.dsl.ScriptSQLExecListener.exitSql(ScriptSQLExec.scala:448) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at streaming.dsl.parser.DSLSQLParser$SqlContext.exitRule(DSLSQLParser.java:296) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.antlr.v4.runtime.tree.ParseTreeWalker.exitRule(ParseTreeWalker.java:47) ~[antlr4-runtime-4.8.jar:4.8]
	at org.antlr.v4.runtime.tree.ParseTreeWalker.walk(ParseTreeWalker.java:30) ~[antlr4-runtime-4.8.jar:4.8]
	at org.antlr.v4.runtime.tree.ParseTreeWalker.walk(ParseTreeWalker.java:28) ~[antlr4-runtime-4.8.jar:4.8]
	at streaming.dsl.ScriptSQLExec$._parse(ScriptSQLExec.scala:159) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at streaming.dsl.ScriptSQLExec$.parse(ScriptSQLExec.scala:146) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at streaming.rest.RestController.$anonfun$script$9(RestController.scala:201) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at tech.mlsql.job.JobManager$.run(JobManager.scala:74) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at streaming.rest.RestController.query$1(RestController.scala:196) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at streaming.rest.RestController.script(RestController.scala:223) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at sun.reflect.GeneratedMethodAccessor119.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_151]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_151]
	at net.csdn.modules.http.RestController.filter(RestController.java:139) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at net.csdn.modules.http.RestController.dispatchRequest(RestController.java:99) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at net.csdn.modules.http.HttpServer$DefaultHandler.handle(HttpServer.java:182) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:52) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:97) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.server.Server.handle(Server.java:499) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:311) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:257) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.io.AbstractConnection$2.run(AbstractConnection.java:544) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_151]
23/03/08 20:14:47  INFO impl: Completed 500 in 98ms 	POST /run/script



23/03/08 20:14:59  INFO SparkContext: Starting job: collect at ScriptSQLExec.scala:221
23/03/08 20:14:59  INFO SparkContext: Created broadcast 105 from broadcast at DAGScheduler.scala:1513
23/03/08 20:14:59  INFO DefaultConsoleClient: [owner] [admin] [groupId] [cf0cdd2a-f695-4d9e-b481-4c0092d5c13c] __MMMMMM__ auth admin  want access tables: []
23/03/08 20:14:59  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [cf0cdd2a-f695-4d9e-b481-4c0092d5c13c] __MMMMMM__ Total jobs: 1 current job:1 job script:register RandomForest.`/tmp/iris_model` as model_predict 
23/03/08 20:14:59  INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
23/03/08 20:14:59  INFO SparkContext: Starting job: parquet at MllibFunctions.scala:199
23/03/08 20:14:59  INFO SparkContext: Created broadcast 106 from broadcast at DAGScheduler.scala:1513
23/03/08 20:14:59  INFO FileSourceStrategy: Pushed Filters: 
23/03/08 20:14:59  INFO FileSourceStrategy: Post-Scan Filters: 
23/03/08 20:14:59  INFO FileSourceStrategy: Output Data Schema: struct<modelPath: string, algIndex: int, alg: string, metrics: array<struct<name:string,value:double>>, status: string ... 7 more fields>
23/03/08 20:14:59  INFO SparkContext: Created broadcast 107 from collect at MllibFunctions.scala:199
23/03/08 20:14:59  INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 20:14:59  INFO SparkContext: Starting job: collect at MllibFunctions.scala:199
23/03/08 20:14:59  INFO SparkContext: Created broadcast 108 from broadcast at DAGScheduler.scala:1513
23/03/08 20:14:59  INFO FileScanRDD: Reading File path: file:///tmp/iris_model/_model_0/meta/0/part-00000-355d4130-6afb-4b1a-9e27-8c32fc84f7bc-c000.snappy.parquet, range: 0-5932, partition values: [empty row]
23/03/08 20:14:59  INFO SQLRandomForest: [owner] [admin] [groupId] [cf0cdd2a-f695-4d9e-b481-4c0092d5c13c] __MMMMMM__ No metric is found, system  will use first model
23/03/08 20:14:59  INFO SQLRandomForest: [owner] [admin] [groupId] [cf0cdd2a-f695-4d9e-b481-4c0092d5c13c] __MMMMMM__ No metric is found, system  will use first model
23/03/08 20:14:59  INFO SparkContext: Created broadcast 109 from textFile at ReadWrite.scala:587
23/03/08 20:14:59  ERROR RestController: An error occurred while the job manager was executing the task, 
org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/tmp/iris_model/_model_0/model/1/metadata
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304) ~[hadoop-client-api-3.3.2.jar:?]
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244) ~[hadoop-client-api-3.3.2.jar:?]
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332) ~[hadoop-client-api-3.3.2.jar:?]
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208) ~[spark-core_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292) ~[spark-core_2.12-3.3.0.jar:3.3.0]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.15.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:288) ~[spark-core_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) ~[spark-core_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292) ~[spark-core_2.12-3.3.0.jar:3.3.0]
	at scala.Option.getOrElse(Option.scala:189) ~[scala-library-2.12.15.jar:?]
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:288) ~[spark-core_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1449) ~[spark-core_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) ~[spark-core_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112) ~[spark-core_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:406) ~[spark-core_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDD.take(RDD.scala:1443) ~[spark-core_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDD.$anonfun$first$1(RDD.scala:1484) ~[spark-core_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) ~[spark-core_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112) ~[spark-core_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:406) ~[spark-core_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.rdd.RDD.first(RDD.scala:1484) ~[spark-core_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.ml.util.DefaultParamsReader$.loadMetadata(ReadWrite.scala:587) ~[spark-mllib_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.ml.tree.EnsembleModelReadWrite$.loadImpl(treeModels.scala:511) ~[spark-mllib_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.ml.classification.RandomForestClassificationModel$RandomForestClassificationModelReader.load(RandomForestClassifier.scala:420) ~[spark-mllib_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.ml.classification.RandomForestClassificationModel$RandomForestClassificationModelReader.load(RandomForestClassifier.scala:410) ~[spark-mllib_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.ml.util.MLReadable.load(ReadWrite.scala:355) ~[spark-mllib_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.ml.util.MLReadable.load$(ReadWrite.scala:355) ~[spark-mllib_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.ml.classification.RandomForestClassificationModel$.load(RandomForestClassifier.scala:394) ~[spark-mllib_2.12-3.3.0.jar:3.3.0]
	at streaming.dsl.mmlib.algs.SQLRandomForest.load(SQLRandomForest.scala:86) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at tech.mlsql.dsl.adaptor.RegisterAdaptor.parse(RegisterAdaptor.scala:70) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at streaming.dsl.ScriptSQLExecListener.execute$1(ScriptSQLExec.scala:406) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at streaming.dsl.ScriptSQLExecListener.exitSql(ScriptSQLExec.scala:448) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at streaming.dsl.parser.DSLSQLParser$SqlContext.exitRule(DSLSQLParser.java:296) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.antlr.v4.runtime.tree.ParseTreeWalker.exitRule(ParseTreeWalker.java:47) ~[antlr4-runtime-4.8.jar:4.8]
	at org.antlr.v4.runtime.tree.ParseTreeWalker.walk(ParseTreeWalker.java:30) ~[antlr4-runtime-4.8.jar:4.8]
	at org.antlr.v4.runtime.tree.ParseTreeWalker.walk(ParseTreeWalker.java:28) ~[antlr4-runtime-4.8.jar:4.8]
	at streaming.dsl.ScriptSQLExec$._parse(ScriptSQLExec.scala:159) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at streaming.dsl.ScriptSQLExec$.parse(ScriptSQLExec.scala:146) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at streaming.rest.RestController.$anonfun$script$9(RestController.scala:201) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at tech.mlsql.job.JobManager$.run(JobManager.scala:74) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at streaming.rest.RestController.query$1(RestController.scala:196) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at streaming.rest.RestController.script(RestController.scala:223) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at sun.reflect.GeneratedMethodAccessor119.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_151]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_151]
	at net.csdn.modules.http.RestController.filter(RestController.java:139) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at net.csdn.modules.http.RestController.dispatchRequest(RestController.java:99) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at net.csdn.modules.http.HttpServer$DefaultHandler.handle(HttpServer.java:182) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:52) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:97) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.server.Server.handle(Server.java:499) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:311) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:257) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.io.AbstractConnection$2.run(AbstractConnection.java:544) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_151]
Caused by: java.io.IOException: Input path does not exist: file:/tmp/iris_model/_model_0/model/1/metadata
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278) ~[hadoop-client-api-3.3.2.jar:?]
	... 56 more
23/03/08 20:14:59  INFO impl: Completed 500 in 466ms 	POST /run/script



23/03/08 20:15:26  INFO MLSQLLanguageServer: shutdown......
23/03/08 20:15:29  INFO MLSQLLanguageServer: exit......
23/03/08 20:15:29  INFO SparkContext: Invoking stop() from shutdown hook
23/03/08 20:15:29  INFO SparkUI: Stopped Spark web UI at http://192.168.3.13:4041
23/03/08 20:15:29  INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
23/03/08 20:15:29  INFO SparkContext: Successfully stopped SparkContext
23/03/08 20:15:29  INFO ShutdownHookManager: Shutdown hook called
23/03/08 20:15:29  INFO ShutdownHookManager: Deleting directory /private/var/folders/dm/0xljd5nn10b7bwwmwv8w5v100000gn/T/spark-6b7dc8c3-0866-4cd7-a00a-91ac361ffa87
23/03/08 20:15:29  INFO MLSQLLanguageServer: start....
23/03/08 20:15:30  WARN Utils: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 192.168.3.13 instead (on interface en0)
23/03/08 20:15:30  WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
23/03/08 20:15:30  INFO SparkRuntime: register HiveSqlDialect.....
23/03/08 20:15:30  INFO SparkRuntime: create Runtime...
23/03/08 20:15:30  INFO SparkRuntime: PSExecutor configured...
23/03/08 20:15:30  INFO log: Logging initialized @1795ms
23/03/08 20:15:30  WARN NetTool: Your hostname, localhost resolves to a loopback address: 127.0.0.1; using 192.168.3.13 instead (on interface en0)
23/03/08 20:15:30  WARN NetTool: Set LOCAL_IP if you need to bind to another address
23/03/08 20:15:30  INFO Server: jetty-9.2.z-SNAPSHOT
23/03/08 20:15:30  INFO ServerConnector: Started ServerConnector@7007af45{HTTP/1.1}{192.168.3.13:53334}
23/03/08 20:15:30  INFO Server: Started @1883ms
23/03/08 20:15:30  INFO NetTool: Successfully started service 'driver-log-server' on port 53334.
23/03/08 20:15:30  INFO SparkRuntime: DriverLogServer is started in http://192.168.3.13:53334/v2/writelog with token:94f6310e-2d02-4cec-8f27-2868a9018da9
23/03/08 20:15:30  INFO SparkContext: Running Spark version 3.3.0
23/03/08 20:15:30  WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
23/03/08 20:15:30  INFO ResourceUtils: ==============================================================
23/03/08 20:15:30  INFO ResourceUtils: No custom resources configured for spark.driver.
23/03/08 20:15:30  INFO ResourceUtils: ==============================================================
23/03/08 20:15:30  INFO SparkContext: Submitted application: MLSQL-desktop
23/03/08 20:15:30  INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
23/03/08 20:15:30  INFO ResourceProfile: Limiting resource is cpu
23/03/08 20:15:30  INFO ResourceProfileManager: Added ResourceProfile id: 0
23/03/08 20:15:30  INFO SecurityManager: Changing view acls to: allwefantasy
23/03/08 20:15:30  INFO SecurityManager: Changing modify acls to: allwefantasy
23/03/08 20:15:30  INFO SecurityManager: Changing view acls groups to: 
23/03/08 20:15:30  INFO SecurityManager: Changing modify acls groups to: 
23/03/08 20:15:30  INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(allwefantasy); groups with view permissions: Set(); users  with modify permissions: Set(allwefantasy); groups with modify permissions: Set()
23/03/08 20:15:31  INFO Utils: Successfully started service 'sparkDriver' on port 53335.
23/03/08 20:15:31  INFO SparkEnv: Registering MapOutputTracker
23/03/08 20:15:31  INFO SparkEnv: Registering BlockManagerMaster
23/03/08 20:15:31  INFO SparkEnv: Registering BlockManagerMasterHeartbeat
23/03/08 20:15:31  INFO SparkEnv: Registering OutputCommitCoordinator
23/03/08 20:15:31  WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
23/03/08 20:15:31  INFO Utils: Successfully started service 'SparkUI' on port 4041.
23/03/08 20:15:31  INFO PSExecutorPlugin: PSExecutorPlugin starting.....
23/03/08 20:15:31  INFO Utils: Successfully started service 'PSExecutorBackend' on port 53341.
23/03/08 20:15:31  INFO ExecutorPluginContainer: Initialized executor component for plugin org.apache.spark.ps.cluster.PSExecutorPlugin.
23/03/08 20:15:31  INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 53343.
23/03/08 20:15:31  INFO NettyBlockTransferService: Server created on 192.168.3.13:53343
23/03/08 20:15:32  INFO JobManager: JobManager started with initialDelay=30 checkTimeInterval=5
23/03/08 20:15:32  INFO SparkRuntime: PSDriver starting...
23/03/08 20:15:32  INFO PSDriverBackend: setup ps driver rpc env: 192.168.3.13:7780 clientMode=false
23/03/08 20:15:32  INFO Utils: Successfully started service 'PSDriverEndpoint' on port 7780.
23/03/08 20:15:32  INFO SparkRuntime: mlsql server start with configuration!
23/03/08 20:15:32  INFO SparkRuntime: --------------------------------------------------------------------------------------------------------------------------------------------------------------------------
23/03/08 20:15:32  INFO SparkRuntime: |streaming.plugin.clzznames                        |tech.mlsql.plugins.ds.MLSQLExcelApp,tech.mlsql.plugins.shell.app.MLSQLShell,tech.mlsql.plugins.assert.app.MLSQLAssert|
23/03/08 20:15:32  INFO SparkRuntime: |spark.sql.files.maxPartitionBytes                 |5242880                                                                                                              |
23/03/08 20:15:32  INFO SparkRuntime: |spark.io.compression.lz4.blockSize                |128k                                                                                                                 |
23/03/08 20:15:32  INFO SparkRuntime: |streaming.job.cancel                              |true                                                                                                                 |
23/03/08 20:15:32  INFO SparkRuntime: |spark.shuffle.spill.batchSize                     |1000                                                                                                                 |
23/03/08 20:15:32  INFO SparkRuntime: |spark.sql.pivotMaxValues                          |1000                                                                                                                 |
23/03/08 20:15:32  INFO SparkRuntime: |spark.sql.catalog.spark_catalog                   |org.apache.spark.sql.delta.catalog.DeltaCatalog                                                                      |
23/03/08 20:15:32  INFO SparkRuntime: |spark.memory.storageFraction                      |0.1                                                                                                                  |
23/03/08 20:15:32  INFO SparkRuntime: |spark.file.transferTo                             |false                                                                                                                |
23/03/08 20:15:32  INFO SparkRuntime: |spark.sql.parquet.columnarReaderBatchSize         |1000                                                                                                                 |
23/03/08 20:15:32  INFO SparkRuntime: |streaming.name                                    |MLSQL-desktop                                                                                                        |
23/03/08 20:15:32  INFO SparkRuntime: |streaming.rest                                    |true                                                                                                                 |
23/03/08 20:15:32  INFO SparkRuntime: |streaming.thrift                                  |false                                                                                                                |
23/03/08 20:15:32  INFO SparkRuntime: |spark.shuffle.unsafe.file.ouput.buffer            |1m                                                                                                                   |
23/03/08 20:15:32  INFO SparkRuntime: |streaming.master                                  |local[*]                                                                                                             |
23/03/08 20:15:32  INFO SparkRuntime: |streaming.driver.port                             |8002                                                                                                                 |
23/03/08 20:15:32  INFO SparkRuntime: |streaming.spark.service                           |true                                                                                                                 |
23/03/08 20:15:32  INFO SparkRuntime: |streaming.platform                                |spark                                                                                                                |
23/03/08 20:15:32  INFO SparkRuntime: |spark.unsafe.sorter.spill.reader.buffer.size      |1m                                                                                                                   |
23/03/08 20:15:32  INFO SparkRuntime: |spark.shuffle.spill.numElementsForceSpillThreshold|10000                                                                                                                |
23/03/08 20:15:32  INFO SparkRuntime: |spark.shuffle.accurateBlockThreshold              |5242880                                                                                                              |
23/03/08 20:15:32  INFO SparkRuntime: |streaming.datalake.path                           |./data                                                                                                               |
23/03/08 20:15:32  INFO SparkRuntime: |spark.shuffle.file.buffer                         |1m                                                                                                                   |
23/03/08 20:15:32  INFO SparkRuntime: |spark.sql.inMemoryColumnarStorage.batchSize       |1000                                                                                                                 |
23/03/08 20:15:32  INFO SparkRuntime: |spark.memory.fraction                             |0.1                                                                                                                  |
23/03/08 20:15:32  INFO SparkRuntime: |spark.sql.extensions                              |io.delta.sql.DeltaSparkSessionExtension                                                                              |
23/03/08 20:15:32  INFO SparkRuntime: |spark.sql.shuffle.partitions                      |8                                                                                                                    |
23/03/08 20:15:32  INFO SparkRuntime: --------------------------------------------------------------------------------------------------------------------------------------------------------------------------
23/03/08 20:15:32  INFO SparkSessionCacheManager: Scheduling SparkSession cache cleaning every 60 seconds
23/03/08 20:15:32  INFO SparkRuntime: register functions.....
23/03/08 20:15:32  INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
23/03/08 20:15:32  INFO SharedState: Warehouse path is 'file:/Users/allwefantasy/projects/byzer-ml-example/spark-warehouse'.
23/03/08 20:15:34  WARN SimpleFunctionRegistry: The function array_intersect replaced a previously registered function.
23/03/08 20:15:34  WARN SimpleFunctionRegistry: The function uuid replaced a previously registered function.
23/03/08 20:15:34  WARN SimpleFunctionRegistry: The function bytestringasgb replaced a previously registered function.
23/03/08 20:15:34  INFO SparkRuntime: register functions.....
23/03/08 20:15:34  WARN SimpleFunctionRegistry: The function rest_request replaced a previously registered function.
23/03/08 20:15:34  INFO SparkRuntime: register functions.....
23/03/08 20:15:34  WARN SimpleFunctionRegistry: The function aes_encrypt replaced a previously registered function.
23/03/08 20:15:34  WARN SimpleFunctionRegistry: The function aes_decrypt replaced a previously registered function.
23/03/08 20:15:34  INFO MLSQLStreamManager: Start streaming job monitor....
23/03/08 20:15:34  INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
23/03/08 20:15:35  INFO PSExecutorBackend: Connecting to driver: spark://ps-driver-endpoint@192.168.3.13:7780
23/03/08 20:15:35  INFO TransportClientFactory: Successfully created connection to /192.168.3.13:7780 after 120 ms (0 ms spent in bootstraps)
23/03/08 20:15:35  INFO PSExecutorBackend: 0@192.168.3.13 register with driver: spark://ps-driver-endpoint@192.168.3.13:7780 success
23/03/08 20:15:35  INFO impl: scan service package => null
23/03/08 20:15:35  INFO impl: load service in ServiceFramwork.serviceModules =>0
23/03/08 20:15:35  INFO impl: total load service  =>7
23/03/08 20:15:36  INFO impl: controller load :    streaming.rest.RestController
23/03/08 20:15:36  INFO impl: controller load :    streaming.rest.RestPredictController
23/03/08 20:15:36  INFO impl: controller load :    streaming.rest.HealthController
23/03/08 20:15:36  INFO Server: jetty-9.2.z-SNAPSHOT
23/03/08 20:15:36  INFO PluginHook: Load build-in plugins....
23/03/08 20:15:36  INFO ServerConnector: Started ServerConnector@77d894f6{HTTP/1.1}{0.0.0.0:8002}
23/03/08 20:15:36  INFO Server: Started @8187ms
23/03/08 20:15:37  INFO DelegatingLogStore: LogStore `LogStoreAdapter(io.delta.storage.HDFSLogStore)` is used for scheme `file`
23/03/08 20:15:37  INFO DeltaLog: Creating initial snapshot without metadata, because the directory is empty
23/03/08 20:15:37  INFO InitialSnapshot: [tableId=2db42072-ca3c-4074-95ba-4abb85b337c8] Created snapshot InitialSnapshot(path=file:/Users/allwefantasy/projects/byzer-ml-example/data/__instances__/MLSQL-desktop/__mlsql__/config/_delta_log, version=-1, metadata=Metadata(f6c70aea-e203-448b-92be-ab5f3aad1520,null,null,Format(parquet,Map()),null,List(),Map(),Some(1678277737349)), logSegment=LogSegment(file:/Users/allwefantasy/projects/byzer-ml-example/data/__instances__/MLSQL-desktop/__mlsql__/config/_delta_log,-1,List(),List(),None,-1), checksumOpt=None)
23/03/08 20:15:37  INFO CodeGenerator: Code generated in 204.841999 ms
23/03/08 20:15:38  INFO SparkContext: Starting job: collect at DeltaLakeDBStore.scala:24
23/03/08 20:15:38  INFO SnapshotTimer: Scheduler MLSQL state every 3 seconds
23/03/08 20:15:39  INFO MLSQLExcelApp: Load ds: tech.mlsql.plugins.ds.MLSQLExcel
23/03/08 20:15:40  INFO DelegatingLogStore: LogStore `LogStoreAdapter(io.delta.storage.HDFSLogStore)` is used for scheme `file`
23/03/08 20:15:40  INFO DeltaLog: Creating initial snapshot without metadata, because the directory is empty
23/03/08 20:15:40  INFO InitialSnapshot: [tableId=49d06735-a470-4828-98b7-fc579348df51] Created snapshot InitialSnapshot(path=file:/Users/allwefantasy/projects/byzer-ml-example/data/__instances__/MLSQL-desktop/__mlsql__/plugins/_delta_log, version=-1, metadata=Metadata(5dcb7b6e-f004-404f-b597-95ebe3f5565c,null,null,Format(parquet,Map()),null,List(),Map(),Some(1678277740465)), logSegment=LogSegment(file:/Users/allwefantasy/projects/byzer-ml-example/data/__instances__/MLSQL-desktop/__mlsql__/plugins/_delta_log,-1,List(),List(),None,-1), checksumOpt=None)
23/03/08 20:15:40  INFO DelegatingLogStore: LogStore `LogStoreAdapter(io.delta.storage.HDFSLogStore)` is used for scheme `file`
23/03/08 20:15:40  INFO DeltaLog: Creating initial snapshot without metadata, because the directory is empty
23/03/08 20:15:40  INFO InitialSnapshot: [tableId=b8ec6f24-4d29-4c9d-8d2b-ab568df538f0] Created snapshot InitialSnapshot(path=file:/Users/allwefantasy/projects/byzer-ml-example/data/__instances__/MLSQL-desktop/__mlsql__/etRecord/_delta_log, version=-1, metadata=Metadata(c706543a-de5f-4f5b-bba0-12ba599bcec7,null,null,Format(parquet,Map()),null,List(),Map(),Some(1678277740494)), logSegment=LogSegment(file:/Users/allwefantasy/projects/byzer-ml-example/data/__instances__/MLSQL-desktop/__mlsql__/etRecord/_delta_log,-1,List(),List(),None,-1), checksumOpt=None)
23/03/08 20:15:40  INFO DelegatingLogStore: LogStore `LogStoreAdapter(io.delta.storage.HDFSLogStore)` is used for scheme `file`
23/03/08 20:15:40  INFO DeltaLog: Creating initial snapshot without metadata, because the directory is empty
23/03/08 20:15:40  INFO InitialSnapshot: [tableId=1b5321fb-03d2-465b-a587-8d5cd2ab66ab] Created snapshot InitialSnapshot(path=file:/Users/allwefantasy/projects/byzer-ml-example/data/__instances__/MLSQL-desktop/__mlsql__/dsRecord/_delta_log, version=-1, metadata=Metadata(a23306de-317c-483d-8868-56c1311c21bd,null,null,Format(parquet,Map()),null,List(),Map(),Some(1678277740521)), logSegment=LogSegment(file:/Users/allwefantasy/projects/byzer-ml-example/data/__instances__/MLSQL-desktop/__mlsql__/dsRecord/_delta_log,-1,List(),List(),None,-1), checksumOpt=None)
23/03/08 20:15:40  INFO DelegatingLogStore: LogStore `LogStoreAdapter(io.delta.storage.HDFSLogStore)` is used for scheme `file`
23/03/08 20:15:40  INFO DeltaLog: Creating initial snapshot without metadata, because the directory is empty
23/03/08 20:15:40  INFO InitialSnapshot: [tableId=72293b74-8ba6-4aa4-9486-500b66a4c652] Created snapshot InitialSnapshot(path=file:/Users/allwefantasy/projects/byzer-ml-example/data/__instances__/MLSQL-desktop/__mlsql__/appRecord/_delta_log, version=-1, metadata=Metadata(108d8b5f-65ad-4ee8-ad7a-a50c244ac0a1,null,null,Format(parquet,Map()),null,List(),Map(),Some(1678277740543)), logSegment=LogSegment(file:/Users/allwefantasy/projects/byzer-ml-example/data/__instances__/MLSQL-desktop/__mlsql__/appRecord/_delta_log,-1,List(),List(),None,-1), checksumOpt=None)
23/03/08 20:15:46  INFO DefaultConsoleClient: [owner] [admin] [groupId] [8ad8a14c-0e20-4515-8b25-d4b162cc6eb6] __MMMMMM__ auth admin  want access tables: [{"db":"","table":"./example-data/iris.csv","operateType":{"i":1,"name":"load"},"sourceType":"csv","tableType":{"name":"hdfs","includes":["rate","json","image","parquet","csv","text","binlogRate","excel","libsvm","xml","delta","streamParquet"]}},{"table":"iris","operateType":{"i":1,"name":"load"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}}]
23/03/08 20:15:46  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [8ad8a14c-0e20-4515-8b25-d4b162cc6eb6] __MMMMMM__ Total jobs: 1 current job:1 job script:load csv.`./example-data/iris.csv` where header = "true"
and inferSchema = "true" 
as iris 
23/03/08 20:15:46  INFO InMemoryFileIndex: It took 30 ms to list leaf files for 1 paths.
23/03/08 20:15:46  INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
23/03/08 20:15:46  INFO FileSourceStrategy: Pushed Filters: 
23/03/08 20:15:46  INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#320, None)) > 0)
23/03/08 20:15:46  INFO FileSourceStrategy: Output Data Schema: struct<value: string>
23/03/08 20:15:47  INFO CodeGenerator: Code generated in 11.498472 ms
23/03/08 20:15:47  INFO SparkContext: Created broadcast 0 from load at MLSQLCSV.scala:65
23/03/08 20:15:47  INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 20:15:47  INFO SparkContext: Starting job: load at MLSQLCSV.scala:65
23/03/08 20:15:47  INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513
23/03/08 20:15:47  INFO FileScanRDD: Reading File path: file:///Users/allwefantasy/projects/byzer-ml-example/example-data/iris.csv, range: 0-4169, partition values: [empty row]
23/03/08 20:15:47  INFO CodeGenerator: Code generated in 9.504664 ms
23/03/08 20:15:47  INFO CodeGenerator: Code generated in 6.159014 ms
23/03/08 20:15:47  INFO FileSourceStrategy: Pushed Filters: 
23/03/08 20:15:47  INFO FileSourceStrategy: Post-Scan Filters: 
23/03/08 20:15:47  INFO FileSourceStrategy: Output Data Schema: struct<value: string>
23/03/08 20:15:47  INFO SparkContext: Created broadcast 2 from load at MLSQLCSV.scala:65
23/03/08 20:15:47  INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 20:15:48  INFO SparkContext: Starting job: load at MLSQLCSV.scala:65
23/03/08 20:15:48  INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513
23/03/08 20:15:48  INFO FileScanRDD: Reading File path: file:///Users/allwefantasy/projects/byzer-ml-example/example-data/iris.csv, range: 0-4169, partition values: [empty row]
23/03/08 20:15:48  INFO FileSourceStrategy: Pushed Filters: 
23/03/08 20:15:48  INFO FileSourceStrategy: Post-Scan Filters: 
23/03/08 20:15:48  INFO FileSourceStrategy: Output Data Schema: struct<sepal_length: double, sepal_width: double, petal_length: double, petal_width: double, species: string ... 1 more field>
23/03/08 20:15:48  INFO CodeGenerator: Code generated in 11.672306 ms
23/03/08 20:15:48  INFO SparkContext: Created broadcast 4 from take at RestController.scala:290
23/03/08 20:15:48  INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 20:15:48  INFO SparkContext: Starting job: take at RestController.scala:290
23/03/08 20:15:48  INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513
23/03/08 20:15:48  INFO CodeGenerator: Code generated in 9.379532 ms
23/03/08 20:15:48  INFO FileScanRDD: Reading File path: file:///Users/allwefantasy/projects/byzer-ml-example/example-data/iris.csv, range: 0-4169, partition values: [empty row]
23/03/08 20:15:48  INFO CodeGenerator: Code generated in 9.175407 ms
23/03/08 20:15:48  INFO CodeGenerator: Code generated in 41.747634 ms
23/03/08 20:15:48  INFO impl: Completed 200 in 1916ms 	POST /run/script



23/03/08 20:15:50  INFO DefaultConsoleClient: [owner] [admin] [groupId] [da14cdfc-88df-4055-90f6-bc8436bbdcec] __MMMMMM__ auth admin  want access tables: [{"table":"iris","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"iris_features","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}}]
23/03/08 20:15:50  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [da14cdfc-88df-4055-90f6-bc8436bbdcec] __MMMMMM__ Total jobs: 1 current job:1 job script:select array(sepal_length,sepal_width,petal_length,petal_width) as feature, cast(species_id as double) as label from iris 
as iris_features 
23/03/08 20:15:50  INFO FileSourceStrategy: Pushed Filters: 
23/03/08 20:15:50  INFO FileSourceStrategy: Post-Scan Filters: 
23/03/08 20:15:50  INFO FileSourceStrategy: Output Data Schema: struct<sepal_length: double, sepal_width: double, petal_length: double, petal_width: double, species_id: int ... 3 more fields>
23/03/08 20:15:50  INFO CodeGenerator: Code generated in 16.752769 ms
23/03/08 20:15:50  INFO CodeGenerator: Code generated in 12.18075 ms
23/03/08 20:15:50  INFO SparkContext: Created broadcast 6 from take at RestController.scala:290
23/03/08 20:15:50  INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 20:15:50  INFO SparkContext: Starting job: take at RestController.scala:290
23/03/08 20:15:50  INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513
23/03/08 20:15:50  INFO CodeGenerator: Code generated in 19.799819 ms
23/03/08 20:15:50  INFO FileScanRDD: Reading File path: file:///Users/allwefantasy/projects/byzer-ml-example/example-data/iris.csv, range: 0-4169, partition values: [empty row]
23/03/08 20:15:50  INFO CodeGenerator: Code generated in 10.203213 ms
23/03/08 20:15:50  INFO CodeGenerator: Code generated in 22.250939 ms
23/03/08 20:15:50  INFO impl: Completed 200 in 450ms 	POST /run/script



23/03/08 20:15:54  INFO DefaultConsoleClient: [owner] [admin] [groupId] [e8673837-6bfc-455e-8468-42affb1f8bd3] __MMMMMM__ auth admin  want access tables: [{"table":"iris_features","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"iris_features_split","operateType":{"i":1,"name":"load"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"train_data","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"test_data","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}}]
23/03/08 20:15:54  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [e8673837-6bfc-455e-8468-42affb1f8bd3] __MMMMMM__ Total jobs: 2 current job:1 job script:run iris_features as RateSampler.`` where sampleRate="0.9,0.1" and labelCol="label" as iris_features_split 
23/03/08 20:15:54  INFO FileSourceStrategy: Pushed Filters: 
23/03/08 20:15:54  INFO FileSourceStrategy: Post-Scan Filters: 
23/03/08 20:15:54  INFO FileSourceStrategy: Output Data Schema: struct<species_id: int>
23/03/08 20:15:54  INFO CodeGenerator: Code generated in 39.735903 ms
23/03/08 20:15:54  INFO SparkContext: Created broadcast 8 from collect at SQLRateSampler.scala:78
23/03/08 20:15:54  INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 20:15:54  INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513
23/03/08 20:15:55  INFO CodeGenerator: Code generated in 9.076948 ms
23/03/08 20:15:55  INFO CodeGenerator: Code generated in 4.482647 ms
23/03/08 20:15:55  INFO CodeGenerator: Code generated in 4.756928 ms
23/03/08 20:15:55  INFO CodeGenerator: Code generated in 5.009188 ms
23/03/08 20:15:55  INFO FileScanRDD: Reading File path: file:///Users/allwefantasy/projects/byzer-ml-example/example-data/iris.csv, range: 0-4169, partition values: [empty row]
23/03/08 20:15:55  INFO CodeGenerator: Code generated in 4.575266 ms
23/03/08 20:15:55  INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/03/08 20:15:55  INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
23/03/08 20:15:55  INFO CodeGenerator: Code generated in 15.971038 ms
23/03/08 20:15:55  INFO CodeGenerator: Code generated in 11.816629 ms
23/03/08 20:15:55  INFO SparkContext: Starting job: collect at SQLRateSampler.scala:78
23/03/08 20:15:55  INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1513
23/03/08 20:15:55  INFO CodeGenerator: Code generated in 4.361536 ms
23/03/08 20:15:55  INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513
23/03/08 20:15:55  INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/03/08 20:15:55  INFO CodeGenerator: Code generated in 7.671458 ms
23/03/08 20:15:55  INFO SparkContext: Starting job: collect at SQLRateSampler.scala:78
23/03/08 20:15:55  INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1513
23/03/08 20:15:55  INFO CodeGenerator: Code generated in 4.876688 ms
23/03/08 20:15:55  INFO CodeGenerator: Code generated in 4.443258 ms
23/03/08 20:15:55  INFO CodeGenerator: Code generated in 3.436576 ms
23/03/08 20:15:55  INFO SQLRateSampler: [owner] [admin] [groupId] [e8673837-6bfc-455e-8468-42affb1f8bd3] __MMMMMM__ computing data stat:1:50,3:50,2:50
23/03/08 20:15:55  INFO FileSourceStrategy: Pushed Filters: 
23/03/08 20:15:55  INFO FileSourceStrategy: Post-Scan Filters: 
23/03/08 20:15:55  INFO FileSourceStrategy: Output Data Schema: struct<sepal_length: double, sepal_width: double, petal_length: double, petal_width: double, species_id: int ... 3 more fields>
23/03/08 20:15:55  INFO SparkContext: Created broadcast 13 from rdd at SQLRateSampler.scala:116
23/03/08 20:15:55  INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 20:15:55  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [e8673837-6bfc-455e-8468-42affb1f8bd3] __MMMMMM__ Total jobs: 2 current job:2 job script:select * from iris_features_split where __split__=1 as test_data 
23/03/08 20:15:55  INFO CodeGenerator: Code generated in 6.424571 ms
23/03/08 20:15:55  INFO SparkContext: Starting job: take at RestController.scala:290
23/03/08 20:15:55  INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1513
23/03/08 20:15:55  INFO FileScanRDD: Reading File path: file:///Users/allwefantasy/projects/byzer-ml-example/example-data/iris.csv, range: 0-4169, partition values: [empty row]
23/03/08 20:15:55  INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513
23/03/08 20:15:55  INFO CodeGenerator: Code generated in 9.88017 ms
23/03/08 20:15:55  INFO CodeGenerator: Code generated in 14.941287 ms
23/03/08 20:15:55  INFO SparkContext: Starting job: take at RestController.scala:290
23/03/08 20:15:55  INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513
23/03/08 20:15:55  INFO impl: Completed 200 in 1142ms 	POST /run/script



23/03/08 20:16:13  INFO DefaultConsoleClient: [owner] [admin] [groupId] [178b28b9-2b91-4afa-a8c0-df8353c7b347] __MMMMMM__ auth admin  want access tables: [{"table":"iris","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"iris_features","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}}]
23/03/08 20:16:13  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [178b28b9-2b91-4afa-a8c0-df8353c7b347] __MMMMMM__ Total jobs: 1 current job:1 job script:select vec_dense(array(sepal_length,sepal_width,petal_length,petal_width)) as feature, cast(species_id as double) as label from iris 
as iris_features 
23/03/08 20:16:13  INFO FileSourceStrategy: Pushed Filters: 
23/03/08 20:16:13  INFO FileSourceStrategy: Post-Scan Filters: 
23/03/08 20:16:13  INFO FileSourceStrategy: Output Data Schema: struct<sepal_length: double, sepal_width: double, petal_length: double, petal_width: double, species_id: int ... 3 more fields>
23/03/08 20:16:13  INFO CodeGenerator: Code generated in 10.013698 ms
23/03/08 20:16:13  INFO SparkContext: Created broadcast 17 from take at RestController.scala:290
23/03/08 20:16:13  INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 20:16:13  INFO SparkContext: Starting job: take at RestController.scala:290
23/03/08 20:16:13  INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:13  INFO CodeGenerator: Code generated in 6.08648 ms
23/03/08 20:16:13  INFO FileScanRDD: Reading File path: file:///Users/allwefantasy/projects/byzer-ml-example/example-data/iris.csv, range: 0-4169, partition values: [empty row]
23/03/08 20:16:13  INFO CodeGenerator: Code generated in 8.880027 ms
23/03/08 20:16:13  INFO CodeGenerator: Code generated in 7.205111 ms
23/03/08 20:16:13  INFO CodeGenerator: Code generated in 14.342826 ms
23/03/08 20:16:13  INFO impl: Completed 200 in 387ms 	POST /run/script



23/03/08 20:16:16  INFO DefaultConsoleClient: [owner] [admin] [groupId] [14abc6cf-ac0a-4c66-bee6-7d5ae2d3779e] __MMMMMM__ auth admin  want access tables: [{"table":"iris_features","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"iris_features_split","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"train_data","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"test_data","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}}]
23/03/08 20:16:16  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [14abc6cf-ac0a-4c66-bee6-7d5ae2d3779e] __MMMMMM__ Total jobs: 2 current job:1 job script:run iris_features as RateSampler.`` where sampleRate="0.9,0.1" and labelCol="label" as iris_features_split 
23/03/08 20:16:16  INFO FileSourceStrategy: Pushed Filters: 
23/03/08 20:16:16  INFO FileSourceStrategy: Post-Scan Filters: 
23/03/08 20:16:16  INFO FileSourceStrategy: Output Data Schema: struct<species_id: int>
23/03/08 20:16:16  INFO SparkContext: Created broadcast 19 from collect at SQLRateSampler.scala:78
23/03/08 20:16:16  INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 20:16:16  INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:16  INFO FileScanRDD: Reading File path: file:///Users/allwefantasy/projects/byzer-ml-example/example-data/iris.csv, range: 0-4169, partition values: [empty row]
23/03/08 20:16:16  INFO ShufflePartitionsUtil: For shuffle(3), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/03/08 20:16:16  INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
23/03/08 20:16:16  INFO SparkContext: Starting job: collect at SQLRateSampler.scala:78
23/03/08 20:16:16  INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:16  INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:16  INFO ShufflePartitionsUtil: For shuffle(4), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
23/03/08 20:16:16  INFO SparkContext: Starting job: collect at SQLRateSampler.scala:78
23/03/08 20:16:16  INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:16  INFO SQLRateSampler: [owner] [admin] [groupId] [14abc6cf-ac0a-4c66-bee6-7d5ae2d3779e] __MMMMMM__ computing data stat:1:50,3:50,2:50
23/03/08 20:16:16  INFO FileSourceStrategy: Pushed Filters: 
23/03/08 20:16:16  INFO FileSourceStrategy: Post-Scan Filters: 
23/03/08 20:16:16  INFO FileSourceStrategy: Output Data Schema: struct<sepal_length: double, sepal_width: double, petal_length: double, petal_width: double, species_id: int ... 3 more fields>
23/03/08 20:16:16  INFO SparkContext: Created broadcast 24 from rdd at SQLRateSampler.scala:116
23/03/08 20:16:16  INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 20:16:16  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [14abc6cf-ac0a-4c66-bee6-7d5ae2d3779e] __MMMMMM__ Total jobs: 2 current job:2 job script:select * from iris_features_split where __split__=1 as test_data 
23/03/08 20:16:16  INFO CodeGenerator: Code generated in 26.805545 ms
23/03/08 20:16:16  INFO SparkContext: Starting job: take at RestController.scala:290
23/03/08 20:16:16  INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:16  INFO FileScanRDD: Reading File path: file:///Users/allwefantasy/projects/byzer-ml-example/example-data/iris.csv, range: 0-4169, partition values: [empty row]
23/03/08 20:16:16  INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:16  INFO CodeGenerator: Code generated in 5.492975 ms
23/03/08 20:16:16  INFO CodeGenerator: Code generated in 17.657726 ms
23/03/08 20:16:16  INFO SparkContext: Starting job: take at RestController.scala:290
23/03/08 20:16:16  INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:16  INFO impl: Completed 200 in 498ms 	POST /run/script



23/03/08 20:16:22  INFO DefaultConsoleClient: [owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ auth admin  want access tables: [{"table":"train_data","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}}]
23/03/08 20:16:22  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ Total jobs: 1 current job:1 job script:train train_data as RandomForest.`/tmp/iris_model` where
keepVersion="true" 
and evaluateTable="test_data"

and `fitParam.0.labelCol`="label"
and `fitParam.0.featuresCol`="features"
and `fitParam.0.maxDepth`="2"

and `fitParam.1.featuresCol`="features"
and `fitParam.1.labelCol`="label"
and `fitParam.1.maxDepth`="10" 
23/03/08 20:16:22  INFO SQLRandomForest: [owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ [training] [alg=org.apache.spark.ml.classification.RandomForestClassifier] [keepVersion=true]
23/03/08 20:16:22  INFO SQLRandomForest: [owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ java.lang.IllegalArgumentException: features does not exist. Available: feature, label, __split__
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.apache.spark.sql.types.StructType.$anonfun$apply$1(StructType.scala:282)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ scala.collection.immutable.Map$Map3.getOrElse(Map.scala:336)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.apache.spark.sql.types.StructType.apply(StructType.scala:281)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.apache.spark.ml.util.SchemaUtils$.checkColumnType(SchemaUtils.scala:42)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.apache.spark.ml.PredictorParams.validateAndTransformSchema(Predictor.scala:52)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.apache.spark.ml.PredictorParams.validateAndTransformSchema$(Predictor.scala:47)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.apache.spark.ml.classification.Classifier.org$apache$spark$ml$classification$ClassifierParams$$super$validateAndTransformSchema(Classifier.scala:73)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.apache.spark.ml.classification.ClassifierParams.validateAndTransformSchema(Classifier.scala:43)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.apache.spark.ml.classification.ClassifierParams.validateAndTransformSchema$(Classifier.scala:39)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.apache.spark.ml.classification.ProbabilisticClassifier.org$apache$spark$ml$classification$ProbabilisticClassifierParams$$super$validateAndTransformSchema(ProbabilisticClassifier.scala:51)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.apache.spark.ml.classification.ProbabilisticClassifierParams.validateAndTransformSchema(ProbabilisticClassifier.scala:38)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.apache.spark.ml.classification.ProbabilisticClassifierParams.validateAndTransformSchema$(ProbabilisticClassifier.scala:34)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.apache.spark.ml.classification.RandomForestClassifier.org$apache$spark$ml$tree$TreeEnsembleClassifierParams$$super$validateAndTransformSchema(RandomForestClassifier.scala:46)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.apache.spark.ml.tree.TreeEnsembleClassifierParams.validateAndTransformSchema(treeParams.scala:404)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.apache.spark.ml.tree.TreeEnsembleClassifierParams.validateAndTransformSchema$(treeParams.scala:400)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.apache.spark.ml.classification.RandomForestClassifier.validateAndTransformSchema(RandomForestClassifier.scala:46)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.apache.spark.ml.Predictor.transformSchema(Predictor.scala:177)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.apache.spark.ml.Predictor.fit(Predictor.scala:133)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.apache.spark.ml.Predictor.fit(Predictor.scala:115)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ streaming.dsl.mmlib.algs.Functions.$anonfun$trainModelsWithMultiParamGroup$2(Functions.scala:331)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ streaming.dsl.mmlib.algs.Functions.$anonfun$trainModelsWithMultiParamGroup$2$adapted(Functions.scala:319)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ streaming.dsl.mmlib.algs.Functions.$anonfun$trainModelsWithMultiParamGroup$7(Functions.scala:366)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ scala.collection.TraversableLike.map(TraversableLike.scala:286)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ scala.collection.TraversableLike.map$(TraversableLike.scala:279)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ streaming.dsl.mmlib.algs.Functions.trainModelsWithMultiParamGroup(Functions.scala:365)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ streaming.dsl.mmlib.algs.Functions.trainModelsWithMultiParamGroup$(Functions.scala:312)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ streaming.dsl.mmlib.algs.SQLRandomForest.trainModelsWithMultiParamGroup(SQLRandomForest.scala:34)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ streaming.dsl.mmlib.algs.SQLRandomForest.train(SQLRandomForest.scala:52)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ tech.mlsql.dsl.adaptor.TrainAdaptor.parse(TrainAdaptor.scala:116)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ streaming.dsl.ScriptSQLExecListener.execute$1(ScriptSQLExec.scala:406)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ streaming.dsl.ScriptSQLExecListener.exitSql(ScriptSQLExec.scala:445)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ streaming.dsl.parser.DSLSQLParser$SqlContext.exitRule(DSLSQLParser.java:296)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.antlr.v4.runtime.tree.ParseTreeWalker.exitRule(ParseTreeWalker.java:47)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.antlr.v4.runtime.tree.ParseTreeWalker.walk(ParseTreeWalker.java:30)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.antlr.v4.runtime.tree.ParseTreeWalker.walk(ParseTreeWalker.java:28)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ streaming.dsl.ScriptSQLExec$._parse(ScriptSQLExec.scala:159)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ streaming.dsl.ScriptSQLExec$.parse(ScriptSQLExec.scala:146)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ streaming.rest.RestController.$anonfun$script$9(RestController.scala:201)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ tech.mlsql.job.JobManager$.run(JobManager.scala:74)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ streaming.rest.RestController.query$1(RestController.scala:196)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ streaming.rest.RestController.script(RestController.scala:223)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ java.lang.reflect.Method.invoke(Method.java:498)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ net.csdn.modules.http.RestController.filter(RestController.java:139)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ net.csdn.modules.http.RestController.dispatchRequest(RestController.java:99)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ net.csdn.modules.http.HttpServer$DefaultHandler.handle(HttpServer.java:182)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:52)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:97)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.eclipse.jetty.server.Server.handle(Server.java:499)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:311)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:257)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.eclipse.jetty.io.AbstractConnection$2.run(AbstractConnection.java:544)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ java.lang.Thread.run(Thread.java:748)
23/03/08 20:16:22  INFO SQLRandomForest: [owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ [training] [alg=org.apache.spark.ml.classification.RandomForestClassifier] [keepVersion=true]
23/03/08 20:16:22  INFO SQLRandomForest: [owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ java.lang.IllegalArgumentException: features does not exist. Available: feature, label, __split__
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.apache.spark.sql.types.StructType.$anonfun$apply$1(StructType.scala:282)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ scala.collection.immutable.Map$Map3.getOrElse(Map.scala:336)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.apache.spark.sql.types.StructType.apply(StructType.scala:281)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.apache.spark.ml.util.SchemaUtils$.checkColumnType(SchemaUtils.scala:42)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.apache.spark.ml.PredictorParams.validateAndTransformSchema(Predictor.scala:52)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.apache.spark.ml.PredictorParams.validateAndTransformSchema$(Predictor.scala:47)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.apache.spark.ml.classification.Classifier.org$apache$spark$ml$classification$ClassifierParams$$super$validateAndTransformSchema(Classifier.scala:73)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.apache.spark.ml.classification.ClassifierParams.validateAndTransformSchema(Classifier.scala:43)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.apache.spark.ml.classification.ClassifierParams.validateAndTransformSchema$(Classifier.scala:39)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.apache.spark.ml.classification.ProbabilisticClassifier.org$apache$spark$ml$classification$ProbabilisticClassifierParams$$super$validateAndTransformSchema(ProbabilisticClassifier.scala:51)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.apache.spark.ml.classification.ProbabilisticClassifierParams.validateAndTransformSchema(ProbabilisticClassifier.scala:38)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.apache.spark.ml.classification.ProbabilisticClassifierParams.validateAndTransformSchema$(ProbabilisticClassifier.scala:34)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.apache.spark.ml.classification.RandomForestClassifier.org$apache$spark$ml$tree$TreeEnsembleClassifierParams$$super$validateAndTransformSchema(RandomForestClassifier.scala:46)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.apache.spark.ml.tree.TreeEnsembleClassifierParams.validateAndTransformSchema(treeParams.scala:404)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.apache.spark.ml.tree.TreeEnsembleClassifierParams.validateAndTransformSchema$(treeParams.scala:400)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.apache.spark.ml.classification.RandomForestClassifier.validateAndTransformSchema(RandomForestClassifier.scala:46)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.apache.spark.ml.Predictor.transformSchema(Predictor.scala:177)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.apache.spark.ml.Predictor.fit(Predictor.scala:133)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.apache.spark.ml.Predictor.fit(Predictor.scala:115)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ streaming.dsl.mmlib.algs.Functions.$anonfun$trainModelsWithMultiParamGroup$2(Functions.scala:331)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ streaming.dsl.mmlib.algs.Functions.$anonfun$trainModelsWithMultiParamGroup$2$adapted(Functions.scala:319)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ streaming.dsl.mmlib.algs.Functions.$anonfun$trainModelsWithMultiParamGroup$7(Functions.scala:366)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ scala.collection.TraversableLike.map(TraversableLike.scala:286)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ scala.collection.TraversableLike.map$(TraversableLike.scala:279)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ streaming.dsl.mmlib.algs.Functions.trainModelsWithMultiParamGroup(Functions.scala:365)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ streaming.dsl.mmlib.algs.Functions.trainModelsWithMultiParamGroup$(Functions.scala:312)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ streaming.dsl.mmlib.algs.SQLRandomForest.trainModelsWithMultiParamGroup(SQLRandomForest.scala:34)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ streaming.dsl.mmlib.algs.SQLRandomForest.train(SQLRandomForest.scala:52)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ tech.mlsql.dsl.adaptor.TrainAdaptor.parse(TrainAdaptor.scala:116)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ streaming.dsl.ScriptSQLExecListener.execute$1(ScriptSQLExec.scala:406)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ streaming.dsl.ScriptSQLExecListener.exitSql(ScriptSQLExec.scala:445)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ streaming.dsl.parser.DSLSQLParser$SqlContext.exitRule(DSLSQLParser.java:296)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.antlr.v4.runtime.tree.ParseTreeWalker.exitRule(ParseTreeWalker.java:47)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.antlr.v4.runtime.tree.ParseTreeWalker.walk(ParseTreeWalker.java:30)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.antlr.v4.runtime.tree.ParseTreeWalker.walk(ParseTreeWalker.java:28)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ streaming.dsl.ScriptSQLExec$._parse(ScriptSQLExec.scala:159)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ streaming.dsl.ScriptSQLExec$.parse(ScriptSQLExec.scala:146)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ streaming.rest.RestController.$anonfun$script$9(RestController.scala:201)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ tech.mlsql.job.JobManager$.run(JobManager.scala:74)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ streaming.rest.RestController.query$1(RestController.scala:196)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ streaming.rest.RestController.script(RestController.scala:223)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ java.lang.reflect.Method.invoke(Method.java:498)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ net.csdn.modules.http.RestController.filter(RestController.java:139)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ net.csdn.modules.http.RestController.dispatchRequest(RestController.java:99)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ net.csdn.modules.http.HttpServer$DefaultHandler.handle(HttpServer.java:182)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:52)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:97)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.eclipse.jetty.server.Server.handle(Server.java:499)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:311)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:257)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.eclipse.jetty.io.AbstractConnection$2.run(AbstractConnection.java:544)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555)
[owner] [admin] [groupId] [097dfa49-dc5e-42e2-a769-dfdd9312d332] __MMMMMM__ java.lang.Thread.run(Thread.java:748)
23/03/08 20:16:22  INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 20:16:22  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 20:16:22  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 20:16:22  INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 20:16:22  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 20:16:22  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 20:16:22  INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 20:16:22  INFO CodeGenerator: Code generated in 18.174563 ms
23/03/08 20:16:22  INFO SparkContext: Starting job: parquet at Functions.scala:388
23/03/08 20:16:22  INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:22  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 20:16:22  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 20:16:22  INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 20:16:22  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 20:16:22  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 20:16:22  INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 20:16:23  INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "modelPath",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "algIndex",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "alg",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "metrics",
    "type" : {
      "type" : "array",
      "elementType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "name",
          "type" : "string",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "value",
          "type" : "double",
          "nullable" : true,
          "metadata" : { }
        } ]
      },
      "containsNull" : true
    },
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "status",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "message",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "startTime",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "endTime",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trainParams",
    "type" : {
      "type" : "map",
      "keyType" : "string",
      "valueType" : "string",
      "valueContainsNull" : true
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary modelPath (STRING);
  optional int32 algIndex;
  optional binary alg (STRING);
  optional group metrics (LIST) {
    repeated group list {
      optional group element {
        optional binary name (STRING);
        optional double value;
      }
    }
  }
  optional binary status (STRING);
  optional binary message (STRING);
  optional int64 startTime;
  optional int64 endTime;
  optional group trainParams (MAP) {
    repeated group key_value {
      required binary key (STRING);
      optional binary value (STRING);
    }
  }
}

       
23/03/08 20:16:23  INFO CodecPool: Got brand-new compressor [.snappy]
23/03/08 20:16:23  INFO CodeGenerator: Code generated in 39.038462 ms
23/03/08 20:16:23  INFO FileOutputCommitter: Saved output of task 'attempt_202303082016225572590702476750224_0029_m_000000_21' to file:/tmp/iris_model/_model_1/meta/0/_temporary/0/task_202303082016225572590702476750224_0029_m_000000
23/03/08 20:16:23  INFO SparkHadoopMapRedUtil: attempt_202303082016225572590702476750224_0029_m_000000_21: Committed. Elapsed time: 0 ms.
23/03/08 20:16:23  INFO FileFormatWriter: Start to commit write Job fffa1967-bac7-46cc-a80c-289afcd22952.
23/03/08 20:16:23  INFO FileFormatWriter: Write Job fffa1967-bac7-46cc-a80c-289afcd22952 committed. Elapsed time: 20 ms.
23/03/08 20:16:23  INFO FileFormatWriter: Finished processing stats for write job fffa1967-bac7-46cc-a80c-289afcd22952.
23/03/08 20:16:23  INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
23/03/08 20:16:23  INFO SparkContext: Starting job: parquet at BaseParams.scala:80
23/03/08 20:16:23  INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:24  INFO FileSourceStrategy: Pushed Filters: 
23/03/08 20:16:24  INFO FileSourceStrategy: Post-Scan Filters: 
23/03/08 20:16:24  INFO FileSourceStrategy: Output Data Schema: struct<modelPath: string, algIndex: int, alg: string, metrics: array<struct<name:string,value:double>>, status: string ... 7 more fields>
23/03/08 20:16:24  INFO SparkContext: Created broadcast 30 from collect at MllibFunctions.scala:54
23/03/08 20:16:24  INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 20:16:24  INFO SparkContext: Starting job: collect at MllibFunctions.scala:54
23/03/08 20:16:24  INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:24  INFO CodeGenerator: Code generated in 6.839569 ms
23/03/08 20:16:24  INFO FileScanRDD: Reading File path: file:///tmp/iris_model/_model_1/meta/0/part-00000-0baf5ea4-04f9-4ace-8b31-dac1c821da4f-c000.snappy.parquet, range: 0-5943, partition values: [empty row]
23/03/08 20:16:24  INFO CodecPool: Got brand-new decompressor [.snappy]
23/03/08 20:16:24  INFO CodeGenerator: Code generated in 13.878757 ms
23/03/08 20:16:24  INFO CodeGenerator: Code generated in 4.451941 ms
23/03/08 20:16:24  INFO SparkContext: Starting job: take at RestController.scala:290
23/03/08 20:16:24  INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:24  INFO CodeGenerator: Code generated in 4.956584 ms
23/03/08 20:16:24  INFO CodeGenerator: Code generated in 8.856065 ms
23/03/08 20:16:24  INFO impl: Completed 200 in 1894ms 	POST /run/script



23/03/08 20:16:57  INFO DefaultConsoleClient: [owner] [admin] [groupId] [5d62f421-455a-4478-ad09-15eb0ed97903] __MMMMMM__ auth admin  want access tables: [{"table":"train_data","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}}]
23/03/08 20:16:57  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [5d62f421-455a-4478-ad09-15eb0ed97903] __MMMMMM__ Total jobs: 1 current job:1 job script:train train_data as RandomForest.`/tmp/iris_model` where
keepVersion="true" 
and evaluateTable="test_data"

and `fitParam.0.labelCol`="label"
and `fitParam.0.featuresCol`="feature"
and `fitParam.0.maxDepth`="2"

and `fitParam.1.featuresCol`="feature"
and `fitParam.1.labelCol`="label"
and `fitParam.1.maxDepth`="10" 
23/03/08 20:16:57  INFO SQLRandomForest: [owner] [admin] [groupId] [5d62f421-455a-4478-ad09-15eb0ed97903] __MMMMMM__ [training] [alg=org.apache.spark.ml.classification.RandomForestClassifier] [keepVersion=true]
23/03/08 20:16:57  INFO Instrumentation: [1a7121bd] Stage class: RandomForestClassifier
23/03/08 20:16:57  INFO Instrumentation: [1a7121bd] Stage uid: rfc_875b0f81f1b3
23/03/08 20:16:57  INFO CodeGenerator: Code generated in 6.24094 ms
23/03/08 20:16:57  INFO Instrumentation: [1a7121bd] training: numPartitions=3 storageLevel=StorageLevel(1 replicas)
23/03/08 20:16:57  INFO CodeGenerator: Code generated in 9.7484 ms
23/03/08 20:16:57  INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:57  INFO CodeGenerator: Code generated in 8.166575 ms
23/03/08 20:16:57  INFO SparkContext: Starting job: take at Classifier.scala:146
23/03/08 20:16:57  INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:57  INFO CodeGenerator: Code generated in 4.14372 ms
23/03/08 20:16:57  INFO RandomForestClassifier: org.apache.spark.ml.classification.RandomForestClassifier inferred 4 classes for labelCol=rfc_875b0f81f1b3__labelCol since numClasses was not specified in the column metadata.
23/03/08 20:16:57  INFO CodeGenerator: Code generated in 7.776566 ms
23/03/08 20:16:57  INFO Instrumentation: [1a7121bd] {"labelCol":"label","featuresCol":"feature","maxDepth":10}
23/03/08 20:16:57  INFO SparkContext: Starting job: take at DecisionTreeMetadata.scala:119
23/03/08 20:16:57  INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:57  INFO CodeGenerator: Code generated in 5.012976 ms
23/03/08 20:16:57  INFO SparkContext: Starting job: aggregate at DecisionTreeMetadata.scala:125
23/03/08 20:16:57  INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:57  INFO SparkContext: Starting job: collectAsMap at RandomForest.scala:1054
23/03/08 20:16:57  INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:57  INFO SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:57  INFO SparkContext: Created broadcast 39 from broadcast at RandomForest.scala:293
23/03/08 20:16:57  INFO Instrumentation: [1a7121bd] {"numFeatures":4}
23/03/08 20:16:57  INFO Instrumentation: [1a7121bd] {"numClasses":4}
23/03/08 20:16:57  INFO Instrumentation: [1a7121bd] {"numExamples":133}
23/03/08 20:16:57  INFO Instrumentation: [1a7121bd] {"sumOfWeights":133.0}
23/03/08 20:16:57  INFO SparkContext: Created broadcast 40 from broadcast at RandomForest.scala:622
23/03/08 20:16:57  INFO SparkContext: Starting job: collectAsMap at RandomForest.scala:663
23/03/08 20:16:57  INFO SparkContext: Created broadcast 41 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:57  INFO SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:58  INFO TorrentBroadcast: Destroying Broadcast(40) (from destroy at RandomForest.scala:674)
23/03/08 20:16:58  INFO SparkContext: Created broadcast 43 from broadcast at RandomForest.scala:622
23/03/08 20:16:58  INFO SparkContext: Starting job: collectAsMap at RandomForest.scala:663
23/03/08 20:16:58  INFO SparkContext: Created broadcast 44 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:58  INFO SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:58  INFO TorrentBroadcast: Destroying Broadcast(43) (from destroy at RandomForest.scala:674)
23/03/08 20:16:58  INFO SparkContext: Created broadcast 46 from broadcast at RandomForest.scala:622
23/03/08 20:16:58  INFO SparkContext: Starting job: collectAsMap at RandomForest.scala:663
23/03/08 20:16:58  INFO SparkContext: Created broadcast 47 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:58  INFO SparkContext: Created broadcast 48 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:58  INFO TorrentBroadcast: Destroying Broadcast(46) (from destroy at RandomForest.scala:674)
23/03/08 20:16:58  INFO SparkContext: Created broadcast 49 from broadcast at RandomForest.scala:622
23/03/08 20:16:58  INFO SparkContext: Starting job: collectAsMap at RandomForest.scala:663
23/03/08 20:16:58  INFO SparkContext: Created broadcast 50 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:58  INFO SparkContext: Created broadcast 51 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:58  INFO TorrentBroadcast: Destroying Broadcast(49) (from destroy at RandomForest.scala:674)
23/03/08 20:16:58  INFO SparkContext: Created broadcast 52 from broadcast at RandomForest.scala:622
23/03/08 20:16:58  INFO SparkContext: Starting job: collectAsMap at RandomForest.scala:663
23/03/08 20:16:58  INFO SparkContext: Created broadcast 53 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:58  INFO SparkContext: Created broadcast 54 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:58  INFO TorrentBroadcast: Destroying Broadcast(52) (from destroy at RandomForest.scala:674)
23/03/08 20:16:58  INFO SparkContext: Created broadcast 55 from broadcast at RandomForest.scala:622
23/03/08 20:16:58  INFO SparkContext: Starting job: collectAsMap at RandomForest.scala:663
23/03/08 20:16:58  INFO SparkContext: Created broadcast 56 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:58  INFO SparkContext: Created broadcast 57 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:58  INFO TorrentBroadcast: Destroying Broadcast(55) (from destroy at RandomForest.scala:674)
23/03/08 20:16:58  INFO SparkContext: Created broadcast 58 from broadcast at RandomForest.scala:622
23/03/08 20:16:58  INFO SparkContext: Starting job: collectAsMap at RandomForest.scala:663
23/03/08 20:16:58  INFO SparkContext: Created broadcast 59 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:58  INFO SparkContext: Created broadcast 60 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:58  INFO TorrentBroadcast: Destroying Broadcast(58) (from destroy at RandomForest.scala:674)
23/03/08 20:16:58  INFO SparkContext: Created broadcast 61 from broadcast at RandomForest.scala:622
23/03/08 20:16:58  INFO SparkContext: Starting job: collectAsMap at RandomForest.scala:663
23/03/08 20:16:58  INFO SparkContext: Created broadcast 62 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:58  INFO SparkContext: Created broadcast 63 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:58  INFO TorrentBroadcast: Destroying Broadcast(61) (from destroy at RandomForest.scala:674)
23/03/08 20:16:58  INFO RandomForest: Internal timing for DecisionTree:
23/03/08 20:16:58  INFO RandomForest:   init: 0.003702743
  total: 0.598595919
  findBestSplits: 0.585293526
  chooseSplits: 0.575545931
23/03/08 20:16:58  INFO MapPartitionsRDD: Removing RDD 126 from persistence list
23/03/08 20:16:58  INFO TorrentBroadcast: Destroying Broadcast(39) (from destroy at RandomForest.scala:305)
23/03/08 20:16:58  INFO Instrumentation: [1a7121bd] {"numClasses":4}
23/03/08 20:16:58  INFO Instrumentation: [1a7121bd] {"numFeatures":4}
23/03/08 20:16:58  INFO CodeGenerator: Code generated in 8.411755 ms
23/03/08 20:16:58  INFO Instrumentation: [1a7121bd] training finished
23/03/08 20:16:58  INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir
23/03/08 20:16:58  INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
23/03/08 20:16:58  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 20:16:58  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 20:16:58  INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83
23/03/08 20:16:58  INFO SparkContext: Created broadcast 64 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:58  INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
23/03/08 20:16:58  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 20:16:58  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 20:16:58  INFO FileOutputCommitter: Saved output of task 'attempt_20230308201658399864519360363626_0157_m_000000_0' to file:/tmp/iris_model/_model_2/model/1/metadata/_temporary/0/task_20230308201658399864519360363626_0157_m_000000
23/03/08 20:16:58  INFO SparkHadoopMapRedUtil: attempt_20230308201658399864519360363626_0157_m_000000_0: Committed. Elapsed time: 0 ms.
23/03/08 20:16:58  INFO SparkHadoopWriter: Start to commit write Job job_20230308201658399864519360363626_0157.
23/03/08 20:16:58  INFO SparkHadoopWriter: Write Job job_20230308201658399864519360363626_0157 committed. Elapsed time: 21 ms.
23/03/08 20:16:58  INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 20:16:58  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 20:16:58  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 20:16:58  INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 20:16:58  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 20:16:58  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 20:16:58  INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 20:16:58  INFO CodeGenerator: Code generated in 3.350937 ms
23/03/08 20:16:58  INFO SparkContext: Created broadcast 65 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:58  INFO SparkContext: Starting job: parquet at treeModels.scala:483
23/03/08 20:16:58  INFO SparkContext: Created broadcast 66 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:58  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 20:16:58  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 20:16:58  INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 20:16:58  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 20:16:58  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 20:16:58  INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 20:16:58  INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "treeID",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "metadata",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "weights",
    "type" : "double",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  required int32 treeID;
  optional binary metadata (STRING);
  required double weights;
}

       
23/03/08 20:16:58  INFO FileOutputCommitter: Saved output of task 'attempt_202303082016586247453408813283344_0072_m_000000_104' to file:/tmp/iris_model/_model_2/model/1/treesMetadata/_temporary/0/task_202303082016586247453408813283344_0072_m_000000
23/03/08 20:16:58  INFO SparkHadoopMapRedUtil: attempt_202303082016586247453408813283344_0072_m_000000_104: Committed. Elapsed time: 0 ms.
23/03/08 20:16:58  INFO FileFormatWriter: Start to commit write Job d8bba818-c5a9-4eae-a169-6e29472a052c.
23/03/08 20:16:59  INFO FileFormatWriter: Write Job d8bba818-c5a9-4eae-a169-6e29472a052c committed. Elapsed time: 16 ms.
23/03/08 20:16:59  INFO FileFormatWriter: Finished processing stats for write job d8bba818-c5a9-4eae-a169-6e29472a052c.
23/03/08 20:16:59  INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 20:16:59  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 20:16:59  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 20:16:59  INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 20:16:59  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 20:16:59  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 20:16:59  INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 20:16:59  INFO CodeGenerator: Code generated in 21.063017 ms
23/03/08 20:16:59  INFO SparkContext: Created broadcast 67 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:59  INFO SparkContext: Starting job: parquet at treeModels.scala:491
23/03/08 20:16:59  INFO SparkContext: Created broadcast 68 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:59  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 20:16:59  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 20:16:59  INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 20:16:59  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 20:16:59  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 20:16:59  INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 20:16:59  INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "treeID",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "nodeData",
    "type" : {
      "type" : "struct",
      "fields" : [ {
        "name" : "id",
        "type" : "integer",
        "nullable" : false,
        "metadata" : { }
      }, {
        "name" : "prediction",
        "type" : "double",
        "nullable" : false,
        "metadata" : { }
      }, {
        "name" : "impurity",
        "type" : "double",
        "nullable" : false,
        "metadata" : { }
      }, {
        "name" : "impurityStats",
        "type" : {
          "type" : "array",
          "elementType" : "double",
          "containsNull" : false
        },
        "nullable" : true,
        "metadata" : { }
      }, {
        "name" : "rawCount",
        "type" : "long",
        "nullable" : false,
        "metadata" : { }
      }, {
        "name" : "gain",
        "type" : "double",
        "nullable" : false,
        "metadata" : { }
      }, {
        "name" : "leftChild",
        "type" : "integer",
        "nullable" : false,
        "metadata" : { }
      }, {
        "name" : "rightChild",
        "type" : "integer",
        "nullable" : false,
        "metadata" : { }
      }, {
        "name" : "split",
        "type" : {
          "type" : "struct",
          "fields" : [ {
            "name" : "featureIndex",
            "type" : "integer",
            "nullable" : false,
            "metadata" : { }
          }, {
            "name" : "leftCategoriesOrThreshold",
            "type" : {
              "type" : "array",
              "elementType" : "double",
              "containsNull" : false
            },
            "nullable" : true,
            "metadata" : { }
          }, {
            "name" : "numCategories",
            "type" : "integer",
            "nullable" : false,
            "metadata" : { }
          } ]
        },
        "nullable" : true,
        "metadata" : { }
      } ]
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  required int32 treeID;
  optional group nodeData {
    required int32 id;
    required double prediction;
    required double impurity;
    optional group impurityStats (LIST) {
      repeated group list {
        required double element;
      }
    }
    required int64 rawCount;
    required double gain;
    required int32 leftChild;
    required int32 rightChild;
    optional group split {
      required int32 featureIndex;
      optional group leftCategoriesOrThreshold (LIST) {
        repeated group list {
          required double element;
        }
      }
      required int32 numCategories;
    }
  }
}

       
23/03/08 20:16:59  INFO FileOutputCommitter: Saved output of task 'attempt_202303082016597507058983094844922_0075_m_000000_121' to file:/tmp/iris_model/_model_2/model/1/data/_temporary/0/task_202303082016597507058983094844922_0075_m_000000
23/03/08 20:16:59  INFO SparkHadoopMapRedUtil: attempt_202303082016597507058983094844922_0075_m_000000_121: Committed. Elapsed time: 0 ms.
23/03/08 20:16:59  INFO FileFormatWriter: Start to commit write Job 58a5e0fb-6c64-44e4-86a9-9f982032b5e8.
23/03/08 20:16:59  INFO FileFormatWriter: Write Job 58a5e0fb-6c64-44e4-86a9-9f982032b5e8 committed. Elapsed time: 14 ms.
23/03/08 20:16:59  INFO FileFormatWriter: Finished processing stats for write job 58a5e0fb-6c64-44e4-86a9-9f982032b5e8.
23/03/08 20:16:59  INFO CodeGenerator: Code generated in 13.072973 ms
23/03/08 20:16:59  INFO SparkContext: Starting job: collectAsMap at MulticlassMetrics.scala:61
23/03/08 20:16:59  INFO SparkContext: Created broadcast 69 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:59  INFO CodeGenerator: Code generated in 4.570498 ms
23/03/08 20:16:59  INFO CodeGenerator: Code generated in 3.819631 ms
23/03/08 20:16:59  INFO SparkContext: Created broadcast 70 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:59  INFO SparkContext: Starting job: collectAsMap at MulticlassMetrics.scala:61
23/03/08 20:16:59  INFO SparkContext: Created broadcast 71 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:59  INFO SparkContext: Created broadcast 72 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:59  INFO SparkContext: Starting job: collectAsMap at MulticlassMetrics.scala:61
23/03/08 20:16:59  INFO SparkContext: Created broadcast 73 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:59  INFO SparkContext: Created broadcast 74 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:59  INFO SparkContext: Starting job: collectAsMap at MulticlassMetrics.scala:61
23/03/08 20:16:59  INFO SparkContext: Created broadcast 75 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:59  INFO SparkContext: Created broadcast 76 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:59  INFO SQLRandomForest: [owner] [admin] [groupId] [5d62f421-455a-4478-ad09-15eb0ed97903] __MMMMMM__ [trained] [alg=org.apache.spark.ml.classification.RandomForestClassifier] [metrics=List(MetricValue(f1,1.0), MetricValue(weightedPrecision,1.0), MetricValue(weightedRecall,1.0), MetricValue(accuracy,1.0))] [model hyperparameters=bootstrap: Whether bootstrap samples are used when building trees. (default: true)	cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. (default: false)	checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext (default: 10)	featureSubsetStrategy: The number of features to consider for splits at each tree node. Supported options: auto, all, onethird, sqrt, log2, (0.0-1.0], [1-n]. (default: auto)	featuresCol: features column name (default: features, current: feature)	impurity: Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini (default: gini)	labelCol: label column name (default: label, current: label)	leafCol: Leaf indices column name. Predicted leaf index of each instance in each tree by preorder (default: )	maxBins: Max number of bins for discretizing continuous features.  Must be at least 2 and at least number of categories for any categorical feature. (default: 32)	maxDepth: Maximum depth of the tree. (Nonnegative) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30]. (default: 5, current: 10)	maxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. (default: 256)	minInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)	minInstancesPerNode: Minimum number of instances each child must have after split.  If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Must be at least 1. (default: 1)	minWeightFractionPerNode: Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5) (default: 0.0)	numTrees: Number of trees to train (at least 1) (default: 20)	predictionCol: prediction column name (default: prediction)	probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities (default: probability)	rawPredictionCol: raw prediction (a.k.a. confidence) column name (default: rawPrediction)	seed: random seed (default: 207336481)	subsamplingRate: Fraction of the training data used for learning each decision tree, in range (0, 1]. (default: 1.0)	thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold (undefined)	weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0 (undefined)]
23/03/08 20:16:59  INFO SQLRandomForest: [owner] [admin] [groupId] [5d62f421-455a-4478-ad09-15eb0ed97903] __MMMMMM__ [training] [alg=org.apache.spark.ml.classification.RandomForestClassifier] [keepVersion=true]
23/03/08 20:16:59  INFO Instrumentation: [b990853f] Stage class: RandomForestClassifier
23/03/08 20:16:59  INFO Instrumentation: [b990853f] Stage uid: rfc_059d1c8ed623
23/03/08 20:16:59  INFO Instrumentation: [b990853f] training: numPartitions=3 storageLevel=StorageLevel(1 replicas)
23/03/08 20:16:59  INFO SparkContext: Created broadcast 77 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:59  INFO SparkContext: Starting job: take at Classifier.scala:146
23/03/08 20:16:59  INFO SparkContext: Created broadcast 78 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:59  INFO RandomForestClassifier: org.apache.spark.ml.classification.RandomForestClassifier inferred 4 classes for labelCol=rfc_059d1c8ed623__labelCol since numClasses was not specified in the column metadata.
23/03/08 20:16:59  INFO Instrumentation: [b990853f] {"labelCol":"label","featuresCol":"feature","maxDepth":2}
23/03/08 20:16:59  INFO SparkContext: Starting job: take at DecisionTreeMetadata.scala:119
23/03/08 20:16:59  INFO SparkContext: Created broadcast 79 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:59  INFO SparkContext: Starting job: aggregate at DecisionTreeMetadata.scala:125
23/03/08 20:16:59  INFO SparkContext: Created broadcast 80 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:59  INFO SparkContext: Starting job: collectAsMap at RandomForest.scala:1054
23/03/08 20:16:59  INFO SparkContext: Created broadcast 81 from broadcast at DAGScheduler.scala:1513
23/03/08 20:16:59  INFO SparkContext: Created broadcast 82 from broadcast at DAGScheduler.scala:1513
23/03/08 20:17:00  INFO SparkContext: Created broadcast 83 from broadcast at RandomForest.scala:293
23/03/08 20:17:00  INFO Instrumentation: [b990853f] {"numFeatures":4}
23/03/08 20:17:00  INFO Instrumentation: [b990853f] {"numClasses":4}
23/03/08 20:17:00  INFO Instrumentation: [b990853f] {"numExamples":130}
23/03/08 20:17:00  INFO Instrumentation: [b990853f] {"sumOfWeights":130.0}
23/03/08 20:17:00  INFO SparkContext: Created broadcast 84 from broadcast at RandomForest.scala:622
23/03/08 20:17:00  INFO SparkContext: Starting job: collectAsMap at RandomForest.scala:663
23/03/08 20:17:00  INFO SparkContext: Created broadcast 85 from broadcast at DAGScheduler.scala:1513
23/03/08 20:17:00  INFO SparkContext: Created broadcast 86 from broadcast at DAGScheduler.scala:1513
23/03/08 20:17:00  INFO TorrentBroadcast: Destroying Broadcast(84) (from destroy at RandomForest.scala:674)
23/03/08 20:17:00  INFO SparkContext: Created broadcast 87 from broadcast at RandomForest.scala:622
23/03/08 20:17:00  INFO SparkContext: Starting job: collectAsMap at RandomForest.scala:663
23/03/08 20:17:00  INFO SparkContext: Created broadcast 88 from broadcast at DAGScheduler.scala:1513
23/03/08 20:17:00  INFO SparkContext: Created broadcast 89 from broadcast at DAGScheduler.scala:1513
23/03/08 20:17:00  INFO TorrentBroadcast: Destroying Broadcast(87) (from destroy at RandomForest.scala:674)
23/03/08 20:17:00  INFO RandomForest: Internal timing for DecisionTree:
23/03/08 20:17:00  INFO RandomForest:   init: 4.3853E-5
  total: 0.07868226
  findBestSplits: 0.077643235
  chooseSplits: 0.076737083
23/03/08 20:17:00  INFO MapPartitionsRDD: Removing RDD 218 from persistence list
23/03/08 20:17:00  INFO TorrentBroadcast: Destroying Broadcast(83) (from destroy at RandomForest.scala:305)
23/03/08 20:17:00  INFO Instrumentation: [b990853f] {"numClasses":4}
23/03/08 20:17:00  INFO Instrumentation: [b990853f] {"numFeatures":4}
23/03/08 20:17:00  INFO Instrumentation: [b990853f] training finished
23/03/08 20:17:00  INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
23/03/08 20:17:00  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 20:17:00  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 20:17:00  INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83
23/03/08 20:17:00  INFO SparkContext: Created broadcast 90 from broadcast at DAGScheduler.scala:1513
23/03/08 20:17:00  INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter
23/03/08 20:17:00  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 20:17:00  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 20:17:00  INFO FileOutputCommitter: Saved output of task 'attempt_202303082017004926101151973568786_0231_m_000000_0' to file:/tmp/iris_model/_model_2/model/0/metadata/_temporary/0/task_202303082017004926101151973568786_0231_m_000000
23/03/08 20:17:00  INFO SparkHadoopMapRedUtil: attempt_202303082017004926101151973568786_0231_m_000000_0: Committed. Elapsed time: 0 ms.
23/03/08 20:17:00  INFO SparkHadoopWriter: Start to commit write Job job_202303082017004926101151973568786_0231.
23/03/08 20:17:00  INFO SparkHadoopWriter: Write Job job_202303082017004926101151973568786_0231 committed. Elapsed time: 12 ms.
23/03/08 20:17:00  INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 20:17:00  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 20:17:00  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 20:17:00  INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 20:17:00  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 20:17:00  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 20:17:00  INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 20:17:00  INFO SparkContext: Created broadcast 91 from broadcast at DAGScheduler.scala:1513
23/03/08 20:17:00  INFO SparkContext: Starting job: parquet at treeModels.scala:483
23/03/08 20:17:00  INFO SparkContext: Created broadcast 92 from broadcast at DAGScheduler.scala:1513
23/03/08 20:17:00  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 20:17:00  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 20:17:00  INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 20:17:00  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 20:17:00  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 20:17:00  INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 20:17:00  INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "treeID",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "metadata",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "weights",
    "type" : "double",
    "nullable" : false,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  required int32 treeID;
  optional binary metadata (STRING);
  required double weights;
}

       
23/03/08 20:17:00  INFO FileOutputCommitter: Saved output of task 'attempt_202303082017003168251876915852306_0109_m_000000_189' to file:/tmp/iris_model/_model_2/model/0/treesMetadata/_temporary/0/task_202303082017003168251876915852306_0109_m_000000
23/03/08 20:17:00  INFO SparkHadoopMapRedUtil: attempt_202303082017003168251876915852306_0109_m_000000_189: Committed. Elapsed time: 0 ms.
23/03/08 20:17:00  INFO FileFormatWriter: Start to commit write Job 19ac17c7-15bc-4386-82e4-7a9c9f2ec7bc.
23/03/08 20:17:00  INFO FileFormatWriter: Write Job 19ac17c7-15bc-4386-82e4-7a9c9f2ec7bc committed. Elapsed time: 11 ms.
23/03/08 20:17:00  INFO FileFormatWriter: Finished processing stats for write job 19ac17c7-15bc-4386-82e4-7a9c9f2ec7bc.
23/03/08 20:17:00  INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 20:17:00  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 20:17:00  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 20:17:00  INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 20:17:00  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 20:17:00  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 20:17:00  INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 20:17:00  INFO SparkContext: Created broadcast 93 from broadcast at DAGScheduler.scala:1513
23/03/08 20:17:00  INFO SparkContext: Starting job: parquet at treeModels.scala:491
23/03/08 20:17:00  INFO SparkContext: Created broadcast 94 from broadcast at DAGScheduler.scala:1513
23/03/08 20:17:00  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 20:17:00  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 20:17:00  INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 20:17:00  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 20:17:00  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 20:17:00  INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 20:17:00  INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "treeID",
    "type" : "integer",
    "nullable" : false,
    "metadata" : { }
  }, {
    "name" : "nodeData",
    "type" : {
      "type" : "struct",
      "fields" : [ {
        "name" : "id",
        "type" : "integer",
        "nullable" : false,
        "metadata" : { }
      }, {
        "name" : "prediction",
        "type" : "double",
        "nullable" : false,
        "metadata" : { }
      }, {
        "name" : "impurity",
        "type" : "double",
        "nullable" : false,
        "metadata" : { }
      }, {
        "name" : "impurityStats",
        "type" : {
          "type" : "array",
          "elementType" : "double",
          "containsNull" : false
        },
        "nullable" : true,
        "metadata" : { }
      }, {
        "name" : "rawCount",
        "type" : "long",
        "nullable" : false,
        "metadata" : { }
      }, {
        "name" : "gain",
        "type" : "double",
        "nullable" : false,
        "metadata" : { }
      }, {
        "name" : "leftChild",
        "type" : "integer",
        "nullable" : false,
        "metadata" : { }
      }, {
        "name" : "rightChild",
        "type" : "integer",
        "nullable" : false,
        "metadata" : { }
      }, {
        "name" : "split",
        "type" : {
          "type" : "struct",
          "fields" : [ {
            "name" : "featureIndex",
            "type" : "integer",
            "nullable" : false,
            "metadata" : { }
          }, {
            "name" : "leftCategoriesOrThreshold",
            "type" : {
              "type" : "array",
              "elementType" : "double",
              "containsNull" : false
            },
            "nullable" : true,
            "metadata" : { }
          }, {
            "name" : "numCategories",
            "type" : "integer",
            "nullable" : false,
            "metadata" : { }
          } ]
        },
        "nullable" : true,
        "metadata" : { }
      } ]
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  required int32 treeID;
  optional group nodeData {
    required int32 id;
    required double prediction;
    required double impurity;
    optional group impurityStats (LIST) {
      repeated group list {
        required double element;
      }
    }
    required int64 rawCount;
    required double gain;
    required int32 leftChild;
    required int32 rightChild;
    optional group split {
      required int32 featureIndex;
      optional group leftCategoriesOrThreshold (LIST) {
        repeated group list {
          required double element;
        }
      }
      required int32 numCategories;
    }
  }
}

       
23/03/08 20:17:00  INFO FileOutputCommitter: Saved output of task 'attempt_202303082017006070484542639701713_0112_m_000000_206' to file:/tmp/iris_model/_model_2/model/0/data/_temporary/0/task_202303082017006070484542639701713_0112_m_000000
23/03/08 20:17:00  INFO SparkHadoopMapRedUtil: attempt_202303082017006070484542639701713_0112_m_000000_206: Committed. Elapsed time: 0 ms.
23/03/08 20:17:00  INFO FileFormatWriter: Start to commit write Job 009ffcac-14e3-498f-9fd8-db2343cd5304.
23/03/08 20:17:00  INFO FileFormatWriter: Write Job 009ffcac-14e3-498f-9fd8-db2343cd5304 committed. Elapsed time: 13 ms.
23/03/08 20:17:00  INFO FileFormatWriter: Finished processing stats for write job 009ffcac-14e3-498f-9fd8-db2343cd5304.
23/03/08 20:17:00  INFO SparkContext: Starting job: collectAsMap at MulticlassMetrics.scala:61
23/03/08 20:17:00  INFO SparkContext: Created broadcast 95 from broadcast at DAGScheduler.scala:1513
23/03/08 20:17:00  INFO SparkContext: Created broadcast 96 from broadcast at DAGScheduler.scala:1513
23/03/08 20:17:00  INFO SparkContext: Starting job: collectAsMap at MulticlassMetrics.scala:61
23/03/08 20:17:00  INFO SparkContext: Created broadcast 97 from broadcast at DAGScheduler.scala:1513
23/03/08 20:17:00  INFO SparkContext: Created broadcast 98 from broadcast at DAGScheduler.scala:1513
23/03/08 20:17:00  INFO SparkContext: Starting job: collectAsMap at MulticlassMetrics.scala:61
23/03/08 20:17:00  INFO SparkContext: Created broadcast 99 from broadcast at DAGScheduler.scala:1513
23/03/08 20:17:00  INFO SparkContext: Created broadcast 100 from broadcast at DAGScheduler.scala:1513
23/03/08 20:17:00  INFO SparkContext: Starting job: collectAsMap at MulticlassMetrics.scala:61
23/03/08 20:17:00  INFO SparkContext: Created broadcast 101 from broadcast at DAGScheduler.scala:1513
23/03/08 20:17:00  INFO SparkContext: Created broadcast 102 from broadcast at DAGScheduler.scala:1513
23/03/08 20:17:00  INFO SQLRandomForest: [owner] [admin] [groupId] [5d62f421-455a-4478-ad09-15eb0ed97903] __MMMMMM__ [trained] [alg=org.apache.spark.ml.classification.RandomForestClassifier] [metrics=List(MetricValue(f1,0.9217948717948717), MetricValue(weightedPrecision,1.0), MetricValue(weightedRecall,1.0), MetricValue(accuracy,1.0))] [model hyperparameters=bootstrap: Whether bootstrap samples are used when building trees. (default: true)	cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. (default: false)	checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext (default: 10)	featureSubsetStrategy: The number of features to consider for splits at each tree node. Supported options: auto, all, onethird, sqrt, log2, (0.0-1.0], [1-n]. (default: auto)	featuresCol: features column name (default: features, current: feature)	impurity: Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini (default: gini)	labelCol: label column name (default: label, current: label)	leafCol: Leaf indices column name. Predicted leaf index of each instance in each tree by preorder (default: )	maxBins: Max number of bins for discretizing continuous features.  Must be at least 2 and at least number of categories for any categorical feature. (default: 32)	maxDepth: Maximum depth of the tree. (Nonnegative) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30]. (default: 5, current: 2)	maxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. (default: 256)	minInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)	minInstancesPerNode: Minimum number of instances each child must have after split.  If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Must be at least 1. (default: 1)	minWeightFractionPerNode: Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5) (default: 0.0)	numTrees: Number of trees to train (at least 1) (default: 20)	predictionCol: prediction column name (default: prediction)	probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities (default: probability)	rawPredictionCol: raw prediction (a.k.a. confidence) column name (default: rawPrediction)	seed: random seed (default: 207336481)	subsamplingRate: Fraction of the training data used for learning each decision tree, in range (0, 1]. (default: 1.0)	thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0 excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold (undefined)	weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0 (undefined)]
23/03/08 20:17:00  INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 20:17:00  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 20:17:00  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 20:17:00  INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 20:17:00  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 20:17:00  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 20:17:00  INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 20:17:01  INFO SparkContext: Starting job: parquet at Functions.scala:388
23/03/08 20:17:01  INFO SparkContext: Created broadcast 103 from broadcast at DAGScheduler.scala:1513
23/03/08 20:17:01  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 20:17:01  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 20:17:01  INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 20:17:01  INFO FileOutputCommitter: File Output Committer Algorithm version is 1
23/03/08 20:17:01  INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
23/03/08 20:17:01  INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
23/03/08 20:17:01  INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "modelPath",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "algIndex",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "alg",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "metrics",
    "type" : {
      "type" : "array",
      "elementType" : {
        "type" : "struct",
        "fields" : [ {
          "name" : "name",
          "type" : "string",
          "nullable" : true,
          "metadata" : { }
        }, {
          "name" : "value",
          "type" : "double",
          "nullable" : true,
          "metadata" : { }
        } ]
      },
      "containsNull" : true
    },
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "status",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "message",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "startTime",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "endTime",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "trainParams",
    "type" : {
      "type" : "map",
      "keyType" : "string",
      "valueType" : "string",
      "valueContainsNull" : true
    },
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional binary modelPath (STRING);
  optional int32 algIndex;
  optional binary alg (STRING);
  optional group metrics (LIST) {
    repeated group list {
      optional group element {
        optional binary name (STRING);
        optional double value;
      }
    }
  }
  optional binary status (STRING);
  optional binary message (STRING);
  optional int64 startTime;
  optional int64 endTime;
  optional group trainParams (MAP) {
    repeated group key_value {
      required binary key (STRING);
      optional binary value (STRING);
    }
  }
}

       
23/03/08 20:17:01  INFO FileOutputCommitter: Saved output of task 'attempt_202303082017015111697793144701181_0125_m_000000_231' to file:/tmp/iris_model/_model_2/meta/0/_temporary/0/task_202303082017015111697793144701181_0125_m_000000
23/03/08 20:17:01  INFO SparkHadoopMapRedUtil: attempt_202303082017015111697793144701181_0125_m_000000_231: Committed. Elapsed time: 0 ms.
23/03/08 20:17:01  INFO FileFormatWriter: Start to commit write Job e7cfa9ee-65df-42a7-a4fa-c26ad486fb9d.
23/03/08 20:17:01  INFO FileFormatWriter: Write Job e7cfa9ee-65df-42a7-a4fa-c26ad486fb9d committed. Elapsed time: 12 ms.
23/03/08 20:17:01  INFO FileFormatWriter: Finished processing stats for write job e7cfa9ee-65df-42a7-a4fa-c26ad486fb9d.
23/03/08 20:17:01  INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
23/03/08 20:17:01  INFO SparkContext: Starting job: parquet at BaseParams.scala:80
23/03/08 20:17:01  INFO SparkContext: Created broadcast 104 from broadcast at DAGScheduler.scala:1513
23/03/08 20:17:01  INFO FileSourceStrategy: Pushed Filters: 
23/03/08 20:17:01  INFO FileSourceStrategy: Post-Scan Filters: 
23/03/08 20:17:01  INFO FileSourceStrategy: Output Data Schema: struct<modelPath: string, algIndex: int, alg: string, metrics: array<struct<name:string,value:double>>, status: string ... 7 more fields>
23/03/08 20:17:01  INFO SparkContext: Created broadcast 105 from collect at MllibFunctions.scala:54
23/03/08 20:17:01  INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 20:17:01  INFO SparkContext: Starting job: collect at MllibFunctions.scala:54
23/03/08 20:17:01  INFO SparkContext: Created broadcast 106 from broadcast at DAGScheduler.scala:1513
23/03/08 20:17:01  INFO FileScanRDD: Reading File path: file:///tmp/iris_model/_model_2/meta/0/part-00000-235ab5ca-fbd8-4816-89ba-6d41ce364af1-c000.snappy.parquet, range: 0-4060, partition values: [empty row]
23/03/08 20:17:01  INFO SparkContext: Starting job: take at RestController.scala:290
23/03/08 20:17:01  INFO SparkContext: Created broadcast 107 from broadcast at DAGScheduler.scala:1513
23/03/08 20:17:01  INFO impl: Completed 200 in 4210ms 	POST /run/script



23/03/08 20:17:04  INFO DefaultConsoleClient: [owner] [admin] [groupId] [3d02248d-aae1-41f8-8329-8ded28b1fce0] __MMMMMM__ auth admin  want access tables: []
23/03/08 20:17:04  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [3d02248d-aae1-41f8-8329-8ded28b1fce0] __MMMMMM__ Total jobs: 1 current job:1 job script:register RandomForest.`/tmp/iris_model` as model_predict 
23/03/08 20:17:04  INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
23/03/08 20:17:04  INFO SparkContext: Starting job: parquet at MllibFunctions.scala:199
23/03/08 20:17:04  INFO SparkContext: Created broadcast 108 from broadcast at DAGScheduler.scala:1513
23/03/08 20:17:04  INFO FileSourceStrategy: Pushed Filters: 
23/03/08 20:17:04  INFO FileSourceStrategy: Post-Scan Filters: 
23/03/08 20:17:04  INFO FileSourceStrategy: Output Data Schema: struct<modelPath: string, algIndex: int, alg: string, metrics: array<struct<name:string,value:double>>, status: string ... 7 more fields>
23/03/08 20:17:04  INFO SparkContext: Created broadcast 109 from collect at MllibFunctions.scala:199
23/03/08 20:17:04  INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 20:17:04  INFO SparkContext: Starting job: collect at MllibFunctions.scala:199
23/03/08 20:17:04  INFO SparkContext: Created broadcast 110 from broadcast at DAGScheduler.scala:1513
23/03/08 20:17:04  INFO FileScanRDD: Reading File path: file:///tmp/iris_model/_model_2/meta/0/part-00000-235ab5ca-fbd8-4816-89ba-6d41ce364af1-c000.snappy.parquet, range: 0-4060, partition values: [empty row]
23/03/08 20:17:04  INFO SparkContext: Created broadcast 111 from textFile at ReadWrite.scala:587
23/03/08 20:17:04  INFO FileInputFormat: Total input files to process : 1
23/03/08 20:17:05  INFO SparkContext: Starting job: first at ReadWrite.scala:587
23/03/08 20:17:05  INFO SparkContext: Created broadcast 112 from broadcast at DAGScheduler.scala:1513
23/03/08 20:17:05  INFO HadoopRDD: Input split: file:/tmp/iris_model/_model_2/model/1/metadata/part-00000:0+723
23/03/08 20:17:05  INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
23/03/08 20:17:05  INFO SparkContext: Starting job: parquet at treeModels.scala:520
23/03/08 20:17:05  INFO SparkContext: Created broadcast 113 from broadcast at DAGScheduler.scala:1513
23/03/08 20:17:05  INFO FileSourceStrategy: Pushed Filters: 
23/03/08 20:17:05  INFO FileSourceStrategy: Post-Scan Filters: 
23/03/08 20:17:05  INFO FileSourceStrategy: Output Data Schema: struct<treeID: int, metadata: string, weights: double ... 1 more fields>
23/03/08 20:17:05  INFO CodeGenerator: Code generated in 10.938349 ms
23/03/08 20:17:05  INFO SparkContext: Created broadcast 114 from rdd at treeModels.scala:522
23/03/08 20:17:05  INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 20:17:05  INFO SparkContext: Starting job: collect at treeModels.scala:527
23/03/08 20:17:05  INFO SparkContext: Created broadcast 115 from broadcast at DAGScheduler.scala:1513
23/03/08 20:17:05  INFO CodeGenerator: Code generated in 5.011509 ms
23/03/08 20:17:05  INFO FileScanRDD: Reading File path: file:///tmp/iris_model/_model_2/model/1/treesMetadata/part-00000-540da374-5dee-43a3-bed0-93b50bf622ee-c000.snappy.parquet, range: 0-4414, partition values: [empty row]
23/03/08 20:17:05  INFO SparkContext: Created broadcast 116 from broadcast at DAGScheduler.scala:1513
23/03/08 20:17:05  INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
23/03/08 20:17:05  INFO SparkContext: Starting job: parquet at treeModels.scala:532
23/03/08 20:17:05  INFO SparkContext: Created broadcast 117 from broadcast at DAGScheduler.scala:1513
23/03/08 20:17:05  INFO FileSourceStrategy: Pushed Filters: 
23/03/08 20:17:05  INFO FileSourceStrategy: Post-Scan Filters: 
23/03/08 20:17:05  INFO FileSourceStrategy: Output Data Schema: struct<treeID: int, nodeData: struct<id: int, prediction: double, impurity: double, impurityStats: array<double>, rawCount: bigint ... 7 more fields>>
23/03/08 20:17:05  INFO SparkContext: Created broadcast 118 from rdd at treeModels.scala:543
23/03/08 20:17:05  INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 20:17:05  INFO SparkContext: Starting job: collect at treeModels.scala:549
23/03/08 20:17:05  INFO SparkContext: Created broadcast 119 from broadcast at DAGScheduler.scala:1513
23/03/08 20:17:05  INFO CodeGenerator: Code generated in 7.911697 ms
23/03/08 20:17:05  INFO CodeGenerator: Code generated in 9.950268 ms
23/03/08 20:17:05  INFO FileScanRDD: Reading File path: file:///tmp/iris_model/_model_2/model/1/data/part-00000-913979cd-abed-48d5-8cf7-cea803bde047-c000.snappy.parquet, range: 0-9561, partition values: [empty row]
23/03/08 20:17:05  INFO SparkContext: Created broadcast 120 from broadcast at DAGScheduler.scala:1513
23/03/08 20:17:05  INFO SparkContext: Created broadcast 121 from broadcast at DAGScheduler.scala:1513
23/03/08 20:17:05  INFO SparkContext: Created broadcast 122 from broadcast at Functions.scala:393
23/03/08 20:17:05  INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.
23/03/08 20:17:05  INFO SparkContext: Starting job: parquet at MllibFunctions.scala:199
23/03/08 20:17:05  INFO SparkContext: Created broadcast 123 from broadcast at DAGScheduler.scala:1513
23/03/08 20:17:05  INFO FileSourceStrategy: Pushed Filters: 
23/03/08 20:17:05  INFO FileSourceStrategy: Post-Scan Filters: 
23/03/08 20:17:05  INFO FileSourceStrategy: Output Data Schema: struct<modelPath: string, algIndex: int, alg: string, metrics: array<struct<name:string,value:double>>, status: string ... 7 more fields>
23/03/08 20:17:05  INFO SparkContext: Created broadcast 124 from collect at MllibFunctions.scala:199
23/03/08 20:17:05  INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 20:17:05  INFO SparkContext: Starting job: collect at MllibFunctions.scala:199
23/03/08 20:17:05  INFO SparkContext: Created broadcast 125 from broadcast at DAGScheduler.scala:1513
23/03/08 20:17:05  INFO FileScanRDD: Reading File path: file:///tmp/iris_model/_model_2/meta/0/part-00000-235ab5ca-fbd8-4816-89ba-6d41ce364af1-c000.snappy.parquet, range: 0-4060, partition values: [empty row]
23/03/08 20:17:05  INFO SparkContext: Created broadcast 126 from textFile at ReadWrite.scala:587
23/03/08 20:17:05  INFO FileInputFormat: Total input files to process : 1
23/03/08 20:17:05  INFO SparkContext: Starting job: first at ReadWrite.scala:587
23/03/08 20:17:05  INFO SparkContext: Created broadcast 127 from broadcast at DAGScheduler.scala:1513
23/03/08 20:17:05  INFO HadoopRDD: Input split: file:/tmp/iris_model/_model_2/model/1/metadata/part-00000:0+723
23/03/08 20:17:05  INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
23/03/08 20:17:05  INFO SparkContext: Starting job: parquet at treeModels.scala:520
23/03/08 20:17:05  INFO SparkContext: Created broadcast 128 from broadcast at DAGScheduler.scala:1513
23/03/08 20:17:05  INFO FileSourceStrategy: Pushed Filters: 
23/03/08 20:17:05  INFO FileSourceStrategy: Post-Scan Filters: 
23/03/08 20:17:05  INFO FileSourceStrategy: Output Data Schema: struct<treeID: int, metadata: string, weights: double ... 1 more fields>
23/03/08 20:17:05  INFO SparkContext: Created broadcast 129 from rdd at treeModels.scala:522
23/03/08 20:17:05  INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 20:17:05  INFO SparkContext: Starting job: collect at treeModels.scala:527
23/03/08 20:17:05  INFO SparkContext: Created broadcast 130 from broadcast at DAGScheduler.scala:1513
23/03/08 20:17:05  INFO FileScanRDD: Reading File path: file:///tmp/iris_model/_model_2/model/1/treesMetadata/part-00000-540da374-5dee-43a3-bed0-93b50bf622ee-c000.snappy.parquet, range: 0-4414, partition values: [empty row]
23/03/08 20:17:05  INFO SparkContext: Created broadcast 131 from broadcast at DAGScheduler.scala:1513
23/03/08 20:17:05  INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.
23/03/08 20:17:06  INFO SparkContext: Starting job: parquet at treeModels.scala:532
23/03/08 20:17:06  INFO SparkContext: Created broadcast 132 from broadcast at DAGScheduler.scala:1513
23/03/08 20:17:06  INFO FileSourceStrategy: Pushed Filters: 
23/03/08 20:17:06  INFO FileSourceStrategy: Post-Scan Filters: 
23/03/08 20:17:06  INFO FileSourceStrategy: Output Data Schema: struct<treeID: int, nodeData: struct<id: int, prediction: double, impurity: double, impurityStats: array<double>, rawCount: bigint ... 7 more fields>>
23/03/08 20:17:06  INFO SparkContext: Created broadcast 133 from rdd at treeModels.scala:543
23/03/08 20:17:06  INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
23/03/08 20:17:06  INFO SparkContext: Starting job: collect at treeModels.scala:549
23/03/08 20:17:06  INFO SparkContext: Created broadcast 134 from broadcast at DAGScheduler.scala:1513
23/03/08 20:17:06  INFO FileScanRDD: Reading File path: file:///tmp/iris_model/_model_2/model/1/data/part-00000-913979cd-abed-48d5-8cf7-cea803bde047-c000.snappy.parquet, range: 0-9561, partition values: [empty row]
23/03/08 20:17:06  INFO SparkContext: Created broadcast 135 from broadcast at DAGScheduler.scala:1513
23/03/08 20:17:06  INFO SparkContext: Created broadcast 136 from broadcast at DAGScheduler.scala:1513
23/03/08 20:17:06  INFO SparkContext: Starting job: take at RestController.scala:290
23/03/08 20:17:06  INFO SparkContext: Created broadcast 137 from broadcast at DAGScheduler.scala:1513
23/03/08 20:17:06  INFO impl: Completed 200 in 1503ms 	POST /run/script



23/03/08 20:17:31  INFO DefaultConsoleClient: [owner] [admin] [groupId] [2e86bbb7-2973-4274-876c-cd82667b66e0] __MMMMMM__ auth admin  want access tables: [{"table":"test_data","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"output","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}}]
23/03/08 20:17:31  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [2e86bbb7-2973-4274-876c-cd82667b66e0] __MMMMMM__ Total jobs: 1 current job:1 job script:select vec_argmax(model_predict(features)) from test_data as output 
23/03/08 20:17:31  ERROR RestController: An error occurred while the job manager was executing the task, 
org.apache.spark.sql.AnalysisException: Column 'features' does not exist. Did you mean one of the following? [test_data.feature, test_data.label, test_data.__split__]; line 1 pos 32;
'Project [unresolvedalias('vec_argmax('model_predict('features)), None)]
+- SubqueryAlias test_data
   +- View (`test_data`, [feature#527,label#528,__split__#529])
      +- Project [feature#527, label#528, __split__#529]
         +- Filter (__split__#529 = 1)
            +- SubqueryAlias iris_features_split
               +- View (`iris_features_split`, [feature#527,label#528,__split__#529])
                  +- LogicalRDD [feature#527, label#528, __split__#529], false

	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:54) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$7(CheckAnalysis.scala:199) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$7$adapted(CheckAnalysis.scala:192) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62) ~[scala-library-2.12.15.jar:?]
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55) ~[scala-library-2.12.15.jar:?]
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49) ~[scala-library-2.12.15.jar:?]
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62) ~[scala-library-2.12.15.jar:?]
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55) ~[scala-library-2.12.15.jar:?]
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49) ~[scala-library-2.12.15.jar:?]
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:366) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:366) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at scala.collection.Iterator.foreach(Iterator.scala:943) ~[scala-library-2.12.15.jar:?]
	at scala.collection.Iterator.foreach$(Iterator.scala:943) ~[scala-library-2.12.15.jar:?]
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431) ~[scala-library-2.12.15.jar:?]
	at scala.collection.IterableLike.foreach(IterableLike.scala:74) ~[scala-library-2.12.15.jar:?]
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73) ~[scala-library-2.12.15.jar:?]
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56) ~[scala-library-2.12.15.jar:?]
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:366) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$6(CheckAnalysis.scala:192) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$6$adapted(CheckAnalysis.scala:192) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at scala.collection.immutable.Stream.foreach(Stream.scala:533) ~[scala-library-2.12.15.jar:?]
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:192) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:101) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:367) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:101) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:96) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:187) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:210) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:207) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76) ~[spark-sql_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111) ~[spark-catalyst_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185) ~[spark-sql_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510) ~[spark-sql_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185) ~[spark-sql_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779) ~[spark-sql_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184) ~[spark-sql_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76) ~[spark-sql_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74) ~[spark-sql_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66) ~[spark-sql_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99) ~[spark-sql_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779) ~[spark-sql_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97) ~[spark-sql_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622) ~[spark-sql_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779) ~[spark-sql_2.12-3.3.0.jar:3.3.0]
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617) ~[spark-sql_2.12-3.3.0.jar:3.3.0]
	at tech.mlsql.dsl.adaptor.SelectAdaptor.parse(SelectAdaptor.scala:73) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at streaming.dsl.ScriptSQLExecListener.execute$1(ScriptSQLExec.scala:406) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at streaming.dsl.ScriptSQLExecListener.exitSql(ScriptSQLExec.scala:420) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at streaming.dsl.parser.DSLSQLParser$SqlContext.exitRule(DSLSQLParser.java:296) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.antlr.v4.runtime.tree.ParseTreeWalker.exitRule(ParseTreeWalker.java:47) ~[antlr4-runtime-4.8.jar:4.8]
	at org.antlr.v4.runtime.tree.ParseTreeWalker.walk(ParseTreeWalker.java:30) ~[antlr4-runtime-4.8.jar:4.8]
	at org.antlr.v4.runtime.tree.ParseTreeWalker.walk(ParseTreeWalker.java:28) ~[antlr4-runtime-4.8.jar:4.8]
	at streaming.dsl.ScriptSQLExec$._parse(ScriptSQLExec.scala:159) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at streaming.dsl.ScriptSQLExec$.parse(ScriptSQLExec.scala:146) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at streaming.rest.RestController.$anonfun$script$9(RestController.scala:201) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at tech.mlsql.job.JobManager$.run(JobManager.scala:74) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at streaming.rest.RestController.query$1(RestController.scala:196) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at streaming.rest.RestController.script(RestController.scala:223) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_151]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_151]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_151]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_151]
	at net.csdn.modules.http.RestController.filter(RestController.java:139) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at net.csdn.modules.http.RestController.dispatchRequest(RestController.java:99) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at net.csdn.modules.http.HttpServer$DefaultHandler.handle(HttpServer.java:182) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.server.handler.HandlerList.handle(HandlerList.java:52) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:97) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.server.Server.handle(Server.java:499) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:311) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:257) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.io.AbstractConnection$2.run(AbstractConnection.java:544) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555) ~[byzer-lang-3.3.0-2.12-2.4.0-SNAPSHOT.jar:?]
	at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_151]
23/03/08 20:17:31  INFO impl: Completed 500 in 84ms 	POST /run/script



23/03/08 20:17:36  INFO DefaultConsoleClient: [owner] [admin] [groupId] [08a146ef-da59-48a8-ba44-e126548f7628] __MMMMMM__ auth admin  want access tables: [{"table":"test_data","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}},{"table":"output","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}}]
23/03/08 20:17:36  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [08a146ef-da59-48a8-ba44-e126548f7628] __MMMMMM__ Total jobs: 1 current job:1 job script:select vec_argmax(model_predict(feature)) from test_data as output 
23/03/08 20:17:36  INFO CodeGenerator: Code generated in 8.249149 ms
23/03/08 20:17:36  INFO SparkContext: Starting job: take at RestController.scala:290
23/03/08 20:17:36  INFO SparkContext: Created broadcast 138 from broadcast at DAGScheduler.scala:1513
23/03/08 20:17:36  INFO CodeGenerator: Code generated in 7.211948 ms
23/03/08 20:17:36  INFO CodeGenerator: Code generated in 6.147592 ms
23/03/08 20:17:36  INFO SparkContext: Starting job: take at RestController.scala:290
23/03/08 20:17:36  INFO SparkContext: Created broadcast 139 from broadcast at DAGScheduler.scala:1513
23/03/08 20:17:36  INFO impl: Completed 200 in 142ms 	POST /run/script



23/03/08 20:18:10  INFO DefaultConsoleClient: [owner] [admin] [groupId] [60a79ef4-4ad3-4caa-b0d4-d48b44b437bc] __MMMMMM__ auth admin  want access tables: [{"table":"command","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}}]
23/03/08 20:18:10  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [60a79ef4-4ad3-4caa-b0d4-d48b44b437bc] __MMMMMM__ Total jobs: 1 current job:1 job script:run command as ShellExecute.`` where parameters='''["curl","-XPOST","http://127.0.0.1:9004/model/predict","-d","dataType=row&sql=select vec_argmax(model_predict(vec_dense(doc))) as label &data=[{\"doc\":[5.1,3.5,1.4,0.2]}]"]''' 
23/03/08 20:18:10  INFO DefaultConsoleClient: [owner] [admin] [groupId] [60a79ef4-4ad3-4caa-b0d4-d48b44b437bc] __MMMMMM__ auth admin  want access tables: [{"db":"mlsql_system","table":"__shell_execute__","operateType":{"i":9,"name":"empty"},"sourceType":"_mlsql_","tableType":{"name":"system","includes":["model","modelList","_mlsql_","modelExplain","modelExample","modelParams"]}}]
23/03/08 20:18:10  INFO ShellExecute: [owner] [admin] [groupId] [60a79ef4-4ad3-4caa-b0d4-d48b44b437bc] __MMMMMM__   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
23/03/08 20:18:10  INFO ShellExecute: [owner] [admin] [groupId] [60a79ef4-4ad3-4caa-b0d4-d48b44b437bc] __MMMMMM__                                  Dload  Upload   Total   Spent    Left  Speed
23/03/08 20:18:10  INFO ShellExecute: [owner] [admin] [groupId] [60a79ef4-4ad3-4caa-b0d4-d48b44b437bc] __MMMMMM__ 
23/03/08 20:18:10  INFO ShellExecute: [owner] [admin] [groupId] [60a79ef4-4ad3-4caa-b0d4-d48b44b437bc] __MMMMMM__   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
23/03/08 20:18:10  INFO ShellExecute: [owner] [admin] [groupId] [60a79ef4-4ad3-4caa-b0d4-d48b44b437bc] __MMMMMM__   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
23/03/08 20:18:10  INFO ShellExecute: [owner] [admin] [groupId] [60a79ef4-4ad3-4caa-b0d4-d48b44b437bc] __MMMMMM__ curl: (7) Failed to connect to 127.0.0.1 port 9004: Connection refused
23/03/08 20:18:10  INFO CodeGenerator: Code generated in 5.627887 ms
23/03/08 20:18:10  INFO SparkContext: Starting job: take at RestController.scala:290
23/03/08 20:18:10  INFO SparkContext: Created broadcast 140 from broadcast at DAGScheduler.scala:1513
23/03/08 20:18:10  INFO CodeGenerator: Code generated in 3.585592 ms
23/03/08 20:18:10  INFO CodeGenerator: Code generated in 7.418431 ms
23/03/08 20:18:10  INFO SparkContext: Starting job: take at RestController.scala:290
23/03/08 20:18:10  INFO SparkContext: Created broadcast 141 from broadcast at DAGScheduler.scala:1513
23/03/08 20:18:10  INFO SparkContext: Starting job: take at RestController.scala:290
23/03/08 20:18:10  INFO SparkContext: Created broadcast 142 from broadcast at DAGScheduler.scala:1513
23/03/08 20:18:10  INFO impl: Completed 200 in 175ms 	POST /run/script



23/03/08 20:18:20  INFO DefaultConsoleClient: [owner] [admin] [groupId] [100830b0-a06d-440b-94ca-62e21522dd47] __MMMMMM__ auth admin  want access tables: [{"table":"command","operateType":{"i":7,"name":"select"},"tableType":{"name":"temp","includes":["jsonStr","mockStream","csvStr","temp","webConsole","script","console"]}}]
23/03/08 20:18:20  INFO DefaultMLSQLJobProgressListener: [owner] [admin] [groupId] [100830b0-a06d-440b-94ca-62e21522dd47] __MMMMMM__ Total jobs: 1 current job:1 job script:run command as ShellExecute.`` where parameters='''["curl","-XPOST","http://127.0.0.1:9003/model/predict","-d","dataType=row&sql=select vec_argmax(model_predict(vec_dense(doc))) as label &data=[{\"doc\":[5.1,3.5,1.4,0.2]}]"]''' 
23/03/08 20:18:20  INFO DefaultConsoleClient: [owner] [admin] [groupId] [100830b0-a06d-440b-94ca-62e21522dd47] __MMMMMM__ auth admin  want access tables: [{"db":"mlsql_system","table":"__shell_execute__","operateType":{"i":9,"name":"empty"},"sourceType":"_mlsql_","tableType":{"name":"system","includes":["model","modelList","_mlsql_","modelExplain","modelExample","modelParams"]}}]
23/03/08 20:18:20  INFO ShellExecute: [owner] [admin] [groupId] [100830b0-a06d-440b-94ca-62e21522dd47] __MMMMMM__   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
23/03/08 20:18:20  INFO ShellExecute: [owner] [admin] [groupId] [100830b0-a06d-440b-94ca-62e21522dd47] __MMMMMM__                                  Dload  Upload   Total   Spent    Left  Speed
23/03/08 20:18:20  INFO ShellExecute: [owner] [admin] [groupId] [100830b0-a06d-440b-94ca-62e21522dd47] __MMMMMM__ 
23/03/08 20:18:20  INFO ShellExecute: [owner] [admin] [groupId] [100830b0-a06d-440b-94ca-62e21522dd47] __MMMMMM__   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
23/03/08 20:18:20  INFO ShellExecute: [owner] [admin] [groupId] [100830b0-a06d-440b-94ca-62e21522dd47] __MMMMMM__   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
23/03/08 20:18:20  INFO ShellExecute: [owner] [admin] [groupId] [100830b0-a06d-440b-94ca-62e21522dd47] __MMMMMM__ curl: (7) Failed to connect to 127.0.0.1 port 9003: Connection refused
23/03/08 20:18:20  INFO SparkContext: Starting job: take at RestController.scala:290
23/03/08 20:18:20  INFO SparkContext: Created broadcast 143 from broadcast at DAGScheduler.scala:1513
23/03/08 20:18:20  INFO SparkContext: Starting job: take at RestController.scala:290
23/03/08 20:18:20  INFO SparkContext: Created broadcast 144 from broadcast at DAGScheduler.scala:1513
23/03/08 20:18:20  INFO SparkContext: Starting job: take at RestController.scala:290
23/03/08 20:18:20  INFO SparkContext: Created broadcast 145 from broadcast at DAGScheduler.scala:1513
23/03/08 20:18:20  INFO impl: Completed 200 in 100ms 	POST /run/script



